{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "X = torch.arange(0, 1, 0.02).to(device=device)\n",
    "y = bias + weights * X\n",
    "\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.unsqueeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train, train_lables=y_train, test_data=X_test, test_lables=y_test, predictions=None):\n",
    "    plt.scatter(train_data.cpu().numpy(), train_lables.cpu().numpy(), c='b', s=4, label='Train data')\n",
    "    plt.scatter(test_data.cpu().numpy(), test_lables.cpu().numpy(), c='g', s=4, label='Test data')\n",
    "\n",
    "    if predictions != None:\n",
    "        plt.scatter(test_data.cpu().numpy(), predictions.cpu().numpy(), c='r', s=4, label='Predictions')\n",
    "    \n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.linear_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[-0.6091]])), ('0.bias', tensor([0.6142]))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=1, out_features=1)\n",
    ")\n",
    "\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\environments\\mltorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "d:\\environments\\mltorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:101: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.5552012920379639 | Test loss: 0.574013888835907\n",
      "Epoch: 2 | Loss: 0.5437553524971008 | Test loss: 0.5605806112289429\n",
      "Epoch: 3 | Loss: 0.5323348045349121 | Test loss: 0.547185480594635\n",
      "Epoch: 4 | Loss: 0.5209753513336182 | Test loss: 0.5338417887687683\n",
      "Epoch: 5 | Loss: 0.5097047090530396 | Test loss: 0.5204979777336121\n",
      "Epoch: 6 | Loss: 0.4984827935695648 | Test loss: 0.5072189569473267\n",
      "Epoch: 7 | Loss: 0.4873623251914978 | Test loss: 0.4940182864665985\n",
      "Epoch: 8 | Loss: 0.47636815905570984 | Test loss: 0.48084279894828796\n",
      "Epoch: 9 | Loss: 0.4654720723628998 | Test loss: 0.46773427724838257\n",
      "Epoch: 10 | Loss: 0.45469412207603455 | Test loss: 0.4547320008277893\n",
      "Epoch: 11 | Loss: 0.44407570362091064 | Test loss: 0.44179439544677734\n",
      "Epoch: 12 | Loss: 0.4336152672767639 | Test loss: 0.42891258001327515\n",
      "Epoch: 13 | Loss: 0.4232781231403351 | Test loss: 0.4161657691001892\n",
      "Epoch: 14 | Loss: 0.4131300151348114 | Test loss: 0.4035395681858063\n",
      "Epoch: 15 | Loss: 0.403207927942276 | Test loss: 0.3909425139427185\n",
      "Epoch: 16 | Loss: 0.39340102672576904 | Test loss: 0.37851014733314514\n",
      "Epoch: 17 | Loss: 0.38381293416023254 | Test loss: 0.3662576973438263\n",
      "Epoch: 18 | Loss: 0.3745076656341553 | Test loss: 0.35400518774986267\n",
      "Epoch: 19 | Loss: 0.3653089702129364 | Test loss: 0.3419479429721832\n",
      "Epoch: 20 | Loss: 0.3563535213470459 | Test loss: 0.33010154962539673\n",
      "Epoch: 21 | Loss: 0.34771475195884705 | Test loss: 0.31825515627861023\n",
      "Epoch: 22 | Loss: 0.33918750286102295 | Test loss: 0.30663546919822693\n",
      "Epoch: 23 | Loss: 0.33093276619911194 | Test loss: 0.29516634345054626\n",
      "Epoch: 24 | Loss: 0.3229411244392395 | Test loss: 0.28378933668136597\n",
      "Epoch: 25 | Loss: 0.31513267755508423 | Test loss: 0.2726588845252991\n",
      "Epoch: 26 | Loss: 0.3076286315917969 | Test loss: 0.261666864156723\n",
      "Epoch: 27 | Loss: 0.3003527820110321 | Test loss: 0.2508412003517151\n",
      "Epoch: 28 | Loss: 0.2933224141597748 | Test loss: 0.24019895493984222\n",
      "Epoch: 29 | Loss: 0.2865433394908905 | Test loss: 0.22976283729076385\n",
      "Epoch: 30 | Loss: 0.28002557158470154 | Test loss: 0.2194744050502777\n",
      "Epoch: 31 | Loss: 0.2737428843975067 | Test loss: 0.20939524471759796\n",
      "Epoch: 32 | Loss: 0.2677084803581238 | Test loss: 0.1995142251253128\n",
      "Epoch: 33 | Loss: 0.26193130016326904 | Test loss: 0.18981143832206726\n",
      "Epoch: 34 | Loss: 0.2563842236995697 | Test loss: 0.18032413721084595\n",
      "Epoch: 35 | Loss: 0.2510877251625061 | Test loss: 0.17104992270469666\n",
      "Epoch: 36 | Loss: 0.24603483080863953 | Test loss: 0.16192850470542908\n",
      "Epoch: 37 | Loss: 0.24118709564208984 | Test loss: 0.15306851267814636\n",
      "Epoch: 38 | Loss: 0.2365986555814743 | Test loss: 0.14438296854496002\n",
      "Epoch: 39 | Loss: 0.23221637308597565 | Test loss: 0.13589869439601898\n",
      "Epoch: 40 | Loss: 0.22805045545101166 | Test loss: 0.12761756777763367\n",
      "Epoch: 41 | Loss: 0.22409218549728394 | Test loss: 0.11953946948051453\n",
      "Epoch: 42 | Loss: 0.22034117579460144 | Test loss: 0.11180897802114487\n",
      "Epoch: 43 | Loss: 0.21679553389549255 | Test loss: 0.10432419180870056\n",
      "Epoch: 44 | Loss: 0.21343618631362915 | Test loss: 0.09729604423046112\n",
      "Epoch: 45 | Loss: 0.21026529371738434 | Test loss: 0.09059655666351318\n",
      "Epoch: 46 | Loss: 0.20728358626365662 | Test loss: 0.08436054736375809\n",
      "Epoch: 47 | Loss: 0.20446009933948517 | Test loss: 0.078519307076931\n",
      "Epoch: 48 | Loss: 0.20181086659431458 | Test loss: 0.07316262274980545\n",
      "Epoch: 49 | Loss: 0.19932140409946442 | Test loss: 0.06823845207691193\n",
      "Epoch: 50 | Loss: 0.19698387384414673 | Test loss: 0.06379757821559906\n",
      "Epoch: 51 | Loss: 0.19479046761989594 | Test loss: 0.059782326221466064\n",
      "Epoch: 52 | Loss: 0.1927352249622345 | Test loss: 0.05621368810534477\n",
      "Epoch: 53 | Loss: 0.1908024400472641 | Test loss: 0.05309798941016197\n",
      "Epoch: 54 | Loss: 0.1889910250902176 | Test loss: 0.05037136748433113\n",
      "Epoch: 55 | Loss: 0.18729670345783234 | Test loss: 0.048153024166822433\n",
      "Epoch: 56 | Loss: 0.1857275366783142 | Test loss: 0.04626530408859253\n",
      "Epoch: 57 | Loss: 0.18426024913787842 | Test loss: 0.044794607907533646\n",
      "Epoch: 58 | Loss: 0.18288905918598175 | Test loss: 0.043714072555303574\n",
      "Epoch: 59 | Loss: 0.18161845207214355 | Test loss: 0.04295841231942177\n",
      "Epoch: 60 | Loss: 0.1804312914609909 | Test loss: 0.042539190500974655\n",
      "Epoch: 61 | Loss: 0.1793341338634491 | Test loss: 0.042406558990478516\n",
      "Epoch: 62 | Loss: 0.17830821871757507 | Test loss: 0.04251650720834732\n",
      "Epoch: 63 | Loss: 0.1773538589477539 | Test loss: 0.04286820814013481\n",
      "Epoch: 64 | Loss: 0.17646579444408417 | Test loss: 0.04342046007514\n",
      "Epoch: 65 | Loss: 0.17563626170158386 | Test loss: 0.04417460411787033\n",
      "Epoch: 66 | Loss: 0.1748693287372589 | Test loss: 0.045072682201862335\n",
      "Epoch: 67 | Loss: 0.1741654872894287 | Test loss: 0.04610194265842438\n",
      "Epoch: 68 | Loss: 0.17351220548152924 | Test loss: 0.0472513772547245\n",
      "Epoch: 69 | Loss: 0.17290370166301727 | Test loss: 0.04847407341003418\n",
      "Epoch: 70 | Loss: 0.1723358929157257 | Test loss: 0.049791883677244186\n",
      "Epoch: 71 | Loss: 0.17180538177490234 | Test loss: 0.051176927983760834\n",
      "Epoch: 72 | Loss: 0.17131389677524567 | Test loss: 0.05261564999818802\n",
      "Epoch: 73 | Loss: 0.1708551049232483 | Test loss: 0.05404452979564667\n",
      "Epoch: 74 | Loss: 0.1704290807247162 | Test loss: 0.05553051829338074\n",
      "Epoch: 75 | Loss: 0.1700276881456375 | Test loss: 0.05704471841454506\n",
      "Epoch: 76 | Loss: 0.1696542650461197 | Test loss: 0.058588284999132156\n",
      "Epoch: 77 | Loss: 0.16930413246154785 | Test loss: 0.06012050807476044\n",
      "Epoch: 78 | Loss: 0.1689743548631668 | Test loss: 0.06162780523300171\n",
      "Epoch: 79 | Loss: 0.16866521537303925 | Test loss: 0.06315368413925171\n",
      "Epoch: 80 | Loss: 0.16837403178215027 | Test loss: 0.0646781176328659\n",
      "Epoch: 81 | Loss: 0.16809909045696259 | Test loss: 0.06621180474758148\n",
      "Epoch: 82 | Loss: 0.16783881187438965 | Test loss: 0.06770656257867813\n",
      "Epoch: 83 | Loss: 0.16759546101093292 | Test loss: 0.06917549669742584\n",
      "Epoch: 84 | Loss: 0.16736412048339844 | Test loss: 0.07062776386737823\n",
      "Epoch: 85 | Loss: 0.1671449989080429 | Test loss: 0.07205751538276672\n",
      "Epoch: 86 | Loss: 0.16693776845932007 | Test loss: 0.07346165180206299\n",
      "Epoch: 87 | Loss: 0.16674432158470154 | Test loss: 0.07487361133098602\n",
      "Epoch: 88 | Loss: 0.1665511131286621 | Test loss: 0.07628054916858673\n",
      "Epoch: 89 | Loss: 0.16637106239795685 | Test loss: 0.07759855687618256\n",
      "Epoch: 90 | Loss: 0.16620199382305145 | Test loss: 0.07893338799476624\n",
      "Epoch: 91 | Loss: 0.166036456823349 | Test loss: 0.08020027726888657\n",
      "Epoch: 92 | Loss: 0.16588155925273895 | Test loss: 0.08146373927593231\n",
      "Epoch: 93 | Loss: 0.16573111712932587 | Test loss: 0.08269870281219482\n",
      "Epoch: 94 | Loss: 0.16558684408664703 | Test loss: 0.08391010016202927\n",
      "Epoch: 95 | Loss: 0.1654488742351532 | Test loss: 0.08509775251150131\n",
      "Epoch: 96 | Loss: 0.16531434655189514 | Test loss: 0.08624482899904251\n",
      "Epoch: 97 | Loss: 0.1651856154203415 | Test loss: 0.08738011866807938\n",
      "Epoch: 98 | Loss: 0.16506068408489227 | Test loss: 0.08848367631435394\n",
      "Epoch: 99 | Loss: 0.16493907570838928 | Test loss: 0.08957748115062714\n",
      "Epoch: 100 | Loss: 0.16482001543045044 | Test loss: 0.09067004919052124\n",
      "Epoch: 101 | Loss: 0.1647040694952011 | Test loss: 0.09171426296234131\n",
      "Epoch: 102 | Loss: 0.16459167003631592 | Test loss: 0.09273865818977356\n",
      "Epoch: 103 | Loss: 0.16448265314102173 | Test loss: 0.09373015910387039\n",
      "Epoch: 104 | Loss: 0.1643763929605484 | Test loss: 0.09468965232372284\n",
      "Epoch: 105 | Loss: 0.16427284479141235 | Test loss: 0.09565165638923645\n",
      "Epoch: 106 | Loss: 0.16417083144187927 | Test loss: 0.09659774601459503\n",
      "Epoch: 107 | Loss: 0.16407135128974915 | Test loss: 0.09750901907682419\n",
      "Epoch: 108 | Loss: 0.16397400200366974 | Test loss: 0.09838850051164627\n",
      "Epoch: 109 | Loss: 0.16387934982776642 | Test loss: 0.09926585108041763\n",
      "Epoch: 110 | Loss: 0.16378594934940338 | Test loss: 0.1001431792974472\n",
      "Epoch: 111 | Loss: 0.16369254887104034 | Test loss: 0.10102050751447678\n",
      "Epoch: 112 | Loss: 0.16360053420066833 | Test loss: 0.101841501891613\n",
      "Epoch: 113 | Loss: 0.16351181268692017 | Test loss: 0.10264679044485092\n",
      "Epoch: 114 | Loss: 0.1634235680103302 | Test loss: 0.10345205664634705\n",
      "Epoch: 115 | Loss: 0.1633359044790268 | Test loss: 0.10422372817993164\n",
      "Epoch: 116 | Loss: 0.1632496565580368 | Test loss: 0.10497745126485825\n",
      "Epoch: 117 | Loss: 0.16316433250904083 | Test loss: 0.10573118180036545\n",
      "Epoch: 118 | Loss: 0.16307960450649261 | Test loss: 0.10646484792232513\n",
      "Epoch: 119 | Loss: 0.1629960983991623 | Test loss: 0.10718221217393875\n",
      "Epoch: 120 | Loss: 0.16291365027427673 | Test loss: 0.10787566751241684\n",
      "Epoch: 121 | Loss: 0.16283316910266876 | Test loss: 0.10856912285089493\n",
      "Epoch: 122 | Loss: 0.16275270283222198 | Test loss: 0.10926257818937302\n",
      "Epoch: 123 | Loss: 0.16267259418964386 | Test loss: 0.10993637144565582\n",
      "Epoch: 124 | Loss: 0.1625935286283493 | Test loss: 0.1105928122997284\n",
      "Epoch: 125 | Loss: 0.16251491010189056 | Test loss: 0.11122894287109375\n",
      "Epoch: 126 | Loss: 0.16243745386600494 | Test loss: 0.11186505854129791\n",
      "Epoch: 127 | Loss: 0.16235999763011932 | Test loss: 0.11250117421150208\n",
      "Epoch: 128 | Loss: 0.1622832864522934 | Test loss: 0.11309827864170074\n",
      "Epoch: 129 | Loss: 0.16220781207084656 | Test loss: 0.11368022859096527\n",
      "Epoch: 130 | Loss: 0.1621323972940445 | Test loss: 0.114262156188488\n",
      "Epoch: 131 | Loss: 0.1620570868253708 | Test loss: 0.11482539772987366\n",
      "Epoch: 132 | Loss: 0.16198241710662842 | Test loss: 0.11538861691951752\n",
      "Epoch: 133 | Loss: 0.16190776228904724 | Test loss: 0.11595182865858078\n",
      "Epoch: 134 | Loss: 0.16183336079120636 | Test loss: 0.11649566143751144\n",
      "Epoch: 135 | Loss: 0.16175951063632965 | Test loss: 0.11703946441411972\n",
      "Epoch: 136 | Loss: 0.16168594360351562 | Test loss: 0.11756324768066406\n",
      "Epoch: 137 | Loss: 0.16161298751831055 | Test loss: 0.11808698624372482\n",
      "Epoch: 138 | Loss: 0.16154052317142487 | Test loss: 0.11859001219272614\n",
      "Epoch: 139 | Loss: 0.1614685356616974 | Test loss: 0.11909305304288864\n",
      "Epoch: 140 | Loss: 0.1613965779542923 | Test loss: 0.11959607899188995\n",
      "Epoch: 141 | Loss: 0.161324605345726 | Test loss: 0.12009910494089127\n",
      "Epoch: 142 | Loss: 0.16125266253948212 | Test loss: 0.12061730027198792\n",
      "Epoch: 143 | Loss: 0.16118073463439941 | Test loss: 0.12113548070192337\n",
      "Epoch: 144 | Loss: 0.1611088216304779 | Test loss: 0.12165367603302002\n",
      "Epoch: 145 | Loss: 0.1610369086265564 | Test loss: 0.12217187881469727\n",
      "Epoch: 146 | Loss: 0.1609649807214737 | Test loss: 0.12269006669521332\n",
      "Epoch: 147 | Loss: 0.16089309751987457 | Test loss: 0.12322276830673218\n",
      "Epoch: 148 | Loss: 0.16082139313220978 | Test loss: 0.1237824484705925\n",
      "Epoch: 149 | Loss: 0.16075067222118378 | Test loss: 0.12425542622804642\n",
      "Epoch: 150 | Loss: 0.16068439185619354 | Test loss: 0.124676913022995\n",
      "Epoch: 151 | Loss: 0.16061843931674957 | Test loss: 0.12509837746620178\n",
      "Epoch: 152 | Loss: 0.1605524718761444 | Test loss: 0.12551985681056976\n",
      "Epoch: 153 | Loss: 0.16048651933670044 | Test loss: 0.12594133615493774\n",
      "Epoch: 154 | Loss: 0.16042056679725647 | Test loss: 0.12636281549930573\n",
      "Epoch: 155 | Loss: 0.1603546142578125 | Test loss: 0.1267843097448349\n",
      "Epoch: 156 | Loss: 0.16028867661952972 | Test loss: 0.1272057741880417\n",
      "Epoch: 157 | Loss: 0.16022272408008575 | Test loss: 0.12762723863124847\n",
      "Epoch: 158 | Loss: 0.16015678644180298 | Test loss: 0.12804873287677765\n",
      "Epoch: 159 | Loss: 0.16009081900119781 | Test loss: 0.12847018241882324\n",
      "Epoch: 160 | Loss: 0.16002486646175385 | Test loss: 0.12889167666435242\n",
      "Epoch: 161 | Loss: 0.15995892882347107 | Test loss: 0.1293131560087204\n",
      "Epoch: 162 | Loss: 0.1598929762840271 | Test loss: 0.12973462045192719\n",
      "Epoch: 163 | Loss: 0.15982702374458313 | Test loss: 0.13015611469745636\n",
      "Epoch: 164 | Loss: 0.15976141393184662 | Test loss: 0.1305566430091858\n",
      "Epoch: 165 | Loss: 0.15969625115394592 | Test loss: 0.13095715641975403\n",
      "Epoch: 166 | Loss: 0.15963119268417358 | Test loss: 0.13133737444877625\n",
      "Epoch: 167 | Loss: 0.15956664085388184 | Test loss: 0.13171757757663727\n",
      "Epoch: 168 | Loss: 0.15950235724449158 | Test loss: 0.1321105659008026\n",
      "Epoch: 169 | Loss: 0.15943846106529236 | Test loss: 0.13250353932380676\n",
      "Epoch: 170 | Loss: 0.1593746691942215 | Test loss: 0.13287685811519623\n",
      "Epoch: 171 | Loss: 0.15931127965450287 | Test loss: 0.1332501769065857\n",
      "Epoch: 172 | Loss: 0.15924814343452454 | Test loss: 0.13363690674304962\n",
      "Epoch: 173 | Loss: 0.15918530523777008 | Test loss: 0.13402362167835236\n",
      "Epoch: 174 | Loss: 0.15912246704101562 | Test loss: 0.13441036641597748\n",
      "Epoch: 175 | Loss: 0.15905962884426117 | Test loss: 0.1347971260547638\n",
      "Epoch: 176 | Loss: 0.1589967906475067 | Test loss: 0.13516487181186676\n",
      "Epoch: 177 | Loss: 0.1589343249797821 | Test loss: 0.13554668426513672\n",
      "Epoch: 178 | Loss: 0.15887229144573212 | Test loss: 0.13592851161956787\n",
      "Epoch: 179 | Loss: 0.15881028771400452 | Test loss: 0.13631033897399902\n",
      "Epoch: 180 | Loss: 0.15874825417995453 | Test loss: 0.13669218122959137\n",
      "Epoch: 181 | Loss: 0.15868625044822693 | Test loss: 0.13705328106880188\n",
      "Epoch: 182 | Loss: 0.15862485766410828 | Test loss: 0.1374143809080124\n",
      "Epoch: 183 | Loss: 0.1585635095834732 | Test loss: 0.1377919614315033\n",
      "Epoch: 184 | Loss: 0.15850220620632172 | Test loss: 0.13816958665847778\n",
      "Epoch: 185 | Loss: 0.15844085812568665 | Test loss: 0.13854719698429108\n",
      "Epoch: 186 | Loss: 0.15837953984737396 | Test loss: 0.13892480731010437\n",
      "Epoch: 187 | Loss: 0.15831823647022247 | Test loss: 0.13931713998317719\n",
      "Epoch: 188 | Loss: 0.1582576185464859 | Test loss: 0.1396840661764145\n",
      "Epoch: 189 | Loss: 0.1581980288028717 | Test loss: 0.1400509774684906\n",
      "Epoch: 190 | Loss: 0.1581384390592575 | Test loss: 0.1404179036617279\n",
      "Epoch: 191 | Loss: 0.1580788642168045 | Test loss: 0.1407848298549652\n",
      "Epoch: 192 | Loss: 0.1580192893743515 | Test loss: 0.14115175604820251\n",
      "Epoch: 193 | Loss: 0.1579596996307373 | Test loss: 0.14151868224143982\n",
      "Epoch: 194 | Loss: 0.15790025889873505 | Test loss: 0.14186440408229828\n",
      "Epoch: 195 | Loss: 0.15784138441085815 | Test loss: 0.14221011102199554\n",
      "Epoch: 196 | Loss: 0.15778258442878723 | Test loss: 0.14256949722766876\n",
      "Epoch: 197 | Loss: 0.15772423148155212 | Test loss: 0.1429288536310196\n",
      "Epoch: 198 | Loss: 0.15766586363315582 | Test loss: 0.14328822493553162\n",
      "Epoch: 199 | Loss: 0.15760768949985504 | Test loss: 0.14362818002700806\n",
      "Epoch: 200 | Loss: 0.15754981338977814 | Test loss: 0.14398063719272614\n",
      "Epoch: 201 | Loss: 0.15749254822731018 | Test loss: 0.1443331092596054\n",
      "Epoch: 202 | Loss: 0.15743528306484222 | Test loss: 0.14468559622764587\n",
      "Epoch: 203 | Loss: 0.15737822651863098 | Test loss: 0.14501751959323883\n",
      "Epoch: 204 | Loss: 0.1573215126991272 | Test loss: 0.14534948766231537\n",
      "Epoch: 205 | Loss: 0.1572647988796234 | Test loss: 0.14568139612674713\n",
      "Epoch: 206 | Loss: 0.1572081744670868 | Test loss: 0.14602875709533691\n",
      "Epoch: 207 | Loss: 0.1571516990661621 | Test loss: 0.1463761031627655\n",
      "Epoch: 208 | Loss: 0.15709525346755981 | Test loss: 0.1467234343290329\n",
      "Epoch: 209 | Loss: 0.15703895688056946 | Test loss: 0.1470850557088852\n",
      "Epoch: 210 | Loss: 0.15698304772377014 | Test loss: 0.14745981991291046\n",
      "Epoch: 211 | Loss: 0.15692751109600067 | Test loss: 0.14783459901809692\n",
      "Epoch: 212 | Loss: 0.1568719893693924 | Test loss: 0.1482093334197998\n",
      "Epoch: 213 | Loss: 0.15681645274162292 | Test loss: 0.14858411252498627\n",
      "Epoch: 214 | Loss: 0.15676093101501465 | Test loss: 0.14893792569637299\n",
      "Epoch: 215 | Loss: 0.15670616924762726 | Test loss: 0.14927193522453308\n",
      "Epoch: 216 | Loss: 0.15665178000926971 | Test loss: 0.14960594475269318\n",
      "Epoch: 217 | Loss: 0.15659748017787933 | Test loss: 0.14992119371891022\n",
      "Epoch: 218 | Loss: 0.15654334425926208 | Test loss: 0.15023645758628845\n",
      "Epoch: 219 | Loss: 0.15648922324180603 | Test loss: 0.1505517214536667\n",
      "Epoch: 220 | Loss: 0.15643508732318878 | Test loss: 0.15086698532104492\n",
      "Epoch: 221 | Loss: 0.1563810557126999 | Test loss: 0.1511949598789215\n",
      "Epoch: 222 | Loss: 0.15632760524749756 | Test loss: 0.1515229344367981\n",
      "Epoch: 223 | Loss: 0.15627412497997284 | Test loss: 0.15185092389583588\n",
      "Epoch: 224 | Loss: 0.15622073411941528 | Test loss: 0.15219272673130035\n",
      "Epoch: 225 | Loss: 0.15616773068904877 | Test loss: 0.15253452956676483\n",
      "Epoch: 226 | Loss: 0.15611478686332703 | Test loss: 0.15285873413085938\n",
      "Epoch: 227 | Loss: 0.15606188774108887 | Test loss: 0.1531829535961151\n",
      "Epoch: 228 | Loss: 0.1560090184211731 | Test loss: 0.15350717306137085\n",
      "Epoch: 229 | Loss: 0.15595625340938568 | Test loss: 0.15381108224391937\n",
      "Epoch: 230 | Loss: 0.15590383112430573 | Test loss: 0.1541149914264679\n",
      "Epoch: 231 | Loss: 0.1558515727519989 | Test loss: 0.15443386137485504\n",
      "Epoch: 232 | Loss: 0.1557994782924652 | Test loss: 0.1547527015209198\n",
      "Epoch: 233 | Loss: 0.15574738383293152 | Test loss: 0.15507157146930695\n",
      "Epoch: 234 | Loss: 0.15569530427455902 | Test loss: 0.1553904265165329\n",
      "Epoch: 235 | Loss: 0.1556432545185089 | Test loss: 0.15572267770767212\n",
      "Epoch: 236 | Loss: 0.1555916965007782 | Test loss: 0.15605494379997253\n",
      "Epoch: 237 | Loss: 0.15554015338420868 | Test loss: 0.15638718008995056\n",
      "Epoch: 238 | Loss: 0.15548869967460632 | Test loss: 0.1567002385854721\n",
      "Epoch: 239 | Loss: 0.15543749928474426 | Test loss: 0.15701334178447723\n",
      "Epoch: 240 | Loss: 0.15538650751113892 | Test loss: 0.15730567276477814\n",
      "Epoch: 241 | Loss: 0.15533578395843506 | Test loss: 0.15759801864624023\n",
      "Epoch: 242 | Loss: 0.1552850753068924 | Test loss: 0.15789034962654114\n",
      "Epoch: 243 | Loss: 0.15523436665534973 | Test loss: 0.15818269550800323\n",
      "Epoch: 244 | Loss: 0.15518376231193542 | Test loss: 0.1584879755973816\n",
      "Epoch: 245 | Loss: 0.15513373911380768 | Test loss: 0.15880775451660156\n",
      "Epoch: 246 | Loss: 0.1550840437412262 | Test loss: 0.15912756323814392\n",
      "Epoch: 247 | Loss: 0.15503434836864471 | Test loss: 0.1594473272562027\n",
      "Epoch: 248 | Loss: 0.15498463809490204 | Test loss: 0.15976710617542267\n",
      "Epoch: 249 | Loss: 0.15493525564670563 | Test loss: 0.16006572544574738\n",
      "Epoch: 250 | Loss: 0.1548861712217331 | Test loss: 0.16038039326667786\n",
      "Epoch: 251 | Loss: 0.1548372358083725 | Test loss: 0.16070754826068878\n",
      "Epoch: 252 | Loss: 0.15478897094726562 | Test loss: 0.161015123128891\n",
      "Epoch: 253 | Loss: 0.1547410488128662 | Test loss: 0.1613226681947708\n",
      "Epoch: 254 | Loss: 0.154693141579628 | Test loss: 0.16163021326065063\n",
      "Epoch: 255 | Loss: 0.15464523434638977 | Test loss: 0.16193778812885284\n",
      "Epoch: 256 | Loss: 0.15459731221199036 | Test loss: 0.16225934028625488\n",
      "Epoch: 257 | Loss: 0.15454982221126556 | Test loss: 0.1625809371471405\n",
      "Epoch: 258 | Loss: 0.15450233221054077 | Test loss: 0.16290251910686493\n",
      "Epoch: 259 | Loss: 0.15445484220981598 | Test loss: 0.16322410106658936\n",
      "Epoch: 260 | Loss: 0.1544073224067688 | Test loss: 0.16354568302631378\n",
      "Epoch: 261 | Loss: 0.154359832406044 | Test loss: 0.1638672649860382\n",
      "Epoch: 262 | Loss: 0.15431247651576996 | Test loss: 0.1641688048839569\n",
      "Epoch: 263 | Loss: 0.1542654186487198 | Test loss: 0.1644703447818756\n",
      "Epoch: 264 | Loss: 0.15421836078166962 | Test loss: 0.1647718846797943\n",
      "Epoch: 265 | Loss: 0.1541714370250702 | Test loss: 0.1650870144367218\n",
      "Epoch: 266 | Loss: 0.15412484109401703 | Test loss: 0.16540215909481049\n",
      "Epoch: 267 | Loss: 0.15407834947109222 | Test loss: 0.16569922864437103\n",
      "Epoch: 268 | Loss: 0.1540319174528122 | Test loss: 0.16599631309509277\n",
      "Epoch: 269 | Loss: 0.15398551523685455 | Test loss: 0.16627287864685059\n",
      "Epoch: 270 | Loss: 0.15393954515457153 | Test loss: 0.1665494740009308\n",
      "Epoch: 271 | Loss: 0.1538936346769333 | Test loss: 0.16683922708034515\n",
      "Epoch: 272 | Loss: 0.15384823083877563 | Test loss: 0.16712898015975952\n",
      "Epoch: 273 | Loss: 0.15380294620990753 | Test loss: 0.16743431985378265\n",
      "Epoch: 274 | Loss: 0.15375776588916779 | Test loss: 0.16773968935012817\n",
      "Epoch: 275 | Loss: 0.15371257066726685 | Test loss: 0.1680450439453125\n",
      "Epoch: 276 | Loss: 0.1536676436662674 | Test loss: 0.16834217309951782\n",
      "Epoch: 277 | Loss: 0.15362359583377838 | Test loss: 0.16863928735256195\n",
      "Epoch: 278 | Loss: 0.15357956290245056 | Test loss: 0.16893641650676727\n",
      "Epoch: 279 | Loss: 0.15353551506996155 | Test loss: 0.1692335456609726\n",
      "Epoch: 280 | Loss: 0.15349146723747253 | Test loss: 0.16953067481517792\n",
      "Epoch: 281 | Loss: 0.15344743430614471 | Test loss: 0.16982778906822205\n",
      "Epoch: 282 | Loss: 0.1534033864736557 | Test loss: 0.17012490332126617\n",
      "Epoch: 283 | Loss: 0.15335935354232788 | Test loss: 0.1704220473766327\n",
      "Epoch: 284 | Loss: 0.15331536531448364 | Test loss: 0.1707158237695694\n",
      "Epoch: 285 | Loss: 0.15327177941799164 | Test loss: 0.1710096150636673\n",
      "Epoch: 286 | Loss: 0.15322819352149963 | Test loss: 0.1713034063577652\n",
      "Epoch: 287 | Loss: 0.15318462252616882 | Test loss: 0.1715971976518631\n",
      "Epoch: 288 | Loss: 0.15314102172851562 | Test loss: 0.1718909740447998\n",
      "Epoch: 289 | Loss: 0.15309745073318481 | Test loss: 0.1721847504377365\n",
      "Epoch: 290 | Loss: 0.1530538648366928 | Test loss: 0.1724785566329956\n",
      "Epoch: 291 | Loss: 0.1530102640390396 | Test loss: 0.1727723330259323\n",
      "Epoch: 292 | Loss: 0.15296682715415955 | Test loss: 0.17308086156845093\n",
      "Epoch: 293 | Loss: 0.1529236137866974 | Test loss: 0.1733703762292862\n",
      "Epoch: 294 | Loss: 0.15288062393665314 | Test loss: 0.17365993559360504\n",
      "Epoch: 295 | Loss: 0.1528376191854477 | Test loss: 0.1739494800567627\n",
      "Epoch: 296 | Loss: 0.15279459953308105 | Test loss: 0.17423903942108154\n",
      "Epoch: 297 | Loss: 0.15275177359580994 | Test loss: 0.1745428442955017\n",
      "Epoch: 298 | Loss: 0.15270912647247314 | Test loss: 0.17484667897224426\n",
      "Epoch: 299 | Loss: 0.15266665816307068 | Test loss: 0.17513111233711243\n",
      "Epoch: 300 | Loss: 0.15262436866760254 | Test loss: 0.17542940378189087\n",
      "Epoch: 301 | Loss: 0.15258251130580902 | Test loss: 0.1757277101278305\n",
      "Epoch: 302 | Loss: 0.1525406837463379 | Test loss: 0.17600612342357635\n",
      "Epoch: 303 | Loss: 0.152499258518219 | Test loss: 0.17629794776439667\n",
      "Epoch: 304 | Loss: 0.15245823562145233 | Test loss: 0.17658977210521698\n",
      "Epoch: 305 | Loss: 0.1524173468351364 | Test loss: 0.1768742948770523\n",
      "Epoch: 306 | Loss: 0.15237732231616974 | Test loss: 0.17715877294540405\n",
      "Epoch: 307 | Loss: 0.1523374319076538 | Test loss: 0.1774350106716156\n",
      "Epoch: 308 | Loss: 0.1522984802722931 | Test loss: 0.17771124839782715\n",
      "Epoch: 309 | Loss: 0.15225964784622192 | Test loss: 0.17796632647514343\n",
      "Epoch: 310 | Loss: 0.1522211730480194 | Test loss: 0.17822137475013733\n",
      "Epoch: 311 | Loss: 0.15218272805213928 | Test loss: 0.17847643792629242\n",
      "Epoch: 312 | Loss: 0.15214428305625916 | Test loss: 0.1787315011024475\n",
      "Epoch: 313 | Loss: 0.15210582315921783 | Test loss: 0.1789865642786026\n",
      "Epoch: 314 | Loss: 0.1520673781633377 | Test loss: 0.1792415976524353\n",
      "Epoch: 315 | Loss: 0.1520289033651352 | Test loss: 0.17949669063091278\n",
      "Epoch: 316 | Loss: 0.15199045836925507 | Test loss: 0.17975173890590668\n",
      "Epoch: 317 | Loss: 0.15195199847221375 | Test loss: 0.18000678718090057\n",
      "Epoch: 318 | Loss: 0.15191353857517242 | Test loss: 0.18026185035705566\n",
      "Epoch: 319 | Loss: 0.1518750935792923 | Test loss: 0.18051692843437195\n",
      "Epoch: 320 | Loss: 0.15183661878108978 | Test loss: 0.18077197670936584\n",
      "Epoch: 321 | Loss: 0.15179817378520966 | Test loss: 0.18102702498435974\n",
      "Epoch: 322 | Loss: 0.15175972878932953 | Test loss: 0.18128210306167603\n",
      "Epoch: 323 | Loss: 0.1517212688922882 | Test loss: 0.18153715133666992\n",
      "Epoch: 324 | Loss: 0.1516828089952469 | Test loss: 0.181792214512825\n",
      "Epoch: 325 | Loss: 0.15164436399936676 | Test loss: 0.1820472627878189\n",
      "Epoch: 326 | Loss: 0.15160590410232544 | Test loss: 0.1823023110628128\n",
      "Epoch: 327 | Loss: 0.15156742930412292 | Test loss: 0.18255740404129028\n",
      "Epoch: 328 | Loss: 0.1515289843082428 | Test loss: 0.18281245231628418\n",
      "Epoch: 329 | Loss: 0.15149053931236267 | Test loss: 0.18306751549243927\n",
      "Epoch: 330 | Loss: 0.15145207941532135 | Test loss: 0.18332257866859436\n",
      "Epoch: 331 | Loss: 0.15141361951828003 | Test loss: 0.18357761204242706\n",
      "Epoch: 332 | Loss: 0.1513751596212387 | Test loss: 0.18383270502090454\n",
      "Epoch: 333 | Loss: 0.1513366997241974 | Test loss: 0.18408775329589844\n",
      "Epoch: 334 | Loss: 0.15129825472831726 | Test loss: 0.18434281647205353\n",
      "Epoch: 335 | Loss: 0.15125979483127594 | Test loss: 0.18459784984588623\n",
      "Epoch: 336 | Loss: 0.15122154355049133 | Test loss: 0.18486568331718445\n",
      "Epoch: 337 | Loss: 0.15118376910686493 | Test loss: 0.18514667451381683\n",
      "Epoch: 338 | Loss: 0.15114638209342957 | Test loss: 0.18542762100696564\n",
      "Epoch: 339 | Loss: 0.15110914409160614 | Test loss: 0.185722216963768\n",
      "Epoch: 340 | Loss: 0.151072159409523 | Test loss: 0.18601679801940918\n",
      "Epoch: 341 | Loss: 0.15103520452976227 | Test loss: 0.18632543087005615\n",
      "Epoch: 342 | Loss: 0.1509985327720642 | Test loss: 0.18663403391838074\n",
      "Epoch: 343 | Loss: 0.15096187591552734 | Test loss: 0.1869426816701889\n",
      "Epoch: 344 | Loss: 0.15092521905899048 | Test loss: 0.1872512847185135\n",
      "Epoch: 345 | Loss: 0.15088874101638794 | Test loss: 0.18755348026752472\n",
      "Epoch: 346 | Loss: 0.15085290372371674 | Test loss: 0.18785566091537476\n",
      "Epoch: 347 | Loss: 0.1508171558380127 | Test loss: 0.18813733756542206\n",
      "Epoch: 348 | Loss: 0.15078182518482208 | Test loss: 0.18841901421546936\n",
      "Epoch: 349 | Loss: 0.15074646472930908 | Test loss: 0.18870067596435547\n",
      "Epoch: 350 | Loss: 0.15071114897727966 | Test loss: 0.18896228075027466\n",
      "Epoch: 351 | Loss: 0.15067622065544128 | Test loss: 0.18923886120319366\n",
      "Epoch: 352 | Loss: 0.150641530752182 | Test loss: 0.18951541185379028\n",
      "Epoch: 353 | Loss: 0.15060682594776154 | Test loss: 0.1897919774055481\n",
      "Epoch: 354 | Loss: 0.15057210624217987 | Test loss: 0.1900685429573059\n",
      "Epoch: 355 | Loss: 0.15053752064704895 | Test loss: 0.19032546877861023\n",
      "Epoch: 356 | Loss: 0.15050312876701355 | Test loss: 0.19058242440223694\n",
      "Epoch: 357 | Loss: 0.15046875178813934 | Test loss: 0.19083935022354126\n",
      "Epoch: 358 | Loss: 0.15043435990810394 | Test loss: 0.1910962611436844\n",
      "Epoch: 359 | Loss: 0.15040001273155212 | Test loss: 0.19136860966682434\n",
      "Epoch: 360 | Loss: 0.15036582946777344 | Test loss: 0.1916409581899643\n",
      "Epoch: 361 | Loss: 0.15033164620399475 | Test loss: 0.19189411401748657\n",
      "Epoch: 362 | Loss: 0.15029773116111755 | Test loss: 0.19214725494384766\n",
      "Epoch: 363 | Loss: 0.15026380121707916 | Test loss: 0.19240039587020874\n",
      "Epoch: 364 | Loss: 0.15022987127304077 | Test loss: 0.19265349209308624\n",
      "Epoch: 365 | Loss: 0.15019594132900238 | Test loss: 0.1929066777229309\n",
      "Epoch: 366 | Loss: 0.150162011384964 | Test loss: 0.193159818649292\n",
      "Epoch: 367 | Loss: 0.1501280963420868 | Test loss: 0.1934254765510559\n",
      "Epoch: 368 | Loss: 0.15009471774101257 | Test loss: 0.19369113445281982\n",
      "Epoch: 369 | Loss: 0.15006133913993835 | Test loss: 0.19395679235458374\n",
      "Epoch: 370 | Loss: 0.15002796053886414 | Test loss: 0.19422245025634766\n",
      "Epoch: 371 | Loss: 0.14999458193778992 | Test loss: 0.19448810815811157\n",
      "Epoch: 372 | Loss: 0.14996129274368286 | Test loss: 0.19473502039909363\n",
      "Epoch: 373 | Loss: 0.14992839097976685 | Test loss: 0.1950107216835022\n",
      "Epoch: 374 | Loss: 0.1498958170413971 | Test loss: 0.19528642296791077\n",
      "Epoch: 375 | Loss: 0.14986324310302734 | Test loss: 0.19556212425231934\n",
      "Epoch: 376 | Loss: 0.1498306840658188 | Test loss: 0.1958378553390503\n",
      "Epoch: 377 | Loss: 0.1497981995344162 | Test loss: 0.1960923671722412\n",
      "Epoch: 378 | Loss: 0.14976614713668823 | Test loss: 0.1963469237089157\n",
      "Epoch: 379 | Loss: 0.14973416924476624 | Test loss: 0.1966148316860199\n",
      "Epoch: 380 | Loss: 0.1497025191783905 | Test loss: 0.19688273966312408\n",
      "Epoch: 381 | Loss: 0.14967089891433716 | Test loss: 0.19715064764022827\n",
      "Epoch: 382 | Loss: 0.14963924884796143 | Test loss: 0.19741857051849365\n",
      "Epoch: 383 | Loss: 0.1496075987815857 | Test loss: 0.19768646359443665\n",
      "Epoch: 384 | Loss: 0.14957618713378906 | Test loss: 0.1979336440563202\n",
      "Epoch: 385 | Loss: 0.14954498410224915 | Test loss: 0.19818086922168732\n",
      "Epoch: 386 | Loss: 0.14951381087303162 | Test loss: 0.19842804968357086\n",
      "Epoch: 387 | Loss: 0.1494826227426529 | Test loss: 0.19865696132183075\n",
      "Epoch: 388 | Loss: 0.14945170283317566 | Test loss: 0.19889968633651733\n",
      "Epoch: 389 | Loss: 0.14942102134227753 | Test loss: 0.1991424560546875\n",
      "Epoch: 390 | Loss: 0.1493903398513794 | Test loss: 0.19938518106937408\n",
      "Epoch: 391 | Loss: 0.14935967326164246 | Test loss: 0.19962793588638306\n",
      "Epoch: 392 | Loss: 0.14932900667190552 | Test loss: 0.19985035061836243\n",
      "Epoch: 393 | Loss: 0.14929866790771484 | Test loss: 0.200072780251503\n",
      "Epoch: 394 | Loss: 0.14926831424236298 | Test loss: 0.20029520988464355\n",
      "Epoch: 395 | Loss: 0.1492379903793335 | Test loss: 0.2005176544189453\n",
      "Epoch: 396 | Loss: 0.14920765161514282 | Test loss: 0.2007400393486023\n",
      "Epoch: 397 | Loss: 0.14917731285095215 | Test loss: 0.20096248388290405\n",
      "Epoch: 398 | Loss: 0.14914709329605103 | Test loss: 0.2011992335319519\n",
      "Epoch: 399 | Loss: 0.1491170972585678 | Test loss: 0.20143598318099976\n",
      "Epoch: 400 | Loss: 0.1490871161222458 | Test loss: 0.2016727179288864\n",
      "Epoch: 401 | Loss: 0.14905712008476257 | Test loss: 0.20190948247909546\n",
      "Epoch: 402 | Loss: 0.14902713894844055 | Test loss: 0.20214621722698212\n",
      "Epoch: 403 | Loss: 0.14899715781211853 | Test loss: 0.20239922404289246\n",
      "Epoch: 404 | Loss: 0.14896726608276367 | Test loss: 0.202652245759964\n",
      "Epoch: 405 | Loss: 0.14893758296966553 | Test loss: 0.20289812982082367\n",
      "Epoch: 406 | Loss: 0.14890848100185394 | Test loss: 0.20314399898052216\n",
      "Epoch: 407 | Loss: 0.14887937903404236 | Test loss: 0.20338988304138184\n",
      "Epoch: 408 | Loss: 0.14885026216506958 | Test loss: 0.2036357820034027\n",
      "Epoch: 409 | Loss: 0.1488211750984192 | Test loss: 0.20388168096542358\n",
      "Epoch: 410 | Loss: 0.1487920731306076 | Test loss: 0.20412757992744446\n",
      "Epoch: 411 | Loss: 0.14876297116279602 | Test loss: 0.20437346398830414\n",
      "Epoch: 412 | Loss: 0.14873386919498444 | Test loss: 0.2046193778514862\n",
      "Epoch: 413 | Loss: 0.14870481193065643 | Test loss: 0.2048799991607666\n",
      "Epoch: 414 | Loss: 0.14867594838142395 | Test loss: 0.20514065027236938\n",
      "Epoch: 415 | Loss: 0.14864717423915863 | Test loss: 0.20541442930698395\n",
      "Epoch: 416 | Loss: 0.148618683218956 | Test loss: 0.2056882381439209\n",
      "Epoch: 417 | Loss: 0.14859019219875336 | Test loss: 0.20596201717853546\n",
      "Epoch: 418 | Loss: 0.14856170117855072 | Test loss: 0.20623581111431122\n",
      "Epoch: 419 | Loss: 0.14853334426879883 | Test loss: 0.20648863911628723\n",
      "Epoch: 420 | Loss: 0.14850541949272156 | Test loss: 0.20672206580638885\n",
      "Epoch: 421 | Loss: 0.1484777182340622 | Test loss: 0.20695549249649048\n",
      "Epoch: 422 | Loss: 0.14845000207424164 | Test loss: 0.2071889191865921\n",
      "Epoch: 423 | Loss: 0.1484222710132599 | Test loss: 0.20742233097553253\n",
      "Epoch: 424 | Loss: 0.14839455485343933 | Test loss: 0.20765577256679535\n",
      "Epoch: 425 | Loss: 0.14836686849594116 | Test loss: 0.20787137746810913\n",
      "Epoch: 426 | Loss: 0.14833921194076538 | Test loss: 0.20808696746826172\n",
      "Epoch: 427 | Loss: 0.1483115553855896 | Test loss: 0.2083025872707367\n",
      "Epoch: 428 | Loss: 0.14828389883041382 | Test loss: 0.20853181183338165\n",
      "Epoch: 429 | Loss: 0.14825664460659027 | Test loss: 0.20876102149486542\n",
      "Epoch: 430 | Loss: 0.14822939038276672 | Test loss: 0.2089902013540268\n",
      "Epoch: 431 | Loss: 0.14820213615894318 | Test loss: 0.20921942591667175\n",
      "Epoch: 432 | Loss: 0.14817486703395844 | Test loss: 0.20944862067699432\n",
      "Epoch: 433 | Loss: 0.14814774692058563 | Test loss: 0.20965735614299774\n",
      "Epoch: 434 | Loss: 0.14812085032463074 | Test loss: 0.20986606180667877\n",
      "Epoch: 435 | Loss: 0.14809392392635345 | Test loss: 0.2100747674703598\n",
      "Epoch: 436 | Loss: 0.14806725084781647 | Test loss: 0.21029597520828247\n",
      "Epoch: 437 | Loss: 0.14804090559482574 | Test loss: 0.21053233742713928\n",
      "Epoch: 438 | Loss: 0.14801470935344696 | Test loss: 0.21076872944831848\n",
      "Epoch: 439 | Loss: 0.14798851311206818 | Test loss: 0.2110050916671753\n",
      "Epoch: 440 | Loss: 0.1479623168706894 | Test loss: 0.2112414538860321\n",
      "Epoch: 441 | Loss: 0.1479361206293106 | Test loss: 0.21147781610488892\n",
      "Epoch: 442 | Loss: 0.14790992438793182 | Test loss: 0.21171416342258453\n",
      "Epoch: 443 | Loss: 0.14788372814655304 | Test loss: 0.21195054054260254\n",
      "Epoch: 444 | Loss: 0.14785753190517426 | Test loss: 0.21218690276145935\n",
      "Epoch: 445 | Loss: 0.14783133566379547 | Test loss: 0.21242327988147736\n",
      "Epoch: 446 | Loss: 0.1478053480386734 | Test loss: 0.21265476942062378\n",
      "Epoch: 447 | Loss: 0.1477796733379364 | Test loss: 0.2128862589597702\n",
      "Epoch: 448 | Loss: 0.1477540135383606 | Test loss: 0.21311774849891663\n",
      "Epoch: 449 | Loss: 0.1477283388376236 | Test loss: 0.2133621722459793\n",
      "Epoch: 450 | Loss: 0.1477031111717224 | Test loss: 0.21358653903007507\n",
      "Epoch: 451 | Loss: 0.1476781815290451 | Test loss: 0.21381087601184845\n",
      "Epoch: 452 | Loss: 0.1476532369852066 | Test loss: 0.2140352427959442\n",
      "Epoch: 453 | Loss: 0.14762847125530243 | Test loss: 0.21423839032649994\n",
      "Epoch: 454 | Loss: 0.14760395884513855 | Test loss: 0.21444156765937805\n",
      "Epoch: 455 | Loss: 0.14757943153381348 | Test loss: 0.21464471518993378\n",
      "Epoch: 456 | Loss: 0.1475549042224884 | Test loss: 0.21484790742397308\n",
      "Epoch: 457 | Loss: 0.14753039181232452 | Test loss: 0.21505104005336761\n",
      "Epoch: 458 | Loss: 0.14750586450099945 | Test loss: 0.21525420248508453\n",
      "Epoch: 459 | Loss: 0.14748135209083557 | Test loss: 0.21545737981796265\n",
      "Epoch: 460 | Loss: 0.1474568396806717 | Test loss: 0.21566054224967957\n",
      "Epoch: 461 | Loss: 0.14743229746818542 | Test loss: 0.2158636748790741\n",
      "Epoch: 462 | Loss: 0.14740778505802155 | Test loss: 0.2160668522119522\n",
      "Epoch: 463 | Loss: 0.14738327264785767 | Test loss: 0.21627002954483032\n",
      "Epoch: 464 | Loss: 0.1473587602376938 | Test loss: 0.21647319197654724\n",
      "Epoch: 465 | Loss: 0.1473342329263687 | Test loss: 0.21667632460594177\n",
      "Epoch: 466 | Loss: 0.14730985462665558 | Test loss: 0.21689292788505554\n",
      "Epoch: 467 | Loss: 0.14728572964668274 | Test loss: 0.21710950136184692\n",
      "Epoch: 468 | Loss: 0.1472616046667099 | Test loss: 0.21734055876731873\n",
      "Epoch: 469 | Loss: 0.14723773300647736 | Test loss: 0.21757163107395172\n",
      "Epoch: 470 | Loss: 0.1472138613462448 | Test loss: 0.21780267357826233\n",
      "Epoch: 471 | Loss: 0.14718997478485107 | Test loss: 0.21803374588489532\n",
      "Epoch: 472 | Loss: 0.14716626703739166 | Test loss: 0.21824409067630768\n",
      "Epoch: 473 | Loss: 0.14714276790618896 | Test loss: 0.21845443546772003\n",
      "Epoch: 474 | Loss: 0.14711926877498627 | Test loss: 0.21866478025913239\n",
      "Epoch: 475 | Loss: 0.14709576964378357 | Test loss: 0.21887511014938354\n",
      "Epoch: 476 | Loss: 0.14707233011722565 | Test loss: 0.21910105645656586\n",
      "Epoch: 477 | Loss: 0.14704903960227966 | Test loss: 0.21930739283561707\n",
      "Epoch: 478 | Loss: 0.1470259428024292 | Test loss: 0.21951372921466827\n",
      "Epoch: 479 | Loss: 0.14700286090373993 | Test loss: 0.21972011029720306\n",
      "Epoch: 480 | Loss: 0.14697976410388947 | Test loss: 0.21992644667625427\n",
      "Epoch: 481 | Loss: 0.14695681631565094 | Test loss: 0.22014549374580383\n",
      "Epoch: 482 | Loss: 0.1469341367483139 | Test loss: 0.2203645259141922\n",
      "Epoch: 483 | Loss: 0.14691148698329926 | Test loss: 0.22058357298374176\n",
      "Epoch: 484 | Loss: 0.1468888223171234 | Test loss: 0.22080260515213013\n",
      "Epoch: 485 | Loss: 0.14686614274978638 | Test loss: 0.2210216522216797\n",
      "Epoch: 486 | Loss: 0.14684349298477173 | Test loss: 0.22124068439006805\n",
      "Epoch: 487 | Loss: 0.1468208283185959 | Test loss: 0.22145973145961761\n",
      "Epoch: 488 | Loss: 0.14679814875125885 | Test loss: 0.22167876362800598\n",
      "Epoch: 489 | Loss: 0.14677561819553375 | Test loss: 0.22191165387630463\n",
      "Epoch: 490 | Loss: 0.14675328135490417 | Test loss: 0.2221260368824005\n",
      "Epoch: 491 | Loss: 0.14673109352588654 | Test loss: 0.2223404198884964\n",
      "Epoch: 492 | Loss: 0.1467088758945465 | Test loss: 0.2225547730922699\n",
      "Epoch: 493 | Loss: 0.14668668806552887 | Test loss: 0.22276915609836578\n",
      "Epoch: 494 | Loss: 0.14666448533535004 | Test loss: 0.22298352420330048\n",
      "Epoch: 495 | Loss: 0.14664243161678314 | Test loss: 0.22317761182785034\n",
      "Epoch: 496 | Loss: 0.14662052690982819 | Test loss: 0.2233717143535614\n",
      "Epoch: 497 | Loss: 0.14659863710403442 | Test loss: 0.22356578707695007\n",
      "Epoch: 498 | Loss: 0.14657673239707947 | Test loss: 0.22375987470149994\n",
      "Epoch: 499 | Loss: 0.1465548276901245 | Test loss: 0.2239539623260498\n",
      "Epoch: 500 | Loss: 0.14653293788433075 | Test loss: 0.22414806485176086\n",
      "Epoch: 501 | Loss: 0.1465110331773758 | Test loss: 0.22434215247631073\n",
      "Epoch: 502 | Loss: 0.14648929238319397 | Test loss: 0.22454939782619476\n",
      "Epoch: 503 | Loss: 0.14646779000759125 | Test loss: 0.22475668787956238\n",
      "Epoch: 504 | Loss: 0.1464463174343109 | Test loss: 0.22497889399528503\n",
      "Epoch: 505 | Loss: 0.14642499387264252 | Test loss: 0.2252010703086853\n",
      "Epoch: 506 | Loss: 0.14640365540981293 | Test loss: 0.22542327642440796\n",
      "Epoch: 507 | Loss: 0.1463824063539505 | Test loss: 0.22562451660633087\n",
      "Epoch: 508 | Loss: 0.14636147022247314 | Test loss: 0.22582575678825378\n",
      "Epoch: 509 | Loss: 0.1463405340909958 | Test loss: 0.2260270118713379\n",
      "Epoch: 510 | Loss: 0.14631961286067963 | Test loss: 0.2262282371520996\n",
      "Epoch: 511 | Loss: 0.14629867672920227 | Test loss: 0.22644196450710297\n",
      "Epoch: 512 | Loss: 0.14627817273139954 | Test loss: 0.2266557216644287\n",
      "Epoch: 513 | Loss: 0.1462576687335968 | Test loss: 0.22686944901943207\n",
      "Epoch: 514 | Loss: 0.14623719453811646 | Test loss: 0.22706401348114014\n",
      "Epoch: 515 | Loss: 0.14621688425540924 | Test loss: 0.22725854814052582\n",
      "Epoch: 516 | Loss: 0.14619658887386322 | Test loss: 0.2274531126022339\n",
      "Epoch: 517 | Loss: 0.146176278591156 | Test loss: 0.22764766216278076\n",
      "Epoch: 518 | Loss: 0.14615598320960999 | Test loss: 0.22784222662448883\n",
      "Epoch: 519 | Loss: 0.14613567292690277 | Test loss: 0.2280368059873581\n",
      "Epoch: 520 | Loss: 0.14611545205116272 | Test loss: 0.22824566066265106\n",
      "Epoch: 521 | Loss: 0.146095409989357 | Test loss: 0.22845448553562164\n",
      "Epoch: 522 | Loss: 0.14607535302639008 | Test loss: 0.2286633402109146\n",
      "Epoch: 523 | Loss: 0.14605529606342316 | Test loss: 0.22887219488620758\n",
      "Epoch: 524 | Loss: 0.14603525400161743 | Test loss: 0.22908104956150055\n",
      "Epoch: 525 | Loss: 0.1460151970386505 | Test loss: 0.22928988933563232\n",
      "Epoch: 526 | Loss: 0.1459951549768448 | Test loss: 0.2294987440109253\n",
      "Epoch: 527 | Loss: 0.14597520232200623 | Test loss: 0.22968776524066925\n",
      "Epoch: 528 | Loss: 0.1459554135799408 | Test loss: 0.2298767864704132\n",
      "Epoch: 529 | Loss: 0.14593560993671417 | Test loss: 0.23006579279899597\n",
      "Epoch: 530 | Loss: 0.14591597020626068 | Test loss: 0.2302684336900711\n",
      "Epoch: 531 | Loss: 0.14589650928974152 | Test loss: 0.23047101497650146\n",
      "Epoch: 532 | Loss: 0.14587703347206116 | Test loss: 0.2306736260652542\n",
      "Epoch: 533 | Loss: 0.1458575576543808 | Test loss: 0.23087623715400696\n",
      "Epoch: 534 | Loss: 0.14583809673786163 | Test loss: 0.2310788333415985\n",
      "Epoch: 535 | Loss: 0.14581872522830963 | Test loss: 0.23126094043254852\n",
      "Epoch: 536 | Loss: 0.1457996368408203 | Test loss: 0.2314559817314148\n",
      "Epoch: 537 | Loss: 0.14578084647655487 | Test loss: 0.23165103793144226\n",
      "Epoch: 538 | Loss: 0.14576207101345062 | Test loss: 0.23184607923030853\n",
      "Epoch: 539 | Loss: 0.14574329555034637 | Test loss: 0.2320411205291748\n",
      "Epoch: 540 | Loss: 0.14572452008724213 | Test loss: 0.23223617672920227\n",
      "Epoch: 541 | Loss: 0.1457059234380722 | Test loss: 0.23241004347801208\n",
      "Epoch: 542 | Loss: 0.1456875056028366 | Test loss: 0.2326006442308426\n",
      "Epoch: 543 | Loss: 0.1456691175699234 | Test loss: 0.2327912300825119\n",
      "Epoch: 544 | Loss: 0.1456507295370102 | Test loss: 0.2329818308353424\n",
      "Epoch: 545 | Loss: 0.14563235640525818 | Test loss: 0.23317241668701172\n",
      "Epoch: 546 | Loss: 0.14561396837234497 | Test loss: 0.23336298763751984\n",
      "Epoch: 547 | Loss: 0.14559559524059296 | Test loss: 0.23355358839035034\n",
      "Epoch: 548 | Loss: 0.14557722210884094 | Test loss: 0.23374418914318085\n",
      "Epoch: 549 | Loss: 0.14555883407592773 | Test loss: 0.23393476009368896\n",
      "Epoch: 550 | Loss: 0.14554044604301453 | Test loss: 0.23412536084651947\n",
      "Epoch: 551 | Loss: 0.14552205801010132 | Test loss: 0.23431594669818878\n",
      "Epoch: 552 | Loss: 0.1455036848783493 | Test loss: 0.2345065474510193\n",
      "Epoch: 553 | Loss: 0.1454852968454361 | Test loss: 0.2346971333026886\n",
      "Epoch: 554 | Loss: 0.14546692371368408 | Test loss: 0.2348877340555191\n",
      "Epoch: 555 | Loss: 0.14544853568077087 | Test loss: 0.23507830500602722\n",
      "Epoch: 556 | Loss: 0.14543016254901886 | Test loss: 0.23526889085769653\n",
      "Epoch: 557 | Loss: 0.14541177451610565 | Test loss: 0.23545947670936584\n",
      "Epoch: 558 | Loss: 0.14539338648319244 | Test loss: 0.23565009236335754\n",
      "Epoch: 559 | Loss: 0.14537501335144043 | Test loss: 0.23584066331386566\n",
      "Epoch: 560 | Loss: 0.14535664021968842 | Test loss: 0.23603126406669617\n",
      "Epoch: 561 | Loss: 0.1453382819890976 | Test loss: 0.23623792827129364\n",
      "Epoch: 562 | Loss: 0.14531996846199036 | Test loss: 0.2364446222782135\n",
      "Epoch: 563 | Loss: 0.1453016698360443 | Test loss: 0.2366512566804886\n",
      "Epoch: 564 | Loss: 0.14528335630893707 | Test loss: 0.23685796558856964\n",
      "Epoch: 565 | Loss: 0.14526505768299103 | Test loss: 0.23707999289035797\n",
      "Epoch: 566 | Loss: 0.14524683356285095 | Test loss: 0.2373020499944687\n",
      "Epoch: 567 | Loss: 0.14522863924503326 | Test loss: 0.23753884434700012\n",
      "Epoch: 568 | Loss: 0.14521057903766632 | Test loss: 0.23778967559337616\n",
      "Epoch: 569 | Loss: 0.14519278705120087 | Test loss: 0.2380666434764862\n",
      "Epoch: 570 | Loss: 0.14517532289028168 | Test loss: 0.2382834255695343\n",
      "Epoch: 571 | Loss: 0.14515918493270874 | Test loss: 0.23844599723815918\n",
      "Epoch: 572 | Loss: 0.14514319598674774 | Test loss: 0.23860853910446167\n",
      "Epoch: 573 | Loss: 0.14512722194194794 | Test loss: 0.23877112567424774\n",
      "Epoch: 574 | Loss: 0.14511123299598694 | Test loss: 0.23893369734287262\n",
      "Epoch: 575 | Loss: 0.14509527385234833 | Test loss: 0.2390962541103363\n",
      "Epoch: 576 | Loss: 0.14507927000522614 | Test loss: 0.2392762303352356\n",
      "Epoch: 577 | Loss: 0.14506329596042633 | Test loss: 0.23943881690502167\n",
      "Epoch: 578 | Loss: 0.14504732191562653 | Test loss: 0.23960135877132416\n",
      "Epoch: 579 | Loss: 0.14503133296966553 | Test loss: 0.23976394534111023\n",
      "Epoch: 580 | Loss: 0.14501534402370453 | Test loss: 0.23992648720741272\n",
      "Epoch: 581 | Loss: 0.14499936997890472 | Test loss: 0.2400890737771988\n",
      "Epoch: 582 | Loss: 0.14498338103294373 | Test loss: 0.24025163054466248\n",
      "Epoch: 583 | Loss: 0.14496740698814392 | Test loss: 0.24041418731212616\n",
      "Epoch: 584 | Loss: 0.14495143294334412 | Test loss: 0.24059417843818665\n",
      "Epoch: 585 | Loss: 0.14493544399738312 | Test loss: 0.24075673520565033\n",
      "Epoch: 586 | Loss: 0.14491945505142212 | Test loss: 0.2409193217754364\n",
      "Epoch: 587 | Loss: 0.14490348100662231 | Test loss: 0.24108187854290009\n",
      "Epoch: 588 | Loss: 0.1448875069618225 | Test loss: 0.24124445021152496\n",
      "Epoch: 589 | Loss: 0.1448715180158615 | Test loss: 0.24140700697898865\n",
      "Epoch: 590 | Loss: 0.1448555290699005 | Test loss: 0.24156959354877472\n",
      "Epoch: 591 | Loss: 0.1448395550251007 | Test loss: 0.2417321503162384\n",
      "Epoch: 592 | Loss: 0.1448235809803009 | Test loss: 0.2419121116399765\n",
      "Epoch: 593 | Loss: 0.1448075920343399 | Test loss: 0.24207469820976257\n",
      "Epoch: 594 | Loss: 0.1447916030883789 | Test loss: 0.24223726987838745\n",
      "Epoch: 595 | Loss: 0.1447756290435791 | Test loss: 0.24239982664585114\n",
      "Epoch: 596 | Loss: 0.1447596400976181 | Test loss: 0.24256236851215363\n",
      "Epoch: 597 | Loss: 0.1447436660528183 | Test loss: 0.2427249401807785\n",
      "Epoch: 598 | Loss: 0.1447276920080185 | Test loss: 0.24288752675056458\n",
      "Epoch: 599 | Loss: 0.14471183717250824 | Test loss: 0.2430626004934311\n",
      "Epoch: 600 | Loss: 0.14469625055789948 | Test loss: 0.2432377189397812\n",
      "Epoch: 601 | Loss: 0.14468064904212952 | Test loss: 0.24341276288032532\n",
      "Epoch: 602 | Loss: 0.14466506242752075 | Test loss: 0.24358786642551422\n",
      "Epoch: 603 | Loss: 0.14464949071407318 | Test loss: 0.24376292526721954\n",
      "Epoch: 604 | Loss: 0.14463399350643158 | Test loss: 0.24395118653774261\n",
      "Epoch: 605 | Loss: 0.14461871981620789 | Test loss: 0.2441394180059433\n",
      "Epoch: 606 | Loss: 0.1446034461259842 | Test loss: 0.2443276345729828\n",
      "Epoch: 607 | Loss: 0.1445881426334381 | Test loss: 0.24451586604118347\n",
      "Epoch: 608 | Loss: 0.14457286894321442 | Test loss: 0.2446831911802292\n",
      "Epoch: 609 | Loss: 0.1445579081773758 | Test loss: 0.24485044181346893\n",
      "Epoch: 610 | Loss: 0.14454296231269836 | Test loss: 0.24501773715019226\n",
      "Epoch: 611 | Loss: 0.14452803134918213 | Test loss: 0.24519886076450348\n",
      "Epoch: 612 | Loss: 0.1445133239030838 | Test loss: 0.2453799545764923\n",
      "Epoch: 613 | Loss: 0.14449861645698547 | Test loss: 0.24556107819080353\n",
      "Epoch: 614 | Loss: 0.14448390901088715 | Test loss: 0.24574218690395355\n",
      "Epoch: 615 | Loss: 0.14446920156478882 | Test loss: 0.24592331051826477\n",
      "Epoch: 616 | Loss: 0.14445453882217407 | Test loss: 0.24608413875102997\n",
      "Epoch: 617 | Loss: 0.14444008469581604 | Test loss: 0.24624499678611755\n",
      "Epoch: 618 | Loss: 0.144425630569458 | Test loss: 0.24640581011772156\n",
      "Epoch: 619 | Loss: 0.14441117644309998 | Test loss: 0.24656666815280914\n",
      "Epoch: 620 | Loss: 0.14439673721790314 | Test loss: 0.24672749638557434\n",
      "Epoch: 621 | Loss: 0.1443822979927063 | Test loss: 0.24688833951950073\n",
      "Epoch: 622 | Loss: 0.14436785876750946 | Test loss: 0.24706366658210754\n",
      "Epoch: 623 | Loss: 0.14435361325740814 | Test loss: 0.24723899364471436\n",
      "Epoch: 624 | Loss: 0.14433936774730682 | Test loss: 0.24741433560848236\n",
      "Epoch: 625 | Loss: 0.1443251222372055 | Test loss: 0.24758967757225037\n",
      "Epoch: 626 | Loss: 0.1443108767271042 | Test loss: 0.24776498973369598\n",
      "Epoch: 627 | Loss: 0.14429663121700287 | Test loss: 0.247940331697464\n",
      "Epoch: 628 | Loss: 0.14428237080574036 | Test loss: 0.2481156438589096\n",
      "Epoch: 629 | Loss: 0.144268199801445 | Test loss: 0.2482713907957077\n",
      "Epoch: 630 | Loss: 0.1442541480064392 | Test loss: 0.2484271228313446\n",
      "Epoch: 631 | Loss: 0.1442400962114334 | Test loss: 0.24858281016349792\n",
      "Epoch: 632 | Loss: 0.1442260593175888 | Test loss: 0.24873855710029602\n",
      "Epoch: 633 | Loss: 0.144212007522583 | Test loss: 0.24889425933361053\n",
      "Epoch: 634 | Loss: 0.1441979557275772 | Test loss: 0.24904997646808624\n",
      "Epoch: 635 | Loss: 0.1441839039325714 | Test loss: 0.24920569360256195\n",
      "Epoch: 636 | Loss: 0.1441698670387268 | Test loss: 0.24936141073703766\n",
      "Epoch: 637 | Loss: 0.14415580034255981 | Test loss: 0.24951712787151337\n",
      "Epoch: 638 | Loss: 0.14414189755916595 | Test loss: 0.24968577921390533\n",
      "Epoch: 639 | Loss: 0.14412817358970642 | Test loss: 0.2498544454574585\n",
      "Epoch: 640 | Loss: 0.1441144496202469 | Test loss: 0.2500230669975281\n",
      "Epoch: 641 | Loss: 0.14410072565078735 | Test loss: 0.25020694732666016\n",
      "Epoch: 642 | Loss: 0.14408713579177856 | Test loss: 0.2503695487976074\n",
      "Epoch: 643 | Loss: 0.14407387375831604 | Test loss: 0.25053220987319946\n",
      "Epoch: 644 | Loss: 0.14406061172485352 | Test loss: 0.2506948411464691\n",
      "Epoch: 645 | Loss: 0.144047349691391 | Test loss: 0.25085747241973877\n",
      "Epoch: 646 | Loss: 0.14403407275676727 | Test loss: 0.2510201334953308\n",
      "Epoch: 647 | Loss: 0.14402082562446594 | Test loss: 0.25118276476860046\n",
      "Epoch: 648 | Loss: 0.14400754868984222 | Test loss: 0.25134536623954773\n",
      "Epoch: 649 | Loss: 0.1439942866563797 | Test loss: 0.25150802731513977\n",
      "Epoch: 650 | Loss: 0.14398102462291718 | Test loss: 0.2516706883907318\n",
      "Epoch: 651 | Loss: 0.14396776258945465 | Test loss: 0.2518332898616791\n",
      "Epoch: 652 | Loss: 0.14395463466644287 | Test loss: 0.2519906163215637\n",
      "Epoch: 653 | Loss: 0.14394177496433258 | Test loss: 0.252147912979126\n",
      "Epoch: 654 | Loss: 0.1439289003610611 | Test loss: 0.25230520963668823\n",
      "Epoch: 655 | Loss: 0.143916055560112 | Test loss: 0.2524625360965729\n",
      "Epoch: 656 | Loss: 0.14390318095684052 | Test loss: 0.25261983275413513\n",
      "Epoch: 657 | Loss: 0.14389032125473022 | Test loss: 0.25275665521621704\n",
      "Epoch: 658 | Loss: 0.14387769997119904 | Test loss: 0.25289347767829895\n",
      "Epoch: 659 | Loss: 0.14386506378650665 | Test loss: 0.25303027033805847\n",
      "Epoch: 660 | Loss: 0.14385242760181427 | Test loss: 0.253167062997818\n",
      "Epoch: 661 | Loss: 0.1438397914171219 | Test loss: 0.2533038854598999\n",
      "Epoch: 662 | Loss: 0.1438271552324295 | Test loss: 0.2534407079219818\n",
      "Epoch: 663 | Loss: 0.14381451904773712 | Test loss: 0.25357750058174133\n",
      "Epoch: 664 | Loss: 0.14380188286304474 | Test loss: 0.25371429324150085\n",
      "Epoch: 665 | Loss: 0.14378924667835236 | Test loss: 0.25385114550590515\n",
      "Epoch: 666 | Loss: 0.14377662539482117 | Test loss: 0.25398796796798706\n",
      "Epoch: 667 | Loss: 0.1437639743089676 | Test loss: 0.2541247606277466\n",
      "Epoch: 668 | Loss: 0.1437513381242752 | Test loss: 0.2542615532875061\n",
      "Epoch: 669 | Loss: 0.14373886585235596 | Test loss: 0.25441107153892517\n",
      "Epoch: 670 | Loss: 0.14372658729553223 | Test loss: 0.2545606195926666\n",
      "Epoch: 671 | Loss: 0.14371438324451447 | Test loss: 0.2547244131565094\n",
      "Epoch: 672 | Loss: 0.14370229840278625 | Test loss: 0.25488823652267456\n",
      "Epoch: 673 | Loss: 0.14369019865989685 | Test loss: 0.2550520598888397\n",
      "Epoch: 674 | Loss: 0.14367811381816864 | Test loss: 0.2552158534526825\n",
      "Epoch: 675 | Loss: 0.14366601407527924 | Test loss: 0.25537967681884766\n",
      "Epoch: 676 | Loss: 0.14365392923355103 | Test loss: 0.2555435001850128\n",
      "Epoch: 677 | Loss: 0.14364182949066162 | Test loss: 0.255707323551178\n",
      "Epoch: 678 | Loss: 0.1436297446489334 | Test loss: 0.25587114691734314\n",
      "Epoch: 679 | Loss: 0.143617644906044 | Test loss: 0.2560349702835083\n",
      "Epoch: 680 | Loss: 0.143605574965477 | Test loss: 0.25621458888053894\n",
      "Epoch: 681 | Loss: 0.14359351992607117 | Test loss: 0.25639423727989197\n",
      "Epoch: 682 | Loss: 0.1435815542936325 | Test loss: 0.2565540373325348\n",
      "Epoch: 683 | Loss: 0.14356975257396698 | Test loss: 0.2567138373851776\n",
      "Epoch: 684 | Loss: 0.14355795085430145 | Test loss: 0.25687363743782043\n",
      "Epoch: 685 | Loss: 0.14354614913463593 | Test loss: 0.25703346729278564\n",
      "Epoch: 686 | Loss: 0.1435343325138092 | Test loss: 0.25719329714775085\n",
      "Epoch: 687 | Loss: 0.14352253079414368 | Test loss: 0.2573530673980713\n",
      "Epoch: 688 | Loss: 0.14351074397563934 | Test loss: 0.2575128972530365\n",
      "Epoch: 689 | Loss: 0.143498957157135 | Test loss: 0.2576860785484314\n",
      "Epoch: 690 | Loss: 0.1434873789548874 | Test loss: 0.2578592896461487\n",
      "Epoch: 691 | Loss: 0.14347581565380096 | Test loss: 0.2580324411392212\n",
      "Epoch: 692 | Loss: 0.14346423745155334 | Test loss: 0.2582056224346161\n",
      "Epoch: 693 | Loss: 0.14345267415046692 | Test loss: 0.2583788335323334\n",
      "Epoch: 694 | Loss: 0.1434410959482193 | Test loss: 0.25855201482772827\n",
      "Epoch: 695 | Loss: 0.14342960715293884 | Test loss: 0.2587044835090637\n",
      "Epoch: 696 | Loss: 0.1434183120727539 | Test loss: 0.25885695219039917\n",
      "Epoch: 697 | Loss: 0.14340703189373016 | Test loss: 0.2590094208717346\n",
      "Epoch: 698 | Loss: 0.14339575171470642 | Test loss: 0.2591618597507477\n",
      "Epoch: 699 | Loss: 0.14338448643684387 | Test loss: 0.2592960298061371\n",
      "Epoch: 700 | Loss: 0.1433732807636261 | Test loss: 0.2594301998615265\n",
      "Epoch: 701 | Loss: 0.14336219429969788 | Test loss: 0.2595768868923187\n",
      "Epoch: 702 | Loss: 0.14335131645202637 | Test loss: 0.2597235441207886\n",
      "Epoch: 703 | Loss: 0.14334043860435486 | Test loss: 0.2598702609539032\n",
      "Epoch: 704 | Loss: 0.14332956075668335 | Test loss: 0.26001691818237305\n",
      "Epoch: 705 | Loss: 0.14331868290901184 | Test loss: 0.2601636052131653\n",
      "Epoch: 706 | Loss: 0.14330779016017914 | Test loss: 0.2603102922439575\n",
      "Epoch: 707 | Loss: 0.14329692721366882 | Test loss: 0.26045694947242737\n",
      "Epoch: 708 | Loss: 0.14328603446483612 | Test loss: 0.260603666305542\n",
      "Epoch: 709 | Loss: 0.1432751566171646 | Test loss: 0.26075035333633423\n",
      "Epoch: 710 | Loss: 0.1432643085718155 | Test loss: 0.2609119713306427\n",
      "Epoch: 711 | Loss: 0.14325354993343353 | Test loss: 0.26107358932495117\n",
      "Epoch: 712 | Loss: 0.14324282109737396 | Test loss: 0.26123520731925964\n",
      "Epoch: 713 | Loss: 0.143232062458992 | Test loss: 0.2613968253135681\n",
      "Epoch: 714 | Loss: 0.14322130382061005 | Test loss: 0.2615584433078766\n",
      "Epoch: 715 | Loss: 0.1432105451822281 | Test loss: 0.26172009110450745\n",
      "Epoch: 716 | Loss: 0.14319980144500732 | Test loss: 0.2618817090988159\n",
      "Epoch: 717 | Loss: 0.14318908751010895 | Test loss: 0.2620241343975067\n",
      "Epoch: 718 | Loss: 0.14317847788333893 | Test loss: 0.2621665894985199\n",
      "Epoch: 719 | Loss: 0.1431678980588913 | Test loss: 0.2623090445995331\n",
      "Epoch: 720 | Loss: 0.14315728843212128 | Test loss: 0.26245149970054626\n",
      "Epoch: 721 | Loss: 0.14314672350883484 | Test loss: 0.26260802149772644\n",
      "Epoch: 722 | Loss: 0.14313632249832153 | Test loss: 0.26276451349258423\n",
      "Epoch: 723 | Loss: 0.14312590658664703 | Test loss: 0.2629210352897644\n",
      "Epoch: 724 | Loss: 0.14311552047729492 | Test loss: 0.2630775272846222\n",
      "Epoch: 725 | Loss: 0.1431051790714264 | Test loss: 0.26321396231651306\n",
      "Epoch: 726 | Loss: 0.143095001578331 | Test loss: 0.26335039734840393\n",
      "Epoch: 727 | Loss: 0.1430848389863968 | Test loss: 0.26350000500679016\n",
      "Epoch: 728 | Loss: 0.1430749148130417 | Test loss: 0.2636496126651764\n",
      "Epoch: 729 | Loss: 0.1430649757385254 | Test loss: 0.2637782692909241\n",
      "Epoch: 730 | Loss: 0.1430552899837494 | Test loss: 0.26390692591667175\n",
      "Epoch: 731 | Loss: 0.1430456042289734 | Test loss: 0.2640356123447418\n",
      "Epoch: 732 | Loss: 0.1430359184741974 | Test loss: 0.2641642391681671\n",
      "Epoch: 733 | Loss: 0.1430262327194214 | Test loss: 0.2642928957939148\n",
      "Epoch: 734 | Loss: 0.14301654696464539 | Test loss: 0.26442158222198486\n",
      "Epoch: 735 | Loss: 0.14300687611103058 | Test loss: 0.26455023884773254\n",
      "Epoch: 736 | Loss: 0.14299719035625458 | Test loss: 0.2646788954734802\n",
      "Epoch: 737 | Loss: 0.14298748970031738 | Test loss: 0.2648075520992279\n",
      "Epoch: 738 | Loss: 0.14297780394554138 | Test loss: 0.2649361789226532\n",
      "Epoch: 739 | Loss: 0.14296811819076538 | Test loss: 0.26506486535072327\n",
      "Epoch: 740 | Loss: 0.14295844733715057 | Test loss: 0.26519352197647095\n",
      "Epoch: 741 | Loss: 0.14294876158237457 | Test loss: 0.26532217860221863\n",
      "Epoch: 742 | Loss: 0.14293907582759857 | Test loss: 0.2654508352279663\n",
      "Epoch: 743 | Loss: 0.14292937517166138 | Test loss: 0.265579491853714\n",
      "Epoch: 744 | Loss: 0.14291968941688538 | Test loss: 0.26570814847946167\n",
      "Epoch: 745 | Loss: 0.14291001856327057 | Test loss: 0.26583683490753174\n",
      "Epoch: 746 | Loss: 0.14290033280849457 | Test loss: 0.26596546173095703\n",
      "Epoch: 747 | Loss: 0.14289064705371857 | Test loss: 0.2660941183567047\n",
      "Epoch: 748 | Loss: 0.14288096129894257 | Test loss: 0.2662228047847748\n",
      "Epoch: 749 | Loss: 0.14287127554416656 | Test loss: 0.26635146141052246\n",
      "Epoch: 750 | Loss: 0.14286158978939056 | Test loss: 0.26648008823394775\n",
      "Epoch: 751 | Loss: 0.14285190403461456 | Test loss: 0.2666087746620178\n",
      "Epoch: 752 | Loss: 0.14284221827983856 | Test loss: 0.2667374312877655\n",
      "Epoch: 753 | Loss: 0.14283253252506256 | Test loss: 0.2668660879135132\n",
      "Epoch: 754 | Loss: 0.14282284677028656 | Test loss: 0.26699474453926086\n",
      "Epoch: 755 | Loss: 0.14281317591667175 | Test loss: 0.26712340116500854\n",
      "Epoch: 756 | Loss: 0.14280349016189575 | Test loss: 0.2672520875930786\n",
      "Epoch: 757 | Loss: 0.14279380440711975 | Test loss: 0.2673807144165039\n",
      "Epoch: 758 | Loss: 0.14278411865234375 | Test loss: 0.2675093710422516\n",
      "Epoch: 759 | Loss: 0.14277447760105133 | Test loss: 0.2676509916782379\n",
      "Epoch: 760 | Loss: 0.14276506006717682 | Test loss: 0.26779261231422424\n",
      "Epoch: 761 | Loss: 0.1427556425333023 | Test loss: 0.26793426275253296\n",
      "Epoch: 762 | Loss: 0.1427462249994278 | Test loss: 0.2680759131908417\n",
      "Epoch: 763 | Loss: 0.14273686707019806 | Test loss: 0.26823127269744873\n",
      "Epoch: 764 | Loss: 0.14272764325141907 | Test loss: 0.26838669180870056\n",
      "Epoch: 765 | Loss: 0.14271841943264008 | Test loss: 0.2685210108757019\n",
      "Epoch: 766 | Loss: 0.14270946383476257 | Test loss: 0.26865527033805847\n",
      "Epoch: 767 | Loss: 0.14270050823688507 | Test loss: 0.2687895596027374\n",
      "Epoch: 768 | Loss: 0.14269156754016876 | Test loss: 0.268923819065094\n",
      "Epoch: 769 | Loss: 0.14268261194229126 | Test loss: 0.26905810832977295\n",
      "Epoch: 770 | Loss: 0.14267367124557495 | Test loss: 0.26920706033706665\n",
      "Epoch: 771 | Loss: 0.14266493916511536 | Test loss: 0.2693357765674591\n",
      "Epoch: 772 | Loss: 0.1426563262939453 | Test loss: 0.26946452260017395\n",
      "Epoch: 773 | Loss: 0.14264772832393646 | Test loss: 0.2695932388305664\n",
      "Epoch: 774 | Loss: 0.14263911545276642 | Test loss: 0.26972195506095886\n",
      "Epoch: 775 | Loss: 0.14263050258159637 | Test loss: 0.2698507010936737\n",
      "Epoch: 776 | Loss: 0.14262188971042633 | Test loss: 0.2699793875217438\n",
      "Epoch: 777 | Loss: 0.1426132768392563 | Test loss: 0.2701081335544586\n",
      "Epoch: 778 | Loss: 0.14260467886924744 | Test loss: 0.2702368497848511\n",
      "Epoch: 779 | Loss: 0.1425960659980774 | Test loss: 0.27036556601524353\n",
      "Epoch: 780 | Loss: 0.14258745312690735 | Test loss: 0.2704943120479584\n",
      "Epoch: 781 | Loss: 0.14257889986038208 | Test loss: 0.27060356736183167\n",
      "Epoch: 782 | Loss: 0.14257042109966278 | Test loss: 0.2707129120826721\n",
      "Epoch: 783 | Loss: 0.1425619274377823 | Test loss: 0.2708221673965454\n",
      "Epoch: 784 | Loss: 0.1425534337759018 | Test loss: 0.2709314525127411\n",
      "Epoch: 785 | Loss: 0.14254498481750488 | Test loss: 0.2710563838481903\n",
      "Epoch: 786 | Loss: 0.14253662526607513 | Test loss: 0.2711813151836395\n",
      "Epoch: 787 | Loss: 0.142528235912323 | Test loss: 0.27130624651908875\n",
      "Epoch: 788 | Loss: 0.14251986145973206 | Test loss: 0.27143120765686035\n",
      "Epoch: 789 | Loss: 0.14251147210597992 | Test loss: 0.2715561091899872\n",
      "Epoch: 790 | Loss: 0.14250309765338898 | Test loss: 0.2716810405254364\n",
      "Epoch: 791 | Loss: 0.1424947828054428 | Test loss: 0.2718186378479004\n",
      "Epoch: 792 | Loss: 0.14248664677143097 | Test loss: 0.27195629477500916\n",
      "Epoch: 793 | Loss: 0.14247852563858032 | Test loss: 0.27209389209747314\n",
      "Epoch: 794 | Loss: 0.14247038960456848 | Test loss: 0.2722315192222595\n",
      "Epoch: 795 | Loss: 0.14246225357055664 | Test loss: 0.2723691165447235\n",
      "Epoch: 796 | Loss: 0.142454132437706 | Test loss: 0.2725067138671875\n",
      "Epoch: 797 | Loss: 0.14244599640369415 | Test loss: 0.2726443409919739\n",
      "Epoch: 798 | Loss: 0.1424378752708435 | Test loss: 0.27278196811676025\n",
      "Epoch: 799 | Loss: 0.14242975413799286 | Test loss: 0.27291959524154663\n",
      "Epoch: 800 | Loss: 0.14242161810398102 | Test loss: 0.27305716276168823\n",
      "Epoch: 801 | Loss: 0.14241348206996918 | Test loss: 0.2731947898864746\n",
      "Epoch: 802 | Loss: 0.14240536093711853 | Test loss: 0.273332417011261\n",
      "Epoch: 803 | Loss: 0.14239728450775146 | Test loss: 0.2734515368938446\n",
      "Epoch: 804 | Loss: 0.14238925278186798 | Test loss: 0.2735842764377594\n",
      "Epoch: 805 | Loss: 0.14238141477108002 | Test loss: 0.2737169861793518\n",
      "Epoch: 806 | Loss: 0.14237357676029205 | Test loss: 0.2738496959209442\n",
      "Epoch: 807 | Loss: 0.1423657387495041 | Test loss: 0.273982435464859\n",
      "Epoch: 808 | Loss: 0.14235790073871613 | Test loss: 0.2741151452064514\n",
      "Epoch: 809 | Loss: 0.14235006272792816 | Test loss: 0.2742478549480438\n",
      "Epoch: 810 | Loss: 0.1423422247171402 | Test loss: 0.2743805944919586\n",
      "Epoch: 811 | Loss: 0.1423344910144806 | Test loss: 0.2744928300380707\n",
      "Epoch: 812 | Loss: 0.14232684671878815 | Test loss: 0.27460506558418274\n",
      "Epoch: 813 | Loss: 0.1423192173242569 | Test loss: 0.2747173309326172\n",
      "Epoch: 814 | Loss: 0.14231157302856445 | Test loss: 0.27482959628105164\n",
      "Epoch: 815 | Loss: 0.142303928732872 | Test loss: 0.2749418616294861\n",
      "Epoch: 816 | Loss: 0.14229629933834076 | Test loss: 0.27505409717559814\n",
      "Epoch: 817 | Loss: 0.14228864014148712 | Test loss: 0.2751663625240326\n",
      "Epoch: 818 | Loss: 0.14228101074695587 | Test loss: 0.27527856826782227\n",
      "Epoch: 819 | Loss: 0.14227336645126343 | Test loss: 0.2753908336162567\n",
      "Epoch: 820 | Loss: 0.14226572215557098 | Test loss: 0.27550309896469116\n",
      "Epoch: 821 | Loss: 0.14225809276103973 | Test loss: 0.2756153345108032\n",
      "Epoch: 822 | Loss: 0.14225052297115326 | Test loss: 0.2757400572299957\n",
      "Epoch: 823 | Loss: 0.1422431319952011 | Test loss: 0.27586477994918823\n",
      "Epoch: 824 | Loss: 0.14223575592041016 | Test loss: 0.27598950266838074\n",
      "Epoch: 825 | Loss: 0.1422283947467804 | Test loss: 0.27611419558525085\n",
      "Epoch: 826 | Loss: 0.14222106337547302 | Test loss: 0.27625343203544617\n",
      "Epoch: 827 | Loss: 0.1422138214111328 | Test loss: 0.2763926684856415\n",
      "Epoch: 828 | Loss: 0.1422065645456314 | Test loss: 0.2765318751335144\n",
      "Epoch: 829 | Loss: 0.14219930768013 | Test loss: 0.2766711115837097\n",
      "Epoch: 830 | Loss: 0.1421920508146286 | Test loss: 0.27681031823158264\n",
      "Epoch: 831 | Loss: 0.1421847939491272 | Test loss: 0.27694952487945557\n",
      "Epoch: 832 | Loss: 0.1421775370836258 | Test loss: 0.2770887613296509\n",
      "Epoch: 833 | Loss: 0.1421702802181244 | Test loss: 0.27722805738449097\n",
      "Epoch: 834 | Loss: 0.14216308295726776 | Test loss: 0.277347594499588\n",
      "Epoch: 835 | Loss: 0.1421559900045395 | Test loss: 0.27746719121932983\n",
      "Epoch: 836 | Loss: 0.14214889705181122 | Test loss: 0.27758678793907166\n",
      "Epoch: 837 | Loss: 0.14214181900024414 | Test loss: 0.2777063548564911\n",
      "Epoch: 838 | Loss: 0.14213472604751587 | Test loss: 0.2778259515762329\n",
      "Epoch: 839 | Loss: 0.1421276330947876 | Test loss: 0.27794554829597473\n",
      "Epoch: 840 | Loss: 0.14212055504322052 | Test loss: 0.27806514501571655\n",
      "Epoch: 841 | Loss: 0.14211344718933105 | Test loss: 0.2781847417354584\n",
      "Epoch: 842 | Loss: 0.14210638403892517 | Test loss: 0.2783043086528778\n",
      "Epoch: 843 | Loss: 0.1420992761850357 | Test loss: 0.27842390537261963\n",
      "Epoch: 844 | Loss: 0.1420922577381134 | Test loss: 0.2785568833351135\n",
      "Epoch: 845 | Loss: 0.14208535850048065 | Test loss: 0.2786898612976074\n",
      "Epoch: 846 | Loss: 0.1420784741640091 | Test loss: 0.2788228988647461\n",
      "Epoch: 847 | Loss: 0.14207158982753754 | Test loss: 0.27893513441085815\n",
      "Epoch: 848 | Loss: 0.1420649141073227 | Test loss: 0.2790473699569702\n",
      "Epoch: 849 | Loss: 0.14205822348594666 | Test loss: 0.2791596055030823\n",
      "Epoch: 850 | Loss: 0.14205153286457062 | Test loss: 0.2792718708515167\n",
      "Epoch: 851 | Loss: 0.14204484224319458 | Test loss: 0.2793841063976288\n",
      "Epoch: 852 | Loss: 0.14203815162181854 | Test loss: 0.27949634194374084\n",
      "Epoch: 853 | Loss: 0.1420314610004425 | Test loss: 0.2796085774898529\n",
      "Epoch: 854 | Loss: 0.14202478528022766 | Test loss: 0.27972081303596497\n",
      "Epoch: 855 | Loss: 0.14201807975769043 | Test loss: 0.279833048582077\n",
      "Epoch: 856 | Loss: 0.14201140403747559 | Test loss: 0.2799453139305115\n",
      "Epoch: 857 | Loss: 0.14200471341609955 | Test loss: 0.28005754947662354\n",
      "Epoch: 858 | Loss: 0.1419980376958847 | Test loss: 0.2801697850227356\n",
      "Epoch: 859 | Loss: 0.14199133217334747 | Test loss: 0.28028202056884766\n",
      "Epoch: 860 | Loss: 0.14198465645313263 | Test loss: 0.2803942859172821\n",
      "Epoch: 861 | Loss: 0.1419779509305954 | Test loss: 0.28050652146339417\n",
      "Epoch: 862 | Loss: 0.14197127521038055 | Test loss: 0.2806187868118286\n",
      "Epoch: 863 | Loss: 0.14196458458900452 | Test loss: 0.2807309925556183\n",
      "Epoch: 864 | Loss: 0.14195789396762848 | Test loss: 0.28084325790405273\n",
      "Epoch: 865 | Loss: 0.14195121824741364 | Test loss: 0.2809554934501648\n",
      "Epoch: 866 | Loss: 0.1419445127248764 | Test loss: 0.28106772899627686\n",
      "Epoch: 867 | Loss: 0.14193782210350037 | Test loss: 0.2811799943447113\n",
      "Epoch: 868 | Loss: 0.14193116128444672 | Test loss: 0.28130874037742615\n",
      "Epoch: 869 | Loss: 0.14192448556423187 | Test loss: 0.2814375162124634\n",
      "Epoch: 870 | Loss: 0.14191782474517822 | Test loss: 0.2815662622451782\n",
      "Epoch: 871 | Loss: 0.14191113412380219 | Test loss: 0.28169506788253784\n",
      "Epoch: 872 | Loss: 0.14190447330474854 | Test loss: 0.2818238139152527\n",
      "Epoch: 873 | Loss: 0.14189781248569489 | Test loss: 0.2819525897502899\n",
      "Epoch: 874 | Loss: 0.14189113676548004 | Test loss: 0.28208133578300476\n",
      "Epoch: 875 | Loss: 0.1418844610452652 | Test loss: 0.2822100818157196\n",
      "Epoch: 876 | Loss: 0.14187780022621155 | Test loss: 0.28233885765075684\n",
      "Epoch: 877 | Loss: 0.1418711245059967 | Test loss: 0.2824676036834717\n",
      "Epoch: 878 | Loss: 0.14186444878578186 | Test loss: 0.2825963795185089\n",
      "Epoch: 879 | Loss: 0.1418577879667282 | Test loss: 0.28272515535354614\n",
      "Epoch: 880 | Loss: 0.14185111224651337 | Test loss: 0.2828693389892578\n",
      "Epoch: 881 | Loss: 0.1418447345495224 | Test loss: 0.2829473316669464\n",
      "Epoch: 882 | Loss: 0.1418389230966568 | Test loss: 0.28304174542427063\n",
      "Epoch: 883 | Loss: 0.14183315634727478 | Test loss: 0.2831362187862396\n",
      "Epoch: 884 | Loss: 0.14182738959789276 | Test loss: 0.28323066234588623\n",
      "Epoch: 885 | Loss: 0.14182160794734955 | Test loss: 0.28332510590553284\n",
      "Epoch: 886 | Loss: 0.14181582629680634 | Test loss: 0.28341951966285706\n",
      "Epoch: 887 | Loss: 0.14181004464626312 | Test loss: 0.28351399302482605\n",
      "Epoch: 888 | Loss: 0.1418042778968811 | Test loss: 0.28360840678215027\n",
      "Epoch: 889 | Loss: 0.14179851114749908 | Test loss: 0.2837028503417969\n",
      "Epoch: 890 | Loss: 0.14179272949695587 | Test loss: 0.28379732370376587\n",
      "Epoch: 891 | Loss: 0.14178696274757385 | Test loss: 0.2838917374610901\n",
      "Epoch: 892 | Loss: 0.14178119599819183 | Test loss: 0.2839862108230591\n",
      "Epoch: 893 | Loss: 0.14177541434764862 | Test loss: 0.2840806543827057\n",
      "Epoch: 894 | Loss: 0.1417696326971054 | Test loss: 0.2841750979423523\n",
      "Epoch: 895 | Loss: 0.1417638510465622 | Test loss: 0.2842695415019989\n",
      "Epoch: 896 | Loss: 0.14175808429718018 | Test loss: 0.2843639850616455\n",
      "Epoch: 897 | Loss: 0.14175231754779816 | Test loss: 0.2844584286212921\n",
      "Epoch: 898 | Loss: 0.14174653589725494 | Test loss: 0.2845529019832611\n",
      "Epoch: 899 | Loss: 0.14174076914787292 | Test loss: 0.2846473157405853\n",
      "Epoch: 900 | Loss: 0.1417350023984909 | Test loss: 0.28474175930023193\n",
      "Epoch: 901 | Loss: 0.1417292207479477 | Test loss: 0.28483620285987854\n",
      "Epoch: 902 | Loss: 0.14172345399856567 | Test loss: 0.28493064641952515\n",
      "Epoch: 903 | Loss: 0.14171767234802246 | Test loss: 0.28502508997917175\n",
      "Epoch: 904 | Loss: 0.14171189069747925 | Test loss: 0.28511953353881836\n",
      "Epoch: 905 | Loss: 0.14170612394809723 | Test loss: 0.2852139472961426\n",
      "Epoch: 906 | Loss: 0.14170034229755402 | Test loss: 0.2853084206581116\n",
      "Epoch: 907 | Loss: 0.141694575548172 | Test loss: 0.28540289402008057\n",
      "Epoch: 908 | Loss: 0.14168880879878998 | Test loss: 0.2854973077774048\n",
      "Epoch: 909 | Loss: 0.14168302714824677 | Test loss: 0.2855917811393738\n",
      "Epoch: 910 | Loss: 0.14167724549770355 | Test loss: 0.2856862246990204\n",
      "Epoch: 911 | Loss: 0.14167146384716034 | Test loss: 0.2857806384563446\n",
      "Epoch: 912 | Loss: 0.14166580140590668 | Test loss: 0.28588804602622986\n",
      "Epoch: 913 | Loss: 0.14166024327278137 | Test loss: 0.2859954833984375\n",
      "Epoch: 914 | Loss: 0.14165468513965607 | Test loss: 0.28610289096832275\n",
      "Epoch: 915 | Loss: 0.14164911210536957 | Test loss: 0.2862102687358856\n",
      "Epoch: 916 | Loss: 0.14164355397224426 | Test loss: 0.28631770610809326\n",
      "Epoch: 917 | Loss: 0.14163799583911896 | Test loss: 0.2864251136779785\n",
      "Epoch: 918 | Loss: 0.14163243770599365 | Test loss: 0.28653252124786377\n",
      "Epoch: 919 | Loss: 0.14162686467170715 | Test loss: 0.28661876916885376\n",
      "Epoch: 920 | Loss: 0.14162150025367737 | Test loss: 0.28670498728752136\n",
      "Epoch: 921 | Loss: 0.1416161060333252 | Test loss: 0.28679120540618896\n",
      "Epoch: 922 | Loss: 0.1416107416152954 | Test loss: 0.28689146041870117\n",
      "Epoch: 923 | Loss: 0.14160552620887756 | Test loss: 0.28699180483818054\n",
      "Epoch: 924 | Loss: 0.14160029590129852 | Test loss: 0.28709208965301514\n",
      "Epoch: 925 | Loss: 0.14159509539604187 | Test loss: 0.28719234466552734\n",
      "Epoch: 926 | Loss: 0.14158986508846283 | Test loss: 0.2872926592826843\n",
      "Epoch: 927 | Loss: 0.14158464968204498 | Test loss: 0.28739291429519653\n",
      "Epoch: 928 | Loss: 0.14157943427562714 | Test loss: 0.2874932289123535\n",
      "Epoch: 929 | Loss: 0.14157423377037048 | Test loss: 0.2875935435295105\n",
      "Epoch: 930 | Loss: 0.14156900346279144 | Test loss: 0.2876938283443451\n",
      "Epoch: 931 | Loss: 0.1415637880563736 | Test loss: 0.2877941131591797\n",
      "Epoch: 932 | Loss: 0.14155857264995575 | Test loss: 0.2878943979740143\n",
      "Epoch: 933 | Loss: 0.1415533572435379 | Test loss: 0.28799471259117126\n",
      "Epoch: 934 | Loss: 0.14154814183712006 | Test loss: 0.28809499740600586\n",
      "Epoch: 935 | Loss: 0.1415429413318634 | Test loss: 0.28817519545555115\n",
      "Epoch: 936 | Loss: 0.1415378302335739 | Test loss: 0.2882554233074188\n",
      "Epoch: 937 | Loss: 0.1415327489376068 | Test loss: 0.2883356213569641\n",
      "Epoch: 938 | Loss: 0.14152763783931732 | Test loss: 0.2884158492088318\n",
      "Epoch: 939 | Loss: 0.14152254164218903 | Test loss: 0.2884960472583771\n",
      "Epoch: 940 | Loss: 0.14151743054389954 | Test loss: 0.28857627511024475\n",
      "Epoch: 941 | Loss: 0.14151233434677124 | Test loss: 0.2886565029621124\n",
      "Epoch: 942 | Loss: 0.14150728285312653 | Test loss: 0.28874942660331726\n",
      "Epoch: 943 | Loss: 0.14150241017341614 | Test loss: 0.2888423800468445\n",
      "Epoch: 944 | Loss: 0.14149755239486694 | Test loss: 0.28895053267478943\n",
      "Epoch: 945 | Loss: 0.14149275422096252 | Test loss: 0.289058655500412\n",
      "Epoch: 946 | Loss: 0.1414879560470581 | Test loss: 0.28916677832603455\n",
      "Epoch: 947 | Loss: 0.1414831578731537 | Test loss: 0.2892749011516571\n",
      "Epoch: 948 | Loss: 0.14147835969924927 | Test loss: 0.28938305377960205\n",
      "Epoch: 949 | Loss: 0.14147356152534485 | Test loss: 0.289491206407547\n",
      "Epoch: 950 | Loss: 0.14146876335144043 | Test loss: 0.28959932923316956\n",
      "Epoch: 951 | Loss: 0.1414639800786972 | Test loss: 0.2897074818611145\n",
      "Epoch: 952 | Loss: 0.14145918190479279 | Test loss: 0.28981560468673706\n",
      "Epoch: 953 | Loss: 0.14145438373088837 | Test loss: 0.2899237275123596\n",
      "Epoch: 954 | Loss: 0.14144958555698395 | Test loss: 0.29003188014030457\n",
      "Epoch: 955 | Loss: 0.14144478738307953 | Test loss: 0.2901400029659271\n",
      "Epoch: 956 | Loss: 0.1414400041103363 | Test loss: 0.2902481257915497\n",
      "Epoch: 957 | Loss: 0.14143520593643188 | Test loss: 0.29035627841949463\n",
      "Epoch: 958 | Loss: 0.14143040776252747 | Test loss: 0.2904644012451172\n",
      "Epoch: 959 | Loss: 0.14142560958862305 | Test loss: 0.29057255387306213\n",
      "Epoch: 960 | Loss: 0.14142082631587982 | Test loss: 0.2906806766986847\n",
      "Epoch: 961 | Loss: 0.1414160281419754 | Test loss: 0.29078879952430725\n",
      "Epoch: 962 | Loss: 0.14141122996807098 | Test loss: 0.2908969521522522\n",
      "Epoch: 963 | Loss: 0.14140643179416656 | Test loss: 0.29100510478019714\n",
      "Epoch: 964 | Loss: 0.14140163362026215 | Test loss: 0.2911131978034973\n",
      "Epoch: 965 | Loss: 0.14139685034751892 | Test loss: 0.29122135043144226\n",
      "Epoch: 966 | Loss: 0.1413920372724533 | Test loss: 0.29132944345474243\n",
      "Epoch: 967 | Loss: 0.14138731360435486 | Test loss: 0.2914324998855591\n",
      "Epoch: 968 | Loss: 0.14138272404670715 | Test loss: 0.29153552651405334\n",
      "Epoch: 969 | Loss: 0.14137817919254303 | Test loss: 0.2916385233402252\n",
      "Epoch: 970 | Loss: 0.1413736194372177 | Test loss: 0.2917415499687195\n",
      "Epoch: 971 | Loss: 0.1413690596818924 | Test loss: 0.29184457659721375\n",
      "Epoch: 972 | Loss: 0.14136449992656708 | Test loss: 0.2919475734233856\n",
      "Epoch: 973 | Loss: 0.14135994017124176 | Test loss: 0.2920505702495575\n",
      "Epoch: 974 | Loss: 0.14135536551475525 | Test loss: 0.29215362668037415\n",
      "Epoch: 975 | Loss: 0.14135082066059113 | Test loss: 0.2922690808773041\n",
      "Epoch: 976 | Loss: 0.14134646952152252 | Test loss: 0.29236432909965515\n",
      "Epoch: 977 | Loss: 0.14134225249290466 | Test loss: 0.2924595773220062\n",
      "Epoch: 978 | Loss: 0.1413380354642868 | Test loss: 0.2925547957420349\n",
      "Epoch: 979 | Loss: 0.14133380353450775 | Test loss: 0.2926500141620636\n",
      "Epoch: 980 | Loss: 0.1413295865058899 | Test loss: 0.29274529218673706\n",
      "Epoch: 981 | Loss: 0.14132536947727203 | Test loss: 0.29284048080444336\n",
      "Epoch: 982 | Loss: 0.14132116734981537 | Test loss: 0.29293569922447205\n",
      "Epoch: 983 | Loss: 0.14131693542003632 | Test loss: 0.29303091764450073\n",
      "Epoch: 984 | Loss: 0.14131270349025726 | Test loss: 0.2931261658668518\n",
      "Epoch: 985 | Loss: 0.1413085013628006 | Test loss: 0.2932213842868805\n",
      "Epoch: 986 | Loss: 0.14130426943302155 | Test loss: 0.29331663250923157\n",
      "Epoch: 987 | Loss: 0.1413000524044037 | Test loss: 0.29341185092926025\n",
      "Epoch: 988 | Loss: 0.14129583537578583 | Test loss: 0.29350709915161133\n",
      "Epoch: 989 | Loss: 0.14129160344600677 | Test loss: 0.29360231757164\n",
      "Epoch: 990 | Loss: 0.1412874013185501 | Test loss: 0.2936975359916687\n",
      "Epoch: 991 | Loss: 0.14128316938877106 | Test loss: 0.2937927544116974\n",
      "Epoch: 992 | Loss: 0.1412789523601532 | Test loss: 0.29388800263404846\n",
      "Epoch: 993 | Loss: 0.14127473533153534 | Test loss: 0.29398322105407715\n",
      "Epoch: 994 | Loss: 0.1412705034017563 | Test loss: 0.2940784692764282\n",
      "Epoch: 995 | Loss: 0.14126630127429962 | Test loss: 0.2941736876964569\n",
      "Epoch: 996 | Loss: 0.14126206934452057 | Test loss: 0.294268935918808\n",
      "Epoch: 997 | Loss: 0.1412578523159027 | Test loss: 0.29436415433883667\n",
      "Epoch: 998 | Loss: 0.14125363528728485 | Test loss: 0.29445937275886536\n",
      "Epoch: 999 | Loss: 0.141249418258667 | Test loss: 0.29455459117889404\n",
      "Epoch: 1000 | Loss: 0.14124518632888794 | Test loss: 0.29464980959892273\n",
      "Epoch: 1001 | Loss: 0.14124096930027008 | Test loss: 0.2947450578212738\n",
      "Epoch: 1002 | Loss: 0.14123675227165222 | Test loss: 0.2948402762413025\n",
      "Epoch: 1003 | Loss: 0.14123252034187317 | Test loss: 0.29493552446365356\n",
      "Epoch: 1004 | Loss: 0.1412283033132553 | Test loss: 0.29503077268600464\n",
      "Epoch: 1005 | Loss: 0.14122410118579865 | Test loss: 0.2951259911060333\n",
      "Epoch: 1006 | Loss: 0.1412198692560196 | Test loss: 0.295221209526062\n",
      "Epoch: 1007 | Loss: 0.14121565222740173 | Test loss: 0.2953164279460907\n",
      "Epoch: 1008 | Loss: 0.14121143519878387 | Test loss: 0.2954116761684418\n",
      "Epoch: 1009 | Loss: 0.14120721817016602 | Test loss: 0.29550689458847046\n",
      "Epoch: 1010 | Loss: 0.14120300114154816 | Test loss: 0.29560211300849915\n",
      "Epoch: 1011 | Loss: 0.1411987841129303 | Test loss: 0.295710951089859\n",
      "Epoch: 1012 | Loss: 0.141194686293602 | Test loss: 0.2958196997642517\n",
      "Epoch: 1013 | Loss: 0.14119058847427368 | Test loss: 0.2959285378456116\n",
      "Epoch: 1014 | Loss: 0.14118658006191254 | Test loss: 0.2960168719291687\n",
      "Epoch: 1015 | Loss: 0.14118264615535736 | Test loss: 0.29610517621040344\n",
      "Epoch: 1016 | Loss: 0.14117871224880219 | Test loss: 0.29619351029396057\n",
      "Epoch: 1017 | Loss: 0.1411747932434082 | Test loss: 0.2962818443775177\n",
      "Epoch: 1018 | Loss: 0.14117085933685303 | Test loss: 0.29637017846107483\n",
      "Epoch: 1019 | Loss: 0.14116692543029785 | Test loss: 0.29645851254463196\n",
      "Epoch: 1020 | Loss: 0.14116300642490387 | Test loss: 0.2965468466281891\n",
      "Epoch: 1021 | Loss: 0.1411590576171875 | Test loss: 0.29663515090942383\n",
      "Epoch: 1022 | Loss: 0.14115513861179352 | Test loss: 0.29672351479530334\n",
      "Epoch: 1023 | Loss: 0.14115121960639954 | Test loss: 0.2968118488788605\n",
      "Epoch: 1024 | Loss: 0.14114728569984436 | Test loss: 0.2969001829624176\n",
      "Epoch: 1025 | Loss: 0.14114338159561157 | Test loss: 0.2970034182071686\n",
      "Epoch: 1026 | Loss: 0.14113953709602356 | Test loss: 0.29708752036094666\n",
      "Epoch: 1027 | Loss: 0.1411357820034027 | Test loss: 0.29717162251472473\n",
      "Epoch: 1028 | Loss: 0.14113202691078186 | Test loss: 0.2972557246685028\n",
      "Epoch: 1029 | Loss: 0.141128271818161 | Test loss: 0.2973398268222809\n",
      "Epoch: 1030 | Loss: 0.14112451672554016 | Test loss: 0.29742392897605896\n",
      "Epoch: 1031 | Loss: 0.14112074673175812 | Test loss: 0.29750803112983704\n",
      "Epoch: 1032 | Loss: 0.14111699163913727 | Test loss: 0.2975921332836151\n",
      "Epoch: 1033 | Loss: 0.14111323654651642 | Test loss: 0.2976762056350708\n",
      "Epoch: 1034 | Loss: 0.14110948145389557 | Test loss: 0.29776033759117126\n",
      "Epoch: 1035 | Loss: 0.14110572636127472 | Test loss: 0.29784440994262695\n",
      "Epoch: 1036 | Loss: 0.14110197126865387 | Test loss: 0.2979285418987274\n",
      "Epoch: 1037 | Loss: 0.14109821617603302 | Test loss: 0.2980126440525055\n",
      "Epoch: 1038 | Loss: 0.14109444618225098 | Test loss: 0.2980967164039612\n",
      "Epoch: 1039 | Loss: 0.14109069108963013 | Test loss: 0.29818084836006165\n",
      "Epoch: 1040 | Loss: 0.14108693599700928 | Test loss: 0.2982649505138397\n",
      "Epoch: 1041 | Loss: 0.14108318090438843 | Test loss: 0.2983490526676178\n",
      "Epoch: 1042 | Loss: 0.14107942581176758 | Test loss: 0.2984331548213959\n",
      "Epoch: 1043 | Loss: 0.14107567071914673 | Test loss: 0.29851725697517395\n",
      "Epoch: 1044 | Loss: 0.14107190072536469 | Test loss: 0.298601359128952\n",
      "Epoch: 1045 | Loss: 0.14106814563274384 | Test loss: 0.2986854612827301\n",
      "Epoch: 1046 | Loss: 0.14106439054012299 | Test loss: 0.2987695634365082\n",
      "Epoch: 1047 | Loss: 0.14106063544750214 | Test loss: 0.29885366559028625\n",
      "Epoch: 1048 | Loss: 0.1410568803548813 | Test loss: 0.29893776774406433\n",
      "Epoch: 1049 | Loss: 0.14105312526226044 | Test loss: 0.2990218698978424\n",
      "Epoch: 1050 | Loss: 0.14104942977428436 | Test loss: 0.29911932349205017\n",
      "Epoch: 1051 | Loss: 0.14104580879211426 | Test loss: 0.2991960942745209\n",
      "Epoch: 1052 | Loss: 0.1410423368215561 | Test loss: 0.2992728352546692\n",
      "Epoch: 1053 | Loss: 0.14103887975215912 | Test loss: 0.2993496358394623\n",
      "Epoch: 1054 | Loss: 0.14103540778160095 | Test loss: 0.2994263768196106\n",
      "Epoch: 1055 | Loss: 0.14103193581104279 | Test loss: 0.2995031177997589\n",
      "Epoch: 1056 | Loss: 0.14102846384048462 | Test loss: 0.2995798885822296\n",
      "Epoch: 1057 | Loss: 0.14102499186992645 | Test loss: 0.2996566593647003\n",
      "Epoch: 1058 | Loss: 0.1410215198993683 | Test loss: 0.299733430147171\n",
      "Epoch: 1059 | Loss: 0.14101804792881012 | Test loss: 0.29981017112731934\n",
      "Epoch: 1060 | Loss: 0.14101459085941315 | Test loss: 0.29988694190979004\n",
      "Epoch: 1061 | Loss: 0.14101111888885498 | Test loss: 0.29996371269226074\n",
      "Epoch: 1062 | Loss: 0.14100764691829681 | Test loss: 0.30004045367240906\n",
      "Epoch: 1063 | Loss: 0.14100417494773865 | Test loss: 0.30011722445487976\n",
      "Epoch: 1064 | Loss: 0.14100070297718048 | Test loss: 0.30019399523735046\n",
      "Epoch: 1065 | Loss: 0.14099723100662231 | Test loss: 0.3002707362174988\n",
      "Epoch: 1066 | Loss: 0.14099375903606415 | Test loss: 0.30034753680229187\n",
      "Epoch: 1067 | Loss: 0.14099030196666718 | Test loss: 0.3004242777824402\n",
      "Epoch: 1068 | Loss: 0.140986829996109 | Test loss: 0.3005010187625885\n",
      "Epoch: 1069 | Loss: 0.14098335802555084 | Test loss: 0.3005777895450592\n",
      "Epoch: 1070 | Loss: 0.14097990095615387 | Test loss: 0.3006545603275299\n",
      "Epoch: 1071 | Loss: 0.1409764289855957 | Test loss: 0.300731360912323\n",
      "Epoch: 1072 | Loss: 0.14097295701503754 | Test loss: 0.3008080720901489\n",
      "Epoch: 1073 | Loss: 0.14096949994564056 | Test loss: 0.3009010851383209\n",
      "Epoch: 1074 | Loss: 0.1409660428762436 | Test loss: 0.3009940981864929\n",
      "Epoch: 1075 | Loss: 0.140962615609169 | Test loss: 0.3010871112346649\n",
      "Epoch: 1076 | Loss: 0.14095915853977203 | Test loss: 0.3011800944805145\n",
      "Epoch: 1077 | Loss: 0.14095571637153625 | Test loss: 0.3012731075286865\n",
      "Epoch: 1078 | Loss: 0.14095227420330048 | Test loss: 0.3013661205768585\n",
      "Epoch: 1079 | Loss: 0.1409488320350647 | Test loss: 0.30145910382270813\n",
      "Epoch: 1080 | Loss: 0.14094538986682892 | Test loss: 0.3015521466732025\n",
      "Epoch: 1081 | Loss: 0.14094194769859314 | Test loss: 0.3016451299190521\n",
      "Epoch: 1082 | Loss: 0.14093850553035736 | Test loss: 0.30173811316490173\n",
      "Epoch: 1083 | Loss: 0.1409350484609604 | Test loss: 0.3018311560153961\n",
      "Epoch: 1084 | Loss: 0.1409316211938858 | Test loss: 0.30193889141082764\n",
      "Epoch: 1085 | Loss: 0.1409282684326172 | Test loss: 0.3020598292350769\n",
      "Epoch: 1086 | Loss: 0.14092503488063812 | Test loss: 0.3021225333213806\n",
      "Epoch: 1087 | Loss: 0.14092208445072174 | Test loss: 0.3021853268146515\n",
      "Epoch: 1088 | Loss: 0.14091913402080536 | Test loss: 0.3022480309009552\n",
      "Epoch: 1089 | Loss: 0.14091616868972778 | Test loss: 0.3023107945919037\n",
      "Epoch: 1090 | Loss: 0.1409132182598114 | Test loss: 0.3023735284805298\n",
      "Epoch: 1091 | Loss: 0.14091025292873383 | Test loss: 0.30243629217147827\n",
      "Epoch: 1092 | Loss: 0.14090730249881744 | Test loss: 0.302498996257782\n",
      "Epoch: 1093 | Loss: 0.14090435206890106 | Test loss: 0.30256175994873047\n",
      "Epoch: 1094 | Loss: 0.1409013867378235 | Test loss: 0.30262449383735657\n",
      "Epoch: 1095 | Loss: 0.1408984363079071 | Test loss: 0.3027050197124481\n",
      "Epoch: 1096 | Loss: 0.14089547097682953 | Test loss: 0.3027677834033966\n",
      "Epoch: 1097 | Loss: 0.14089252054691315 | Test loss: 0.3028305470943451\n",
      "Epoch: 1098 | Loss: 0.14088957011699677 | Test loss: 0.3028932809829712\n",
      "Epoch: 1099 | Loss: 0.1408866047859192 | Test loss: 0.3029560148715973\n",
      "Epoch: 1100 | Loss: 0.1408836543560028 | Test loss: 0.3030187785625458\n",
      "Epoch: 1101 | Loss: 0.14088068902492523 | Test loss: 0.3030815124511719\n",
      "Epoch: 1102 | Loss: 0.14087773859500885 | Test loss: 0.303144246339798\n",
      "Epoch: 1103 | Loss: 0.14087477326393127 | Test loss: 0.3032069802284241\n",
      "Epoch: 1104 | Loss: 0.1408718228340149 | Test loss: 0.30326971411705017\n",
      "Epoch: 1105 | Loss: 0.1408688724040985 | Test loss: 0.30333247780799866\n",
      "Epoch: 1106 | Loss: 0.14086590707302094 | Test loss: 0.30339521169662476\n",
      "Epoch: 1107 | Loss: 0.14086294174194336 | Test loss: 0.30345794558525085\n",
      "Epoch: 1108 | Loss: 0.14085997641086578 | Test loss: 0.30352067947387695\n",
      "Epoch: 1109 | Loss: 0.1408570408821106 | Test loss: 0.30358344316482544\n",
      "Epoch: 1110 | Loss: 0.14085407555103302 | Test loss: 0.3036462068557739\n",
      "Epoch: 1111 | Loss: 0.14085112512111664 | Test loss: 0.3037089407444\n",
      "Epoch: 1112 | Loss: 0.14084815979003906 | Test loss: 0.3037717044353485\n",
      "Epoch: 1113 | Loss: 0.14084520936012268 | Test loss: 0.3038344085216522\n",
      "Epoch: 1114 | Loss: 0.1408422589302063 | Test loss: 0.3038971722126007\n",
      "Epoch: 1115 | Loss: 0.1408393234014511 | Test loss: 0.3039728105068207\n",
      "Epoch: 1116 | Loss: 0.14083653688430786 | Test loss: 0.30404844880104065\n",
      "Epoch: 1117 | Loss: 0.14083372056484222 | Test loss: 0.3041240870952606\n",
      "Epoch: 1118 | Loss: 0.14083093404769897 | Test loss: 0.304199755191803\n",
      "Epoch: 1119 | Loss: 0.14082813262939453 | Test loss: 0.30427539348602295\n",
      "Epoch: 1120 | Loss: 0.1408253312110901 | Test loss: 0.3043510317802429\n",
      "Epoch: 1121 | Loss: 0.14082251489162445 | Test loss: 0.3044266700744629\n",
      "Epoch: 1122 | Loss: 0.1408197283744812 | Test loss: 0.30450233817100525\n",
      "Epoch: 1123 | Loss: 0.14081692695617676 | Test loss: 0.3045779764652252\n",
      "Epoch: 1124 | Loss: 0.14081412553787231 | Test loss: 0.3046536147594452\n",
      "Epoch: 1125 | Loss: 0.14081132411956787 | Test loss: 0.30472925305366516\n",
      "Epoch: 1126 | Loss: 0.140808567404747 | Test loss: 0.30478376150131226\n",
      "Epoch: 1127 | Loss: 0.14080588519573212 | Test loss: 0.30483829975128174\n",
      "Epoch: 1128 | Loss: 0.14080320298671722 | Test loss: 0.30489280819892883\n",
      "Epoch: 1129 | Loss: 0.14080053567886353 | Test loss: 0.3049473166465759\n",
      "Epoch: 1130 | Loss: 0.14079785346984863 | Test loss: 0.305001825094223\n",
      "Epoch: 1131 | Loss: 0.14079518616199493 | Test loss: 0.30507081747055054\n",
      "Epoch: 1132 | Loss: 0.1407925933599472 | Test loss: 0.30513978004455566\n",
      "Epoch: 1133 | Loss: 0.14079003036022186 | Test loss: 0.3052087426185608\n",
      "Epoch: 1134 | Loss: 0.14078745245933533 | Test loss: 0.3052777349948883\n",
      "Epoch: 1135 | Loss: 0.14078488945960999 | Test loss: 0.30534669756889343\n",
      "Epoch: 1136 | Loss: 0.14078231155872345 | Test loss: 0.30541568994522095\n",
      "Epoch: 1137 | Loss: 0.14077973365783691 | Test loss: 0.30548468232154846\n",
      "Epoch: 1138 | Loss: 0.14077717065811157 | Test loss: 0.3055536150932312\n",
      "Epoch: 1139 | Loss: 0.14077459275722504 | Test loss: 0.30562257766723633\n",
      "Epoch: 1140 | Loss: 0.1407720148563385 | Test loss: 0.30569157004356384\n",
      "Epoch: 1141 | Loss: 0.14076943695545197 | Test loss: 0.30576056241989136\n",
      "Epoch: 1142 | Loss: 0.14076685905456543 | Test loss: 0.3058295249938965\n",
      "Epoch: 1143 | Loss: 0.1407642960548401 | Test loss: 0.3058984875679016\n",
      "Epoch: 1144 | Loss: 0.14076171815395355 | Test loss: 0.3059674799442291\n",
      "Epoch: 1145 | Loss: 0.14075914025306702 | Test loss: 0.30603644251823425\n",
      "Epoch: 1146 | Loss: 0.14075660705566406 | Test loss: 0.3061181902885437\n",
      "Epoch: 1147 | Loss: 0.14075417816638947 | Test loss: 0.3061998784542084\n",
      "Epoch: 1148 | Loss: 0.14075173437595367 | Test loss: 0.30628156661987305\n",
      "Epoch: 1149 | Loss: 0.14074930548667908 | Test loss: 0.3063632845878601\n",
      "Epoch: 1150 | Loss: 0.1407468467950821 | Test loss: 0.30644500255584717\n",
      "Epoch: 1151 | Loss: 0.1407444179058075 | Test loss: 0.30652672052383423\n",
      "Epoch: 1152 | Loss: 0.1407419592142105 | Test loss: 0.3066084086894989\n",
      "Epoch: 1153 | Loss: 0.1407395750284195 | Test loss: 0.30667048692703247\n",
      "Epoch: 1154 | Loss: 0.14073722064495087 | Test loss: 0.3067325949668884\n",
      "Epoch: 1155 | Loss: 0.14073486626148224 | Test loss: 0.3067946135997772\n",
      "Epoch: 1156 | Loss: 0.1407325118780136 | Test loss: 0.3068567216396332\n",
      "Epoch: 1157 | Loss: 0.14073015749454498 | Test loss: 0.30691879987716675\n",
      "Epoch: 1158 | Loss: 0.14072781801223755 | Test loss: 0.3069808781147003\n",
      "Epoch: 1159 | Loss: 0.14072546362876892 | Test loss: 0.3070429265499115\n",
      "Epoch: 1160 | Loss: 0.1407231092453003 | Test loss: 0.30710500478744507\n",
      "Epoch: 1161 | Loss: 0.14072076976299286 | Test loss: 0.30716705322265625\n",
      "Epoch: 1162 | Loss: 0.14071841537952423 | Test loss: 0.3072291314601898\n",
      "Epoch: 1163 | Loss: 0.1407160609960556 | Test loss: 0.3072912096977234\n",
      "Epoch: 1164 | Loss: 0.14071370661258698 | Test loss: 0.30735328793525696\n",
      "Epoch: 1165 | Loss: 0.14071136713027954 | Test loss: 0.30741533637046814\n",
      "Epoch: 1166 | Loss: 0.1407090127468109 | Test loss: 0.3074774146080017\n",
      "Epoch: 1167 | Loss: 0.1407066434621811 | Test loss: 0.3075394630432129\n",
      "Epoch: 1168 | Loss: 0.14070431888103485 | Test loss: 0.30760157108306885\n",
      "Epoch: 1169 | Loss: 0.14070196449756622 | Test loss: 0.3076636493206024\n",
      "Epoch: 1170 | Loss: 0.1406996101140976 | Test loss: 0.3077256977558136\n",
      "Epoch: 1171 | Loss: 0.14069725573062897 | Test loss: 0.30778777599334717\n",
      "Epoch: 1172 | Loss: 0.14069490134716034 | Test loss: 0.30784982442855835\n",
      "Epoch: 1173 | Loss: 0.1406925618648529 | Test loss: 0.3079119324684143\n",
      "Epoch: 1174 | Loss: 0.14069020748138428 | Test loss: 0.3079740107059479\n",
      "Epoch: 1175 | Loss: 0.14068783819675446 | Test loss: 0.30803605914115906\n",
      "Epoch: 1176 | Loss: 0.14068551361560822 | Test loss: 0.30811065435409546\n",
      "Epoch: 1177 | Loss: 0.14068329334259033 | Test loss: 0.30818524956703186\n",
      "Epoch: 1178 | Loss: 0.14068108797073364 | Test loss: 0.30825984477996826\n",
      "Epoch: 1179 | Loss: 0.14067888259887695 | Test loss: 0.30833446979522705\n",
      "Epoch: 1180 | Loss: 0.14067666232585907 | Test loss: 0.30840903520584106\n",
      "Epoch: 1181 | Loss: 0.14067445695400238 | Test loss: 0.3084836006164551\n",
      "Epoch: 1182 | Loss: 0.14067226648330688 | Test loss: 0.3085725009441376\n",
      "Epoch: 1183 | Loss: 0.14067010581493378 | Test loss: 0.3086613714694977\n",
      "Epoch: 1184 | Loss: 0.14066794514656067 | Test loss: 0.3087502717971802\n",
      "Epoch: 1185 | Loss: 0.14066578447818756 | Test loss: 0.30883917212486267\n",
      "Epoch: 1186 | Loss: 0.14066363871097565 | Test loss: 0.30892807245254517\n",
      "Epoch: 1187 | Loss: 0.14066147804260254 | Test loss: 0.3090169131755829\n",
      "Epoch: 1188 | Loss: 0.14065931737422943 | Test loss: 0.3091058135032654\n",
      "Epoch: 1189 | Loss: 0.14065715670585632 | Test loss: 0.30919474363327026\n",
      "Epoch: 1190 | Loss: 0.14065499603748322 | Test loss: 0.309283584356308\n",
      "Epoch: 1191 | Loss: 0.1406528502702713 | Test loss: 0.30937251448631287\n",
      "Epoch: 1192 | Loss: 0.1406506896018982 | Test loss: 0.309461385011673\n",
      "Epoch: 1193 | Loss: 0.14064852893352509 | Test loss: 0.30955028533935547\n",
      "Epoch: 1194 | Loss: 0.14064638316631317 | Test loss: 0.30963918566703796\n",
      "Epoch: 1195 | Loss: 0.14064422249794006 | Test loss: 0.30972805619239807\n",
      "Epoch: 1196 | Loss: 0.14064206182956696 | Test loss: 0.3098169267177582\n",
      "Epoch: 1197 | Loss: 0.14063993096351624 | Test loss: 0.30988597869873047\n",
      "Epoch: 1198 | Loss: 0.14063788950443268 | Test loss: 0.30995503067970276\n",
      "Epoch: 1199 | Loss: 0.14063584804534912 | Test loss: 0.3100240230560303\n",
      "Epoch: 1200 | Loss: 0.14063382148742676 | Test loss: 0.3100930452346802\n",
      "Epoch: 1201 | Loss: 0.1406317800283432 | Test loss: 0.31016209721565247\n",
      "Epoch: 1202 | Loss: 0.14062973856925964 | Test loss: 0.31023111939430237\n",
      "Epoch: 1203 | Loss: 0.1406276971101761 | Test loss: 0.31030014157295227\n",
      "Epoch: 1204 | Loss: 0.14062567055225372 | Test loss: 0.31036919355392456\n",
      "Epoch: 1205 | Loss: 0.14062362909317017 | Test loss: 0.31043821573257446\n",
      "Epoch: 1206 | Loss: 0.1406215876340866 | Test loss: 0.31050723791122437\n",
      "Epoch: 1207 | Loss: 0.14061954617500305 | Test loss: 0.31057628989219666\n",
      "Epoch: 1208 | Loss: 0.1406175047159195 | Test loss: 0.31064531207084656\n",
      "Epoch: 1209 | Loss: 0.14061546325683594 | Test loss: 0.31071436405181885\n",
      "Epoch: 1210 | Loss: 0.14061342179775238 | Test loss: 0.31078338623046875\n",
      "Epoch: 1211 | Loss: 0.14061139523983002 | Test loss: 0.31085243821144104\n",
      "Epoch: 1212 | Loss: 0.14060935378074646 | Test loss: 0.31092146039009094\n",
      "Epoch: 1213 | Loss: 0.1406073123216629 | Test loss: 0.31099048256874084\n",
      "Epoch: 1214 | Loss: 0.14060527086257935 | Test loss: 0.31105953454971313\n",
      "Epoch: 1215 | Loss: 0.1406032294034958 | Test loss: 0.31112852692604065\n",
      "Epoch: 1216 | Loss: 0.14060118794441223 | Test loss: 0.31119757890701294\n",
      "Epoch: 1217 | Loss: 0.14059914648532867 | Test loss: 0.31126660108566284\n",
      "Epoch: 1218 | Loss: 0.14059710502624512 | Test loss: 0.3113356828689575\n",
      "Epoch: 1219 | Loss: 0.14059506356716156 | Test loss: 0.31140464544296265\n",
      "Epoch: 1220 | Loss: 0.1405930370092392 | Test loss: 0.31147369742393494\n",
      "Epoch: 1221 | Loss: 0.14059099555015564 | Test loss: 0.31154271960258484\n",
      "Epoch: 1222 | Loss: 0.14058895409107208 | Test loss: 0.31161177158355713\n",
      "Epoch: 1223 | Loss: 0.14058691263198853 | Test loss: 0.31168079376220703\n",
      "Epoch: 1224 | Loss: 0.14058488607406616 | Test loss: 0.3117498457431793\n",
      "Epoch: 1225 | Loss: 0.1405828446149826 | Test loss: 0.3118188679218292\n",
      "Epoch: 1226 | Loss: 0.14058080315589905 | Test loss: 0.3118878901004791\n",
      "Epoch: 1227 | Loss: 0.1405787616968155 | Test loss: 0.31195688247680664\n",
      "Epoch: 1228 | Loss: 0.14057672023773193 | Test loss: 0.31202593445777893\n",
      "Epoch: 1229 | Loss: 0.14057467877864838 | Test loss: 0.3120949864387512\n",
      "Epoch: 1230 | Loss: 0.140572652220726 | Test loss: 0.3121640086174011\n",
      "Epoch: 1231 | Loss: 0.14057061076164246 | Test loss: 0.312233030796051\n",
      "Epoch: 1232 | Loss: 0.1405685693025589 | Test loss: 0.3123020827770233\n",
      "Epoch: 1233 | Loss: 0.14056652784347534 | Test loss: 0.3123711049556732\n",
      "Epoch: 1234 | Loss: 0.14056450128555298 | Test loss: 0.3124220669269562\n",
      "Epoch: 1235 | Loss: 0.14056247472763062 | Test loss: 0.31247302889823914\n",
      "Epoch: 1236 | Loss: 0.14056047797203064 | Test loss: 0.3125239908695221\n",
      "Epoch: 1237 | Loss: 0.14055845141410828 | Test loss: 0.31257492303848267\n",
      "Epoch: 1238 | Loss: 0.1405564248561859 | Test loss: 0.31262585520744324\n",
      "Epoch: 1239 | Loss: 0.14055442810058594 | Test loss: 0.3126768171787262\n",
      "Epoch: 1240 | Loss: 0.14055240154266357 | Test loss: 0.3127077519893646\n",
      "Epoch: 1241 | Loss: 0.14055050909519196 | Test loss: 0.31278684735298157\n",
      "Epoch: 1242 | Loss: 0.1405486911535263 | Test loss: 0.3128478527069092\n",
      "Epoch: 1243 | Loss: 0.14054688811302185 | Test loss: 0.3129088878631592\n",
      "Epoch: 1244 | Loss: 0.1405450850725174 | Test loss: 0.3129698932170868\n",
      "Epoch: 1245 | Loss: 0.14054328203201294 | Test loss: 0.3130309283733368\n",
      "Epoch: 1246 | Loss: 0.1405414640903473 | Test loss: 0.3130919635295868\n",
      "Epoch: 1247 | Loss: 0.14053966104984283 | Test loss: 0.3131529688835144\n",
      "Epoch: 1248 | Loss: 0.14053785800933838 | Test loss: 0.3132140040397644\n",
      "Epoch: 1249 | Loss: 0.14053605496883392 | Test loss: 0.3132750391960144\n",
      "Epoch: 1250 | Loss: 0.14053425192832947 | Test loss: 0.3133360743522644\n",
      "Epoch: 1251 | Loss: 0.14053243398666382 | Test loss: 0.31339704990386963\n",
      "Epoch: 1252 | Loss: 0.14053064584732056 | Test loss: 0.31345808506011963\n",
      "Epoch: 1253 | Loss: 0.1405288279056549 | Test loss: 0.31351912021636963\n",
      "Epoch: 1254 | Loss: 0.14052700996398926 | Test loss: 0.31358012557029724\n",
      "Epoch: 1255 | Loss: 0.1405252069234848 | Test loss: 0.31364116072654724\n",
      "Epoch: 1256 | Loss: 0.14052341878414154 | Test loss: 0.31370216608047485\n",
      "Epoch: 1257 | Loss: 0.1405216008424759 | Test loss: 0.31376320123672485\n",
      "Epoch: 1258 | Loss: 0.14051978290081024 | Test loss: 0.31382423639297485\n",
      "Epoch: 1259 | Loss: 0.14051799476146698 | Test loss: 0.31388524174690247\n",
      "Epoch: 1260 | Loss: 0.14051617681980133 | Test loss: 0.3139462471008301\n",
      "Epoch: 1261 | Loss: 0.14051437377929688 | Test loss: 0.3140072524547577\n",
      "Epoch: 1262 | Loss: 0.14051257073879242 | Test loss: 0.3140683174133301\n",
      "Epoch: 1263 | Loss: 0.14051076769828796 | Test loss: 0.3141293227672577\n",
      "Epoch: 1264 | Loss: 0.14050894975662231 | Test loss: 0.3141903281211853\n",
      "Epoch: 1265 | Loss: 0.14050714671611786 | Test loss: 0.3142513632774353\n",
      "Epoch: 1266 | Loss: 0.1405053436756134 | Test loss: 0.3143123984336853\n",
      "Epoch: 1267 | Loss: 0.14050354063510895 | Test loss: 0.3143734037876129\n",
      "Epoch: 1268 | Loss: 0.1405017226934433 | Test loss: 0.3144344389438629\n",
      "Epoch: 1269 | Loss: 0.14049991965293884 | Test loss: 0.3144954741001129\n",
      "Epoch: 1270 | Loss: 0.1404981166124344 | Test loss: 0.31455644965171814\n",
      "Epoch: 1271 | Loss: 0.14049631357192993 | Test loss: 0.31461748480796814\n",
      "Epoch: 1272 | Loss: 0.14049451053142548 | Test loss: 0.31467851996421814\n",
      "Epoch: 1273 | Loss: 0.14049269258975983 | Test loss: 0.31473955512046814\n",
      "Epoch: 1274 | Loss: 0.14049088954925537 | Test loss: 0.31480056047439575\n",
      "Epoch: 1275 | Loss: 0.14048908650875092 | Test loss: 0.31486156582832336\n",
      "Epoch: 1276 | Loss: 0.14048728346824646 | Test loss: 0.31492260098457336\n",
      "Epoch: 1277 | Loss: 0.140485480427742 | Test loss: 0.31498363614082336\n",
      "Epoch: 1278 | Loss: 0.14048367738723755 | Test loss: 0.315044641494751\n",
      "Epoch: 1279 | Loss: 0.1404818594455719 | Test loss: 0.315105676651001\n",
      "Epoch: 1280 | Loss: 0.14048005640506744 | Test loss: 0.3151666820049286\n",
      "Epoch: 1281 | Loss: 0.1404782384634018 | Test loss: 0.3152277171611786\n",
      "Epoch: 1282 | Loss: 0.14047645032405853 | Test loss: 0.3152684271335602\n",
      "Epoch: 1283 | Loss: 0.14047472178936005 | Test loss: 0.3153091371059418\n",
      "Epoch: 1284 | Loss: 0.14047299325466156 | Test loss: 0.31536370515823364\n",
      "Epoch: 1285 | Loss: 0.14047135412693024 | Test loss: 0.3154182732105255\n",
      "Epoch: 1286 | Loss: 0.1404697448015213 | Test loss: 0.3154728412628174\n",
      "Epoch: 1287 | Loss: 0.14046810567378998 | Test loss: 0.31552740931510925\n",
      "Epoch: 1288 | Loss: 0.14046648144721985 | Test loss: 0.3155820071697235\n",
      "Epoch: 1289 | Loss: 0.14046484231948853 | Test loss: 0.315636545419693\n",
      "Epoch: 1290 | Loss: 0.1404632180929184 | Test loss: 0.31569114327430725\n",
      "Epoch: 1291 | Loss: 0.14046157896518707 | Test loss: 0.3157457113265991\n",
      "Epoch: 1292 | Loss: 0.14045995473861694 | Test loss: 0.3158002495765686\n",
      "Epoch: 1293 | Loss: 0.14045833051204681 | Test loss: 0.3158548176288605\n",
      "Epoch: 1294 | Loss: 0.1404566913843155 | Test loss: 0.31590938568115234\n",
      "Epoch: 1295 | Loss: 0.14045506715774536 | Test loss: 0.3159639835357666\n",
      "Epoch: 1296 | Loss: 0.14045342803001404 | Test loss: 0.31601855158805847\n",
      "Epoch: 1297 | Loss: 0.14045178890228271 | Test loss: 0.31607311964035034\n",
      "Epoch: 1298 | Loss: 0.14045016467571259 | Test loss: 0.3161276876926422\n",
      "Epoch: 1299 | Loss: 0.14044854044914246 | Test loss: 0.3161822557449341\n",
      "Epoch: 1300 | Loss: 0.14044690132141113 | Test loss: 0.31623682379722595\n",
      "Epoch: 1301 | Loss: 0.1404452621936798 | Test loss: 0.3162913918495178\n",
      "Epoch: 1302 | Loss: 0.14044365286827087 | Test loss: 0.3163459599018097\n",
      "Epoch: 1303 | Loss: 0.14044199883937836 | Test loss: 0.31640052795410156\n",
      "Epoch: 1304 | Loss: 0.14044038951396942 | Test loss: 0.31645506620407104\n",
      "Epoch: 1305 | Loss: 0.1404387503862381 | Test loss: 0.3165096640586853\n",
      "Epoch: 1306 | Loss: 0.14043712615966797 | Test loss: 0.3165642023086548\n",
      "Epoch: 1307 | Loss: 0.14043548703193665 | Test loss: 0.31661882996559143\n",
      "Epoch: 1308 | Loss: 0.14043384790420532 | Test loss: 0.3166733682155609\n",
      "Epoch: 1309 | Loss: 0.1404322236776352 | Test loss: 0.3167279362678528\n",
      "Epoch: 1310 | Loss: 0.14043059945106506 | Test loss: 0.31678253412246704\n",
      "Epoch: 1311 | Loss: 0.14042897522449493 | Test loss: 0.3168371021747589\n",
      "Epoch: 1312 | Loss: 0.1404273360967636 | Test loss: 0.3168916702270508\n",
      "Epoch: 1313 | Loss: 0.14042571187019348 | Test loss: 0.31694620847702026\n",
      "Epoch: 1314 | Loss: 0.14042407274246216 | Test loss: 0.3170008063316345\n",
      "Epoch: 1315 | Loss: 0.14042244851589203 | Test loss: 0.317055344581604\n",
      "Epoch: 1316 | Loss: 0.1404208093881607 | Test loss: 0.3171099126338959\n",
      "Epoch: 1317 | Loss: 0.14041918516159058 | Test loss: 0.31716451048851013\n",
      "Epoch: 1318 | Loss: 0.14041756093502045 | Test loss: 0.3172190487384796\n",
      "Epoch: 1319 | Loss: 0.14041592180728912 | Test loss: 0.3172736167907715\n",
      "Epoch: 1320 | Loss: 0.140414297580719 | Test loss: 0.31732818484306335\n",
      "Epoch: 1321 | Loss: 0.14041265845298767 | Test loss: 0.3173827826976776\n",
      "Epoch: 1322 | Loss: 0.14041104912757874 | Test loss: 0.31745097041130066\n",
      "Epoch: 1323 | Loss: 0.14040949940681458 | Test loss: 0.3175191581249237\n",
      "Epoch: 1324 | Loss: 0.1404079645872116 | Test loss: 0.317566841840744\n",
      "Epoch: 1325 | Loss: 0.1404065042734146 | Test loss: 0.31761449575424194\n",
      "Epoch: 1326 | Loss: 0.1404050588607788 | Test loss: 0.31766214966773987\n",
      "Epoch: 1327 | Loss: 0.140403613448143 | Test loss: 0.3177098333835602\n",
      "Epoch: 1328 | Loss: 0.140402153134346 | Test loss: 0.3177574872970581\n",
      "Epoch: 1329 | Loss: 0.140400692820549 | Test loss: 0.3178051710128784\n",
      "Epoch: 1330 | Loss: 0.1403992474079132 | Test loss: 0.31785282492637634\n",
      "Epoch: 1331 | Loss: 0.1403977870941162 | Test loss: 0.31790050864219666\n",
      "Epoch: 1332 | Loss: 0.1403963267803192 | Test loss: 0.3179481625556946\n",
      "Epoch: 1333 | Loss: 0.1403948813676834 | Test loss: 0.3179958164691925\n",
      "Epoch: 1334 | Loss: 0.1403934210538864 | Test loss: 0.3180435001850128\n",
      "Epoch: 1335 | Loss: 0.14039196074008942 | Test loss: 0.31809115409851074\n",
      "Epoch: 1336 | Loss: 0.14039050042629242 | Test loss: 0.31813883781433105\n",
      "Epoch: 1337 | Loss: 0.14038905501365662 | Test loss: 0.318186491727829\n",
      "Epoch: 1338 | Loss: 0.14038759469985962 | Test loss: 0.3182341754436493\n",
      "Epoch: 1339 | Loss: 0.14038613438606262 | Test loss: 0.3182818293571472\n",
      "Epoch: 1340 | Loss: 0.14038468897342682 | Test loss: 0.31832951307296753\n",
      "Epoch: 1341 | Loss: 0.14038324356079102 | Test loss: 0.31837713718414307\n",
      "Epoch: 1342 | Loss: 0.1403817981481552 | Test loss: 0.3184248208999634\n",
      "Epoch: 1343 | Loss: 0.14038033783435822 | Test loss: 0.3184725046157837\n",
      "Epoch: 1344 | Loss: 0.14037887752056122 | Test loss: 0.318520188331604\n",
      "Epoch: 1345 | Loss: 0.14037741720676422 | Test loss: 0.31856781244277954\n",
      "Epoch: 1346 | Loss: 0.14037595689296722 | Test loss: 0.31861549615859985\n",
      "Epoch: 1347 | Loss: 0.14037451148033142 | Test loss: 0.31866317987442017\n",
      "Epoch: 1348 | Loss: 0.14037305116653442 | Test loss: 0.31872668862342834\n",
      "Epoch: 1349 | Loss: 0.14037162065505981 | Test loss: 0.3187901973724365\n",
      "Epoch: 1350 | Loss: 0.1403701901435852 | Test loss: 0.3188537359237671\n",
      "Epoch: 1351 | Loss: 0.1403687596321106 | Test loss: 0.31891727447509766\n",
      "Epoch: 1352 | Loss: 0.14036735892295837 | Test loss: 0.31898078322410583\n",
      "Epoch: 1353 | Loss: 0.14036591351032257 | Test loss: 0.3190443217754364\n",
      "Epoch: 1354 | Loss: 0.14036449790000916 | Test loss: 0.31910786032676697\n",
      "Epoch: 1355 | Loss: 0.14036306738853455 | Test loss: 0.31917136907577515\n",
      "Epoch: 1356 | Loss: 0.14036163687705994 | Test loss: 0.3192348778247833\n",
      "Epoch: 1357 | Loss: 0.14036020636558533 | Test loss: 0.3192983865737915\n",
      "Epoch: 1358 | Loss: 0.14035877585411072 | Test loss: 0.31936192512512207\n",
      "Epoch: 1359 | Loss: 0.1403573602437973 | Test loss: 0.3194388747215271\n",
      "Epoch: 1360 | Loss: 0.14035600423812866 | Test loss: 0.31951579451560974\n",
      "Epoch: 1361 | Loss: 0.1403546929359436 | Test loss: 0.3195537030696869\n",
      "Epoch: 1362 | Loss: 0.14035345613956451 | Test loss: 0.31959155201911926\n",
      "Epoch: 1363 | Loss: 0.14035223424434662 | Test loss: 0.31962940096855164\n",
      "Epoch: 1364 | Loss: 0.14035101234912872 | Test loss: 0.31966733932495117\n",
      "Epoch: 1365 | Loss: 0.14034977555274963 | Test loss: 0.31970521807670593\n",
      "Epoch: 1366 | Loss: 0.14034856855869293 | Test loss: 0.3197430968284607\n",
      "Epoch: 1367 | Loss: 0.14034733176231384 | Test loss: 0.31978097558021545\n",
      "Epoch: 1368 | Loss: 0.14034610986709595 | Test loss: 0.3198188245296478\n",
      "Epoch: 1369 | Loss: 0.14034488797187805 | Test loss: 0.3198567032814026\n",
      "Epoch: 1370 | Loss: 0.14034366607666016 | Test loss: 0.31989461183547974\n",
      "Epoch: 1371 | Loss: 0.14034242928028107 | Test loss: 0.3199324607849121\n",
      "Epoch: 1372 | Loss: 0.14034120738506317 | Test loss: 0.31997033953666687\n",
      "Epoch: 1373 | Loss: 0.14033998548984528 | Test loss: 0.32000821828842163\n",
      "Epoch: 1374 | Loss: 0.1403387486934662 | Test loss: 0.3200461268424988\n",
      "Epoch: 1375 | Loss: 0.1403375267982483 | Test loss: 0.32008400559425354\n",
      "Epoch: 1376 | Loss: 0.1403363049030304 | Test loss: 0.3201218843460083\n",
      "Epoch: 1377 | Loss: 0.1403350830078125 | Test loss: 0.32015976309776306\n",
      "Epoch: 1378 | Loss: 0.1403338611125946 | Test loss: 0.32019758224487305\n",
      "Epoch: 1379 | Loss: 0.14033262431621552 | Test loss: 0.3202355206012726\n",
      "Epoch: 1380 | Loss: 0.14033140242099762 | Test loss: 0.32027333974838257\n",
      "Epoch: 1381 | Loss: 0.14033018052577972 | Test loss: 0.3203112781047821\n",
      "Epoch: 1382 | Loss: 0.14032895863056183 | Test loss: 0.32034915685653687\n",
      "Epoch: 1383 | Loss: 0.14032773673534393 | Test loss: 0.3203870356082916\n",
      "Epoch: 1384 | Loss: 0.14032649993896484 | Test loss: 0.320424884557724\n",
      "Epoch: 1385 | Loss: 0.14032527804374695 | Test loss: 0.32046279311180115\n",
      "Epoch: 1386 | Loss: 0.14032405614852905 | Test loss: 0.3205006718635559\n",
      "Epoch: 1387 | Loss: 0.14032281935214996 | Test loss: 0.32053855061531067\n",
      "Epoch: 1388 | Loss: 0.14032159745693207 | Test loss: 0.32057639956474304\n",
      "Epoch: 1389 | Loss: 0.14032039046287537 | Test loss: 0.32062751054763794\n",
      "Epoch: 1390 | Loss: 0.14031925797462463 | Test loss: 0.32067856192588806\n",
      "Epoch: 1391 | Loss: 0.1403181254863739 | Test loss: 0.32072964310646057\n",
      "Epoch: 1392 | Loss: 0.14031700789928436 | Test loss: 0.3207806646823883\n",
      "Epoch: 1393 | Loss: 0.14031587541103363 | Test loss: 0.3208317458629608\n",
      "Epoch: 1394 | Loss: 0.1403147429227829 | Test loss: 0.32088279724121094\n",
      "Epoch: 1395 | Loss: 0.14031361043453217 | Test loss: 0.32093387842178345\n",
      "Epoch: 1396 | Loss: 0.14031249284744263 | Test loss: 0.32098495960235596\n",
      "Epoch: 1397 | Loss: 0.1403113454580307 | Test loss: 0.3210359811782837\n",
      "Epoch: 1398 | Loss: 0.14031022787094116 | Test loss: 0.3210870325565338\n",
      "Epoch: 1399 | Loss: 0.14030909538269043 | Test loss: 0.3211381137371063\n",
      "Epoch: 1400 | Loss: 0.1403079628944397 | Test loss: 0.32118913531303406\n",
      "Epoch: 1401 | Loss: 0.14030683040618896 | Test loss: 0.32124021649360657\n",
      "Epoch: 1402 | Loss: 0.14030571281909943 | Test loss: 0.3212912976741791\n",
      "Epoch: 1403 | Loss: 0.1403045654296875 | Test loss: 0.32132139801979065\n",
      "Epoch: 1404 | Loss: 0.14030350744724274 | Test loss: 0.32135146856307983\n",
      "Epoch: 1405 | Loss: 0.14030244946479797 | Test loss: 0.3213815689086914\n",
      "Epoch: 1406 | Loss: 0.1403013914823532 | Test loss: 0.321411669254303\n",
      "Epoch: 1407 | Loss: 0.14030033349990845 | Test loss: 0.3214417099952698\n",
      "Epoch: 1408 | Loss: 0.14029927551746368 | Test loss: 0.32147181034088135\n",
      "Epoch: 1409 | Loss: 0.14029821753501892 | Test loss: 0.3215019404888153\n",
      "Epoch: 1410 | Loss: 0.14029715955257416 | Test loss: 0.3215319812297821\n",
      "Epoch: 1411 | Loss: 0.1402960866689682 | Test loss: 0.32156211137771606\n",
      "Epoch: 1412 | Loss: 0.14029504358768463 | Test loss: 0.32159218192100525\n",
      "Epoch: 1413 | Loss: 0.14029397070407867 | Test loss: 0.3216223120689392\n",
      "Epoch: 1414 | Loss: 0.1402929127216339 | Test loss: 0.321652352809906\n",
      "Epoch: 1415 | Loss: 0.14029185473918915 | Test loss: 0.32168251276016235\n",
      "Epoch: 1416 | Loss: 0.14029079675674438 | Test loss: 0.32171255350112915\n",
      "Epoch: 1417 | Loss: 0.14028973877429962 | Test loss: 0.3217427134513855\n",
      "Epoch: 1418 | Loss: 0.14028868079185486 | Test loss: 0.3217727541923523\n",
      "Epoch: 1419 | Loss: 0.1402876377105713 | Test loss: 0.3218157887458801\n",
      "Epoch: 1420 | Loss: 0.1402866691350937 | Test loss: 0.32185888290405273\n",
      "Epoch: 1421 | Loss: 0.14028571546077728 | Test loss: 0.32190191745758057\n",
      "Epoch: 1422 | Loss: 0.14028476178646088 | Test loss: 0.3219449520111084\n",
      "Epoch: 1423 | Loss: 0.14028379321098328 | Test loss: 0.3219880163669586\n",
      "Epoch: 1424 | Loss: 0.14028283953666687 | Test loss: 0.32203108072280884\n",
      "Epoch: 1425 | Loss: 0.14028187096118927 | Test loss: 0.32207411527633667\n",
      "Epoch: 1426 | Loss: 0.14028091728687286 | Test loss: 0.3221171796321869\n",
      "Epoch: 1427 | Loss: 0.14027996361255646 | Test loss: 0.32217586040496826\n",
      "Epoch: 1428 | Loss: 0.14027902483940125 | Test loss: 0.32223454117774963\n",
      "Epoch: 1429 | Loss: 0.14027807116508484 | Test loss: 0.322293221950531\n",
      "Epoch: 1430 | Loss: 0.14027713239192963 | Test loss: 0.3223519027233124\n",
      "Epoch: 1431 | Loss: 0.14027617871761322 | Test loss: 0.32240280508995056\n",
      "Epoch: 1432 | Loss: 0.14027522504329681 | Test loss: 0.32246145606040955\n",
      "Epoch: 1433 | Loss: 0.1402742862701416 | Test loss: 0.3225201666355133\n",
      "Epoch: 1434 | Loss: 0.1402733325958252 | Test loss: 0.3225788474082947\n",
      "Epoch: 1435 | Loss: 0.1402723789215088 | Test loss: 0.32263755798339844\n",
      "Epoch: 1436 | Loss: 0.14027144014835358 | Test loss: 0.3226962089538574\n",
      "Epoch: 1437 | Loss: 0.14027050137519836 | Test loss: 0.32273924350738525\n",
      "Epoch: 1438 | Loss: 0.14026954770088196 | Test loss: 0.3227979242801666\n",
      "Epoch: 1439 | Loss: 0.14026860892772675 | Test loss: 0.3228566348552704\n",
      "Epoch: 1440 | Loss: 0.14026767015457153 | Test loss: 0.3228941261768341\n",
      "Epoch: 1441 | Loss: 0.14026683568954468 | Test loss: 0.3229316174983978\n",
      "Epoch: 1442 | Loss: 0.14026597142219543 | Test loss: 0.32296913862228394\n",
      "Epoch: 1443 | Loss: 0.14026513695716858 | Test loss: 0.3230065703392029\n",
      "Epoch: 1444 | Loss: 0.14026427268981934 | Test loss: 0.3230441212654114\n",
      "Epoch: 1445 | Loss: 0.1402634233236313 | Test loss: 0.3230815827846527\n",
      "Epoch: 1446 | Loss: 0.14026258885860443 | Test loss: 0.32311907410621643\n",
      "Epoch: 1447 | Loss: 0.1402617245912552 | Test loss: 0.32315656542778015\n",
      "Epoch: 1448 | Loss: 0.14026087522506714 | Test loss: 0.3231940269470215\n",
      "Epoch: 1449 | Loss: 0.1402600258588791 | Test loss: 0.32323157787323\n",
      "Epoch: 1450 | Loss: 0.14025917649269104 | Test loss: 0.3232690393924713\n",
      "Epoch: 1451 | Loss: 0.140258327126503 | Test loss: 0.3233065605163574\n",
      "Epoch: 1452 | Loss: 0.14025746285915375 | Test loss: 0.32334402203559875\n",
      "Epoch: 1453 | Loss: 0.1402566283941269 | Test loss: 0.32338154315948486\n",
      "Epoch: 1454 | Loss: 0.14025577902793884 | Test loss: 0.3234190344810486\n",
      "Epoch: 1455 | Loss: 0.1402549147605896 | Test loss: 0.3234564960002899\n",
      "Epoch: 1456 | Loss: 0.14025411009788513 | Test loss: 0.32350677251815796\n",
      "Epoch: 1457 | Loss: 0.14025333523750305 | Test loss: 0.32355695962905884\n",
      "Epoch: 1458 | Loss: 0.14025256037712097 | Test loss: 0.3236072063446045\n",
      "Epoch: 1459 | Loss: 0.1402517855167389 | Test loss: 0.32365745306015015\n",
      "Epoch: 1460 | Loss: 0.1402510106563568 | Test loss: 0.3237076997756958\n",
      "Epoch: 1461 | Loss: 0.14025022089481354 | Test loss: 0.32375791668891907\n",
      "Epoch: 1462 | Loss: 0.14024946093559265 | Test loss: 0.32380813360214233\n",
      "Epoch: 1463 | Loss: 0.14024867117404938 | Test loss: 0.323858380317688\n",
      "Epoch: 1464 | Loss: 0.1402478963136673 | Test loss: 0.32390856742858887\n",
      "Epoch: 1465 | Loss: 0.14024712145328522 | Test loss: 0.3239588141441345\n",
      "Epoch: 1466 | Loss: 0.14024634659290314 | Test loss: 0.3240090608596802\n",
      "Epoch: 1467 | Loss: 0.14024557173252106 | Test loss: 0.32405927777290344\n",
      "Epoch: 1468 | Loss: 0.14024478197097778 | Test loss: 0.3241094946861267\n",
      "Epoch: 1469 | Loss: 0.1402440071105957 | Test loss: 0.32415977120399475\n",
      "Epoch: 1470 | Loss: 0.14024324715137482 | Test loss: 0.32420995831489563\n",
      "Epoch: 1471 | Loss: 0.14024247229099274 | Test loss: 0.3242602050304413\n",
      "Epoch: 1472 | Loss: 0.14024168252944946 | Test loss: 0.32431045174598694\n",
      "Epoch: 1473 | Loss: 0.14024090766906738 | Test loss: 0.3243606388568878\n",
      "Epoch: 1474 | Loss: 0.1402401477098465 | Test loss: 0.32441091537475586\n",
      "Epoch: 1475 | Loss: 0.14023934304714203 | Test loss: 0.3244611322879791\n",
      "Epoch: 1476 | Loss: 0.14023858308792114 | Test loss: 0.32451140880584717\n",
      "Epoch: 1477 | Loss: 0.14023780822753906 | Test loss: 0.3245430588722229\n",
      "Epoch: 1478 | Loss: 0.14023706316947937 | Test loss: 0.3245747983455658\n",
      "Epoch: 1479 | Loss: 0.14023631811141968 | Test loss: 0.3246064782142639\n",
      "Epoch: 1480 | Loss: 0.14023557305335999 | Test loss: 0.3246382176876068\n",
      "Epoch: 1481 | Loss: 0.1402348279953003 | Test loss: 0.32466989755630493\n",
      "Epoch: 1482 | Loss: 0.1402340829372406 | Test loss: 0.32470160722732544\n",
      "Epoch: 1483 | Loss: 0.1402333378791809 | Test loss: 0.32473334670066833\n",
      "Epoch: 1484 | Loss: 0.14023259282112122 | Test loss: 0.32476499676704407\n",
      "Epoch: 1485 | Loss: 0.14023184776306152 | Test loss: 0.32479673624038696\n",
      "Epoch: 1486 | Loss: 0.14023111760616302 | Test loss: 0.32482844591140747\n",
      "Epoch: 1487 | Loss: 0.14023037254810333 | Test loss: 0.324860155582428\n",
      "Epoch: 1488 | Loss: 0.14022961258888245 | Test loss: 0.3248918056488037\n",
      "Epoch: 1489 | Loss: 0.14022886753082275 | Test loss: 0.3249235451221466\n",
      "Epoch: 1490 | Loss: 0.14022812247276306 | Test loss: 0.3249552845954895\n",
      "Epoch: 1491 | Loss: 0.14022737741470337 | Test loss: 0.32498693466186523\n",
      "Epoch: 1492 | Loss: 0.14022666215896606 | Test loss: 0.32503119111061096\n",
      "Epoch: 1493 | Loss: 0.14022599160671234 | Test loss: 0.3250754177570343\n",
      "Epoch: 1494 | Loss: 0.1402253359556198 | Test loss: 0.32511961460113525\n",
      "Epoch: 1495 | Loss: 0.1402246654033661 | Test loss: 0.325163871049881\n",
      "Epoch: 1496 | Loss: 0.14022399485111237 | Test loss: 0.3252081274986267\n",
      "Epoch: 1497 | Loss: 0.14022332429885864 | Test loss: 0.3252522945404053\n",
      "Epoch: 1498 | Loss: 0.14022265374660492 | Test loss: 0.325296550989151\n",
      "Epoch: 1499 | Loss: 0.1402219980955124 | Test loss: 0.32534074783325195\n",
      "Epoch: 1500 | Loss: 0.14022134244441986 | Test loss: 0.3253849744796753\n",
      "Epoch: 1501 | Loss: 0.14022065699100494 | Test loss: 0.325429230928421\n",
      "Epoch: 1502 | Loss: 0.14021998643875122 | Test loss: 0.325473427772522\n",
      "Epoch: 1503 | Loss: 0.1402193307876587 | Test loss: 0.3255176544189453\n",
      "Epoch: 1504 | Loss: 0.14021866023540497 | Test loss: 0.32556185126304626\n",
      "Epoch: 1505 | Loss: 0.14021800458431244 | Test loss: 0.3256060779094696\n",
      "Epoch: 1506 | Loss: 0.14021733403205872 | Test loss: 0.32565033435821533\n",
      "Epoch: 1507 | Loss: 0.140216663479805 | Test loss: 0.32569456100463867\n",
      "Epoch: 1508 | Loss: 0.14021600782871246 | Test loss: 0.325738787651062\n",
      "Epoch: 1509 | Loss: 0.14021532237529755 | Test loss: 0.32578298449516296\n",
      "Epoch: 1510 | Loss: 0.14021466672420502 | Test loss: 0.3258272111415863\n",
      "Epoch: 1511 | Loss: 0.1402139961719513 | Test loss: 0.32587140798568726\n",
      "Epoch: 1512 | Loss: 0.14021332561969757 | Test loss: 0.325915664434433\n",
      "Epoch: 1513 | Loss: 0.14021266996860504 | Test loss: 0.3259599208831787\n",
      "Epoch: 1514 | Loss: 0.14021199941635132 | Test loss: 0.3260040879249573\n",
      "Epoch: 1515 | Loss: 0.1402113288640976 | Test loss: 0.326048344373703\n",
      "Epoch: 1516 | Loss: 0.14021065831184387 | Test loss: 0.32609257102012634\n",
      "Epoch: 1517 | Loss: 0.14021000266075134 | Test loss: 0.3261367678642273\n",
      "Epoch: 1518 | Loss: 0.14020933210849762 | Test loss: 0.326181024312973\n",
      "Epoch: 1519 | Loss: 0.1402086615562439 | Test loss: 0.326225221157074\n",
      "Epoch: 1520 | Loss: 0.14020799100399017 | Test loss: 0.3262694478034973\n",
      "Epoch: 1521 | Loss: 0.14020732045173645 | Test loss: 0.32631370425224304\n",
      "Epoch: 1522 | Loss: 0.14020666480064392 | Test loss: 0.3263578712940216\n",
      "Epoch: 1523 | Loss: 0.1402059942483902 | Test loss: 0.32640212774276733\n",
      "Epoch: 1524 | Loss: 0.14020532369613647 | Test loss: 0.3264463245868683\n",
      "Epoch: 1525 | Loss: 0.14020465314388275 | Test loss: 0.3264905512332916\n",
      "Epoch: 1526 | Loss: 0.14020398259162903 | Test loss: 0.32653480768203735\n",
      "Epoch: 1527 | Loss: 0.1402033269405365 | Test loss: 0.3265790045261383\n",
      "Epoch: 1528 | Loss: 0.14020265638828278 | Test loss: 0.32662326097488403\n",
      "Epoch: 1529 | Loss: 0.14020198583602905 | Test loss: 0.3266674876213074\n",
      "Epoch: 1530 | Loss: 0.14020131528377533 | Test loss: 0.3267116844654083\n",
      "Epoch: 1531 | Loss: 0.1402006596326828 | Test loss: 0.32675591111183167\n",
      "Epoch: 1532 | Loss: 0.14019998908042908 | Test loss: 0.326800137758255\n",
      "Epoch: 1533 | Loss: 0.14019931852817535 | Test loss: 0.32684436440467834\n",
      "Epoch: 1534 | Loss: 0.14019866287708282 | Test loss: 0.3268885612487793\n",
      "Epoch: 1535 | Loss: 0.1401979774236679 | Test loss: 0.326932817697525\n",
      "Epoch: 1536 | Loss: 0.14019732177257538 | Test loss: 0.32697704434394836\n",
      "Epoch: 1537 | Loss: 0.14019665122032166 | Test loss: 0.3270212411880493\n",
      "Epoch: 1538 | Loss: 0.14019599556922913 | Test loss: 0.32706549763679504\n",
      "Epoch: 1539 | Loss: 0.1401953250169754 | Test loss: 0.3271097242832184\n",
      "Epoch: 1540 | Loss: 0.14019465446472168 | Test loss: 0.32715392112731934\n",
      "Epoch: 1541 | Loss: 0.14019399881362915 | Test loss: 0.32719817757606506\n",
      "Epoch: 1542 | Loss: 0.14019332826137543 | Test loss: 0.32724234461784363\n",
      "Epoch: 1543 | Loss: 0.1401926428079605 | Test loss: 0.32728660106658936\n",
      "Epoch: 1544 | Loss: 0.14019198715686798 | Test loss: 0.32731208205223083\n",
      "Epoch: 1545 | Loss: 0.14019134640693665 | Test loss: 0.3273375630378723\n",
      "Epoch: 1546 | Loss: 0.1401907056570053 | Test loss: 0.3273630142211914\n",
      "Epoch: 1547 | Loss: 0.14019006490707397 | Test loss: 0.3273885250091553\n",
      "Epoch: 1548 | Loss: 0.14018942415714264 | Test loss: 0.32741397619247437\n",
      "Epoch: 1549 | Loss: 0.1401887834072113 | Test loss: 0.32743945717811584\n",
      "Epoch: 1550 | Loss: 0.14018814265727997 | Test loss: 0.3274649381637573\n",
      "Epoch: 1551 | Loss: 0.14018750190734863 | Test loss: 0.3274904191493988\n",
      "Epoch: 1552 | Loss: 0.1401868611574173 | Test loss: 0.3275158703327179\n",
      "Epoch: 1553 | Loss: 0.14018622040748596 | Test loss: 0.32754138112068176\n",
      "Epoch: 1554 | Loss: 0.14018557965755463 | Test loss: 0.32756683230400085\n",
      "Epoch: 1555 | Loss: 0.1401849389076233 | Test loss: 0.32759231328964233\n",
      "Epoch: 1556 | Loss: 0.14018429815769196 | Test loss: 0.3276177942752838\n",
      "Epoch: 1557 | Loss: 0.14018365740776062 | Test loss: 0.3276432752609253\n",
      "Epoch: 1558 | Loss: 0.14018301665782928 | Test loss: 0.3276687562465668\n",
      "Epoch: 1559 | Loss: 0.14018237590789795 | Test loss: 0.32769423723220825\n",
      "Epoch: 1560 | Loss: 0.1401817500591278 | Test loss: 0.32773512601852417\n",
      "Epoch: 1561 | Loss: 0.14018113911151886 | Test loss: 0.3277759850025177\n",
      "Epoch: 1562 | Loss: 0.1401805281639099 | Test loss: 0.3278168737888336\n",
      "Epoch: 1563 | Loss: 0.14017993211746216 | Test loss: 0.32785773277282715\n",
      "Epoch: 1564 | Loss: 0.14017930626869202 | Test loss: 0.32789862155914307\n",
      "Epoch: 1565 | Loss: 0.14017871022224426 | Test loss: 0.327939510345459\n",
      "Epoch: 1566 | Loss: 0.14017808437347412 | Test loss: 0.3279804289340973\n",
      "Epoch: 1567 | Loss: 0.14017747342586517 | Test loss: 0.3280213177204132\n",
      "Epoch: 1568 | Loss: 0.14017686247825623 | Test loss: 0.3280622065067291\n",
      "Epoch: 1569 | Loss: 0.14017625153064728 | Test loss: 0.32810306549072266\n",
      "Epoch: 1570 | Loss: 0.14017564058303833 | Test loss: 0.3281439542770386\n",
      "Epoch: 1571 | Loss: 0.14017504453659058 | Test loss: 0.3281848430633545\n",
      "Epoch: 1572 | Loss: 0.14017441868782043 | Test loss: 0.328225702047348\n",
      "Epoch: 1573 | Loss: 0.1401738077402115 | Test loss: 0.32826659083366394\n",
      "Epoch: 1574 | Loss: 0.14017319679260254 | Test loss: 0.32830747961997986\n",
      "Epoch: 1575 | Loss: 0.1401725858449936 | Test loss: 0.3283483684062958\n",
      "Epoch: 1576 | Loss: 0.14017198979854584 | Test loss: 0.3283892869949341\n",
      "Epoch: 1577 | Loss: 0.1401713788509369 | Test loss: 0.32843017578125\n",
      "Epoch: 1578 | Loss: 0.14017076790332794 | Test loss: 0.3284710645675659\n",
      "Epoch: 1579 | Loss: 0.140170156955719 | Test loss: 0.32851195335388184\n",
      "Epoch: 1580 | Loss: 0.14016954600811005 | Test loss: 0.32855284214019775\n",
      "Epoch: 1581 | Loss: 0.1401689350605011 | Test loss: 0.3285937011241913\n",
      "Epoch: 1582 | Loss: 0.14016832411289215 | Test loss: 0.3286345601081848\n",
      "Epoch: 1583 | Loss: 0.1401677131652832 | Test loss: 0.32867544889450073\n",
      "Epoch: 1584 | Loss: 0.14016710221767426 | Test loss: 0.32871633768081665\n",
      "Epoch: 1585 | Loss: 0.1401665061712265 | Test loss: 0.32875722646713257\n",
      "Epoch: 1586 | Loss: 0.14016588032245636 | Test loss: 0.3287981450557709\n",
      "Epoch: 1587 | Loss: 0.1401652842760086 | Test loss: 0.3288390338420868\n",
      "Epoch: 1588 | Loss: 0.14016467332839966 | Test loss: 0.3288798928260803\n",
      "Epoch: 1589 | Loss: 0.1401640623807907 | Test loss: 0.32892078161239624\n",
      "Epoch: 1590 | Loss: 0.14016345143318176 | Test loss: 0.32896167039871216\n",
      "Epoch: 1591 | Loss: 0.14016284048557281 | Test loss: 0.3290025293827057\n",
      "Epoch: 1592 | Loss: 0.14016222953796387 | Test loss: 0.3290434181690216\n",
      "Epoch: 1593 | Loss: 0.14016161859035492 | Test loss: 0.3290843367576599\n",
      "Epoch: 1594 | Loss: 0.14016100764274597 | Test loss: 0.32912519574165344\n",
      "Epoch: 1595 | Loss: 0.14016041159629822 | Test loss: 0.32916611433029175\n",
      "Epoch: 1596 | Loss: 0.14015978574752808 | Test loss: 0.3292069435119629\n",
      "Epoch: 1597 | Loss: 0.14015917479991913 | Test loss: 0.3292478621006012\n",
      "Epoch: 1598 | Loss: 0.14015856385231018 | Test loss: 0.3292887508869171\n",
      "Epoch: 1599 | Loss: 0.14015795290470123 | Test loss: 0.32932963967323303\n",
      "Epoch: 1600 | Loss: 0.14015735685825348 | Test loss: 0.32937052845954895\n",
      "Epoch: 1601 | Loss: 0.14015674591064453 | Test loss: 0.3294113874435425\n",
      "Epoch: 1602 | Loss: 0.14015613496303558 | Test loss: 0.3294522762298584\n",
      "Epoch: 1603 | Loss: 0.14015552401542664 | Test loss: 0.3294931650161743\n",
      "Epoch: 1604 | Loss: 0.1401549130678177 | Test loss: 0.32953405380249023\n",
      "Epoch: 1605 | Loss: 0.14015430212020874 | Test loss: 0.32957497239112854\n",
      "Epoch: 1606 | Loss: 0.1401536911725998 | Test loss: 0.3296158015727997\n",
      "Epoch: 1607 | Loss: 0.14015308022499084 | Test loss: 0.329656720161438\n",
      "Epoch: 1608 | Loss: 0.1401524692773819 | Test loss: 0.3296976089477539\n",
      "Epoch: 1609 | Loss: 0.14015185832977295 | Test loss: 0.32973846793174744\n",
      "Epoch: 1610 | Loss: 0.1401512622833252 | Test loss: 0.3297604024410248\n",
      "Epoch: 1611 | Loss: 0.14015066623687744 | Test loss: 0.3297823369503021\n",
      "Epoch: 1612 | Loss: 0.14015008509159088 | Test loss: 0.3298042118549347\n",
      "Epoch: 1613 | Loss: 0.14014948904514313 | Test loss: 0.32982611656188965\n",
      "Epoch: 1614 | Loss: 0.14014890789985657 | Test loss: 0.329848051071167\n",
      "Epoch: 1615 | Loss: 0.14014832675457 | Test loss: 0.32986998558044434\n",
      "Epoch: 1616 | Loss: 0.14014774560928345 | Test loss: 0.3298918902873993\n",
      "Epoch: 1617 | Loss: 0.1401471495628357 | Test loss: 0.32991382479667664\n",
      "Epoch: 1618 | Loss: 0.14014656841754913 | Test loss: 0.3299357295036316\n",
      "Epoch: 1619 | Loss: 0.14014598727226257 | Test loss: 0.32995766401290894\n",
      "Epoch: 1620 | Loss: 0.14014539122581482 | Test loss: 0.3299795389175415\n",
      "Epoch: 1621 | Loss: 0.14014481008052826 | Test loss: 0.33000144362449646\n",
      "Epoch: 1622 | Loss: 0.1401442140340805 | Test loss: 0.3300234079360962\n",
      "Epoch: 1623 | Loss: 0.14014363288879395 | Test loss: 0.33004531264305115\n",
      "Epoch: 1624 | Loss: 0.14014305174350739 | Test loss: 0.3300672173500061\n",
      "Epoch: 1625 | Loss: 0.14014245569705963 | Test loss: 0.33008915185928345\n",
      "Epoch: 1626 | Loss: 0.14014188945293427 | Test loss: 0.3301110863685608\n",
      "Epoch: 1627 | Loss: 0.1401412934064865 | Test loss: 0.33013296127319336\n",
      "Epoch: 1628 | Loss: 0.14014069736003876 | Test loss: 0.3301548659801483\n",
      "Epoch: 1629 | Loss: 0.1401401162147522 | Test loss: 0.33017680048942566\n",
      "Epoch: 1630 | Loss: 0.14013953506946564 | Test loss: 0.330198734998703\n",
      "Epoch: 1631 | Loss: 0.14013895392417908 | Test loss: 0.33022063970565796\n",
      "Epoch: 1632 | Loss: 0.14013835787773132 | Test loss: 0.3302425444126129\n",
      "Epoch: 1633 | Loss: 0.14013777673244476 | Test loss: 0.33026447892189026\n",
      "Epoch: 1634 | Loss: 0.1401371955871582 | Test loss: 0.3302864134311676\n",
      "Epoch: 1635 | Loss: 0.14013659954071045 | Test loss: 0.33030828833580017\n",
      "Epoch: 1636 | Loss: 0.1401360183954239 | Test loss: 0.33034542202949524\n",
      "Epoch: 1637 | Loss: 0.1401354819536209 | Test loss: 0.33038246631622314\n",
      "Epoch: 1638 | Loss: 0.14013491570949554 | Test loss: 0.3304195702075958\n",
      "Epoch: 1639 | Loss: 0.14013436436653137 | Test loss: 0.3304566740989685\n",
      "Epoch: 1640 | Loss: 0.1401338279247284 | Test loss: 0.3304938077926636\n",
      "Epoch: 1641 | Loss: 0.14013327658176422 | Test loss: 0.33053091168403625\n",
      "Epoch: 1642 | Loss: 0.14013272523880005 | Test loss: 0.33056801557540894\n",
      "Epoch: 1643 | Loss: 0.14013217389583588 | Test loss: 0.33060508966445923\n",
      "Epoch: 1644 | Loss: 0.1401316225528717 | Test loss: 0.3306421935558319\n",
      "Epoch: 1645 | Loss: 0.14013108611106873 | Test loss: 0.330679327249527\n",
      "Epoch: 1646 | Loss: 0.14013051986694336 | Test loss: 0.33071640133857727\n",
      "Epoch: 1647 | Loss: 0.14012998342514038 | Test loss: 0.33075350522994995\n",
      "Epoch: 1648 | Loss: 0.1401294320821762 | Test loss: 0.330790638923645\n",
      "Epoch: 1649 | Loss: 0.14012888073921204 | Test loss: 0.3308277130126953\n",
      "Epoch: 1650 | Loss: 0.14012832939624786 | Test loss: 0.3308648467063904\n",
      "Epoch: 1651 | Loss: 0.1401277780532837 | Test loss: 0.33090195059776306\n",
      "Epoch: 1652 | Loss: 0.1401272416114807 | Test loss: 0.33093902468681335\n",
      "Epoch: 1653 | Loss: 0.14012669026851654 | Test loss: 0.33097612857818604\n",
      "Epoch: 1654 | Loss: 0.14012612402439117 | Test loss: 0.3310132622718811\n",
      "Epoch: 1655 | Loss: 0.1401255875825882 | Test loss: 0.3310503363609314\n",
      "Epoch: 1656 | Loss: 0.14012503623962402 | Test loss: 0.3310874104499817\n",
      "Epoch: 1657 | Loss: 0.14012448489665985 | Test loss: 0.33112451434135437\n",
      "Epoch: 1658 | Loss: 0.14012393355369568 | Test loss: 0.33116164803504944\n",
      "Epoch: 1659 | Loss: 0.1401233822107315 | Test loss: 0.33119872212409973\n",
      "Epoch: 1660 | Loss: 0.14012284576892853 | Test loss: 0.3312358260154724\n",
      "Epoch: 1661 | Loss: 0.14012227952480316 | Test loss: 0.3312729597091675\n",
      "Epoch: 1662 | Loss: 0.14012174308300018 | Test loss: 0.3313100337982178\n",
      "Epoch: 1663 | Loss: 0.140121191740036 | Test loss: 0.33134716749191284\n",
      "Epoch: 1664 | Loss: 0.14012065529823303 | Test loss: 0.3313842713832855\n",
      "Epoch: 1665 | Loss: 0.14012010395526886 | Test loss: 0.3314213454723358\n",
      "Epoch: 1666 | Loss: 0.1401195526123047 | Test loss: 0.3314584791660309\n",
      "Epoch: 1667 | Loss: 0.14011900126934052 | Test loss: 0.3314955532550812\n",
      "Epoch: 1668 | Loss: 0.14011844992637634 | Test loss: 0.33153265714645386\n",
      "Epoch: 1669 | Loss: 0.14011789858341217 | Test loss: 0.33156973123550415\n",
      "Epoch: 1670 | Loss: 0.1401173621416092 | Test loss: 0.3316068649291992\n",
      "Epoch: 1671 | Loss: 0.14011679589748383 | Test loss: 0.3316439688205719\n",
      "Epoch: 1672 | Loss: 0.14011625945568085 | Test loss: 0.3316810429096222\n",
      "Epoch: 1673 | Loss: 0.14011570811271667 | Test loss: 0.33171817660331726\n",
      "Epoch: 1674 | Loss: 0.1401151567697525 | Test loss: 0.33175528049468994\n",
      "Epoch: 1675 | Loss: 0.14011460542678833 | Test loss: 0.33179235458374023\n",
      "Epoch: 1676 | Loss: 0.14011406898498535 | Test loss: 0.3318294584751129\n",
      "Epoch: 1677 | Loss: 0.14011351764202118 | Test loss: 0.331866592168808\n",
      "Epoch: 1678 | Loss: 0.140112966299057 | Test loss: 0.3319036662578583\n",
      "Epoch: 1679 | Loss: 0.14011241495609283 | Test loss: 0.33194079995155334\n",
      "Epoch: 1680 | Loss: 0.14011186361312866 | Test loss: 0.33197787404060364\n",
      "Epoch: 1681 | Loss: 0.1401113122701645 | Test loss: 0.3320149779319763\n",
      "Epoch: 1682 | Loss: 0.14011076092720032 | Test loss: 0.332052081823349\n",
      "Epoch: 1683 | Loss: 0.14011022448539734 | Test loss: 0.332069993019104\n",
      "Epoch: 1684 | Loss: 0.14010970294475555 | Test loss: 0.332087904214859\n",
      "Epoch: 1685 | Loss: 0.14010916650295258 | Test loss: 0.332105815410614\n",
      "Epoch: 1686 | Loss: 0.1401086449623108 | Test loss: 0.33212369680404663\n",
      "Epoch: 1687 | Loss: 0.1401081085205078 | Test loss: 0.332141637802124\n",
      "Epoch: 1688 | Loss: 0.14010758697986603 | Test loss: 0.3321595788002014\n",
      "Epoch: 1689 | Loss: 0.14010706543922424 | Test loss: 0.33217746019363403\n",
      "Epoch: 1690 | Loss: 0.14010652899742126 | Test loss: 0.33219534158706665\n",
      "Epoch: 1691 | Loss: 0.14010600745677948 | Test loss: 0.33221328258514404\n",
      "Epoch: 1692 | Loss: 0.1401054710149765 | Test loss: 0.33223116397857666\n",
      "Epoch: 1693 | Loss: 0.14010493457317352 | Test loss: 0.33224910497665405\n",
      "Epoch: 1694 | Loss: 0.14010441303253174 | Test loss: 0.33226698637008667\n",
      "Epoch: 1695 | Loss: 0.14010389149188995 | Test loss: 0.33228492736816406\n",
      "Epoch: 1696 | Loss: 0.14010336995124817 | Test loss: 0.33230283856391907\n",
      "Epoch: 1697 | Loss: 0.14010284841060638 | Test loss: 0.3323207497596741\n",
      "Epoch: 1698 | Loss: 0.1401023119688034 | Test loss: 0.3323386609554291\n",
      "Epoch: 1699 | Loss: 0.14010179042816162 | Test loss: 0.3323715031147003\n",
      "Epoch: 1700 | Loss: 0.14010131359100342 | Test loss: 0.3324044346809387\n",
      "Epoch: 1701 | Loss: 0.14010082185268402 | Test loss: 0.33243727684020996\n",
      "Epoch: 1702 | Loss: 0.14010034501552582 | Test loss: 0.3324701189994812\n",
      "Epoch: 1703 | Loss: 0.14009985327720642 | Test loss: 0.3325030505657196\n",
      "Epoch: 1704 | Loss: 0.14009937644004822 | Test loss: 0.33253592252731323\n",
      "Epoch: 1705 | Loss: 0.14009889960289001 | Test loss: 0.33256882429122925\n",
      "Epoch: 1706 | Loss: 0.14009839296340942 | Test loss: 0.3326016664505005\n",
      "Epoch: 1707 | Loss: 0.14009791612625122 | Test loss: 0.33263450860977173\n",
      "Epoch: 1708 | Loss: 0.14009743928909302 | Test loss: 0.33266738057136536\n",
      "Epoch: 1709 | Loss: 0.14009694755077362 | Test loss: 0.332700252532959\n",
      "Epoch: 1710 | Loss: 0.14009647071361542 | Test loss: 0.332733154296875\n",
      "Epoch: 1711 | Loss: 0.14009597897529602 | Test loss: 0.33276602625846863\n",
      "Epoch: 1712 | Loss: 0.14009548723697662 | Test loss: 0.33279889822006226\n",
      "Epoch: 1713 | Loss: 0.14009501039981842 | Test loss: 0.3328317701816559\n",
      "Epoch: 1714 | Loss: 0.14009451866149902 | Test loss: 0.3328646421432495\n",
      "Epoch: 1715 | Loss: 0.14009404182434082 | Test loss: 0.33289751410484314\n",
      "Epoch: 1716 | Loss: 0.14009355008602142 | Test loss: 0.33293041586875916\n",
      "Epoch: 1717 | Loss: 0.14009307324886322 | Test loss: 0.3329632878303528\n",
      "Epoch: 1718 | Loss: 0.14009259641170502 | Test loss: 0.332996129989624\n",
      "Epoch: 1719 | Loss: 0.14009210467338562 | Test loss: 0.33302900195121765\n",
      "Epoch: 1720 | Loss: 0.14009161293506622 | Test loss: 0.33306190371513367\n",
      "Epoch: 1721 | Loss: 0.14009113609790802 | Test loss: 0.3330947756767273\n",
      "Epoch: 1722 | Loss: 0.14009065926074982 | Test loss: 0.33312761783599854\n",
      "Epoch: 1723 | Loss: 0.14009015262126923 | Test loss: 0.33316054940223694\n",
      "Epoch: 1724 | Loss: 0.14008967578411102 | Test loss: 0.3331933915615082\n",
      "Epoch: 1725 | Loss: 0.14008919894695282 | Test loss: 0.3332262635231018\n",
      "Epoch: 1726 | Loss: 0.14008870720863342 | Test loss: 0.33325910568237305\n",
      "Epoch: 1727 | Loss: 0.14008821547031403 | Test loss: 0.33329200744628906\n",
      "Epoch: 1728 | Loss: 0.14008773863315582 | Test loss: 0.3333248794078827\n",
      "Epoch: 1729 | Loss: 0.14008724689483643 | Test loss: 0.3333577513694763\n",
      "Epoch: 1730 | Loss: 0.14008677005767822 | Test loss: 0.33339065313339233\n",
      "Epoch: 1731 | Loss: 0.14008627831935883 | Test loss: 0.33342352509498596\n",
      "Epoch: 1732 | Loss: 0.14008578658103943 | Test loss: 0.3334563672542572\n",
      "Epoch: 1733 | Loss: 0.14008530974388123 | Test loss: 0.3334892690181732\n",
      "Epoch: 1734 | Loss: 0.14008483290672302 | Test loss: 0.33352214097976685\n",
      "Epoch: 1735 | Loss: 0.14008434116840363 | Test loss: 0.3335550129413605\n",
      "Epoch: 1736 | Loss: 0.14008384943008423 | Test loss: 0.3335878551006317\n",
      "Epoch: 1737 | Loss: 0.14008337259292603 | Test loss: 0.33362075686454773\n",
      "Epoch: 1738 | Loss: 0.14008288085460663 | Test loss: 0.33365362882614136\n",
      "Epoch: 1739 | Loss: 0.14008238911628723 | Test loss: 0.3336865305900574\n",
      "Epoch: 1740 | Loss: 0.14008191227912903 | Test loss: 0.333719402551651\n",
      "Epoch: 1741 | Loss: 0.14008143544197083 | Test loss: 0.33375227451324463\n",
      "Epoch: 1742 | Loss: 0.14008094370365143 | Test loss: 0.33378517627716064\n",
      "Epoch: 1743 | Loss: 0.14008046686649323 | Test loss: 0.3338180184364319\n",
      "Epoch: 1744 | Loss: 0.14007997512817383 | Test loss: 0.3338508903980255\n",
      "Epoch: 1745 | Loss: 0.14007949829101562 | Test loss: 0.33388373255729675\n",
      "Epoch: 1746 | Loss: 0.14007900655269623 | Test loss: 0.3339166045188904\n",
      "Epoch: 1747 | Loss: 0.14007852971553802 | Test loss: 0.3339300751686096\n",
      "Epoch: 1748 | Loss: 0.14007805287837982 | Test loss: 0.33394357562065125\n",
      "Epoch: 1749 | Loss: 0.1400775909423828 | Test loss: 0.3339570462703705\n",
      "Epoch: 1750 | Loss: 0.1400771290063858 | Test loss: 0.33397048711776733\n",
      "Epoch: 1751 | Loss: 0.1400766670703888 | Test loss: 0.33400335907936096\n",
      "Epoch: 1752 | Loss: 0.1400761902332306 | Test loss: 0.3340168297290802\n",
      "Epoch: 1753 | Loss: 0.14007572829723358 | Test loss: 0.33403030037879944\n",
      "Epoch: 1754 | Loss: 0.14007528126239777 | Test loss: 0.3340584933757782\n",
      "Epoch: 1755 | Loss: 0.14007486402988434 | Test loss: 0.33408668637275696\n",
      "Epoch: 1756 | Loss: 0.1400744467973709 | Test loss: 0.3341149091720581\n",
      "Epoch: 1757 | Loss: 0.1400740146636963 | Test loss: 0.33414310216903687\n",
      "Epoch: 1758 | Loss: 0.14007359743118286 | Test loss: 0.3341712951660156\n",
      "Epoch: 1759 | Loss: 0.14007318019866943 | Test loss: 0.3341994881629944\n",
      "Epoch: 1760 | Loss: 0.140072762966156 | Test loss: 0.33422771096229553\n",
      "Epoch: 1761 | Loss: 0.14007234573364258 | Test loss: 0.3342559039592743\n",
      "Epoch: 1762 | Loss: 0.14007192850112915 | Test loss: 0.33428412675857544\n",
      "Epoch: 1763 | Loss: 0.14007149636745453 | Test loss: 0.3343123197555542\n",
      "Epoch: 1764 | Loss: 0.1400710791349411 | Test loss: 0.33434051275253296\n",
      "Epoch: 1765 | Loss: 0.14007066190242767 | Test loss: 0.3343687355518341\n",
      "Epoch: 1766 | Loss: 0.14007025957107544 | Test loss: 0.33439692854881287\n",
      "Epoch: 1767 | Loss: 0.14006982743740082 | Test loss: 0.3344251215457916\n",
      "Epoch: 1768 | Loss: 0.1400694102048874 | Test loss: 0.3344533443450928\n",
      "Epoch: 1769 | Loss: 0.14006899297237396 | Test loss: 0.33448153734207153\n",
      "Epoch: 1770 | Loss: 0.14006856083869934 | Test loss: 0.3345097303390503\n",
      "Epoch: 1771 | Loss: 0.1400681436061859 | Test loss: 0.33453795313835144\n",
      "Epoch: 1772 | Loss: 0.14006772637367249 | Test loss: 0.3345661461353302\n",
      "Epoch: 1773 | Loss: 0.14006730914115906 | Test loss: 0.33459433913230896\n",
      "Epoch: 1774 | Loss: 0.14006689190864563 | Test loss: 0.3346225619316101\n",
      "Epoch: 1775 | Loss: 0.1400664746761322 | Test loss: 0.33465075492858887\n",
      "Epoch: 1776 | Loss: 0.14006604254245758 | Test loss: 0.3346789479255676\n",
      "Epoch: 1777 | Loss: 0.14006564021110535 | Test loss: 0.3347071707248688\n",
      "Epoch: 1778 | Loss: 0.14006520807743073 | Test loss: 0.33473533391952515\n",
      "Epoch: 1779 | Loss: 0.1400647908449173 | Test loss: 0.3347635567188263\n",
      "Epoch: 1780 | Loss: 0.14006437361240387 | Test loss: 0.33479174971580505\n",
      "Epoch: 1781 | Loss: 0.14006395637989044 | Test loss: 0.3348200023174286\n",
      "Epoch: 1782 | Loss: 0.14006353914737701 | Test loss: 0.33484816551208496\n",
      "Epoch: 1783 | Loss: 0.1400631070137024 | Test loss: 0.3348763585090637\n",
      "Epoch: 1784 | Loss: 0.14006270468235016 | Test loss: 0.33490458130836487\n",
      "Epoch: 1785 | Loss: 0.14006227254867554 | Test loss: 0.33493277430534363\n",
      "Epoch: 1786 | Loss: 0.1400618553161621 | Test loss: 0.3349609673023224\n",
      "Epoch: 1787 | Loss: 0.14006143808364868 | Test loss: 0.33498919010162354\n",
      "Epoch: 1788 | Loss: 0.14006102085113525 | Test loss: 0.3350173830986023\n",
      "Epoch: 1789 | Loss: 0.14006060361862183 | Test loss: 0.33504557609558105\n",
      "Epoch: 1790 | Loss: 0.1400601714849472 | Test loss: 0.3350737988948822\n",
      "Epoch: 1791 | Loss: 0.14005976915359497 | Test loss: 0.33510199189186096\n",
      "Epoch: 1792 | Loss: 0.14005933701992035 | Test loss: 0.3351302146911621\n",
      "Epoch: 1793 | Loss: 0.14005891978740692 | Test loss: 0.3351583778858185\n",
      "Epoch: 1794 | Loss: 0.1400585025548935 | Test loss: 0.33518660068511963\n",
      "Epoch: 1795 | Loss: 0.14005807042121887 | Test loss: 0.3352147936820984\n",
      "Epoch: 1796 | Loss: 0.14005766808986664 | Test loss: 0.33524298667907715\n",
      "Epoch: 1797 | Loss: 0.1400572508573532 | Test loss: 0.3352712094783783\n",
      "Epoch: 1798 | Loss: 0.14005683362483978 | Test loss: 0.33529940247535706\n",
      "Epoch: 1799 | Loss: 0.14005641639232635 | Test loss: 0.3353275954723358\n",
      "Epoch: 1800 | Loss: 0.14005599915981293 | Test loss: 0.335370272397995\n",
      "Epoch: 1801 | Loss: 0.1400555968284607 | Test loss: 0.3354129195213318\n",
      "Epoch: 1802 | Loss: 0.14005520939826965 | Test loss: 0.33543601632118225\n",
      "Epoch: 1803 | Loss: 0.140054851770401 | Test loss: 0.3354591429233551\n",
      "Epoch: 1804 | Loss: 0.14005450904369354 | Test loss: 0.3354821801185608\n",
      "Epoch: 1805 | Loss: 0.1400541514158249 | Test loss: 0.33550527691841125\n",
      "Epoch: 1806 | Loss: 0.14005379378795624 | Test loss: 0.3355283737182617\n",
      "Epoch: 1807 | Loss: 0.14005343616008759 | Test loss: 0.3355514407157898\n",
      "Epoch: 1808 | Loss: 0.14005309343338013 | Test loss: 0.33557453751564026\n",
      "Epoch: 1809 | Loss: 0.14005273580551147 | Test loss: 0.3355976343154907\n",
      "Epoch: 1810 | Loss: 0.14005237817764282 | Test loss: 0.3356207311153412\n",
      "Epoch: 1811 | Loss: 0.14005203545093536 | Test loss: 0.33564379811286926\n",
      "Epoch: 1812 | Loss: 0.14005166292190552 | Test loss: 0.33566686511039734\n",
      "Epoch: 1813 | Loss: 0.14005132019519806 | Test loss: 0.3356899917125702\n",
      "Epoch: 1814 | Loss: 0.1400509625673294 | Test loss: 0.33571308851242065\n",
      "Epoch: 1815 | Loss: 0.14005061984062195 | Test loss: 0.33573615550994873\n",
      "Epoch: 1816 | Loss: 0.1400502622127533 | Test loss: 0.3357592225074768\n",
      "Epoch: 1817 | Loss: 0.14004990458488464 | Test loss: 0.33578231930732727\n",
      "Epoch: 1818 | Loss: 0.140049546957016 | Test loss: 0.33580538630485535\n",
      "Epoch: 1819 | Loss: 0.14004918932914734 | Test loss: 0.3358284831047058\n",
      "Epoch: 1820 | Loss: 0.14004884660243988 | Test loss: 0.3358515799045563\n",
      "Epoch: 1821 | Loss: 0.14004848897457123 | Test loss: 0.33587467670440674\n",
      "Epoch: 1822 | Loss: 0.14004813134670258 | Test loss: 0.3358977437019348\n",
      "Epoch: 1823 | Loss: 0.14004777371883392 | Test loss: 0.3359208106994629\n",
      "Epoch: 1824 | Loss: 0.14004741609096527 | Test loss: 0.33594390749931335\n",
      "Epoch: 1825 | Loss: 0.1400470733642578 | Test loss: 0.33596697449684143\n",
      "Epoch: 1826 | Loss: 0.14004670083522797 | Test loss: 0.3359901010990143\n",
      "Epoch: 1827 | Loss: 0.1400463581085205 | Test loss: 0.33601316809654236\n",
      "Epoch: 1828 | Loss: 0.14004601538181305 | Test loss: 0.3360362648963928\n",
      "Epoch: 1829 | Loss: 0.1400456428527832 | Test loss: 0.3360593318939209\n",
      "Epoch: 1830 | Loss: 0.14004530012607574 | Test loss: 0.33608245849609375\n",
      "Epoch: 1831 | Loss: 0.1400449573993683 | Test loss: 0.3361055254936218\n",
      "Epoch: 1832 | Loss: 0.14004458487033844 | Test loss: 0.3361286222934723\n",
      "Epoch: 1833 | Loss: 0.14004424214363098 | Test loss: 0.33615168929100037\n",
      "Epoch: 1834 | Loss: 0.14004388451576233 | Test loss: 0.33617475628852844\n",
      "Epoch: 1835 | Loss: 0.14004352688789368 | Test loss: 0.3361978828907013\n",
      "Epoch: 1836 | Loss: 0.14004318416118622 | Test loss: 0.336220920085907\n",
      "Epoch: 1837 | Loss: 0.14004281163215637 | Test loss: 0.33624404668807983\n",
      "Epoch: 1838 | Loss: 0.1400424689054489 | Test loss: 0.3362670838832855\n",
      "Epoch: 1839 | Loss: 0.14004212617874146 | Test loss: 0.3363044261932373\n",
      "Epoch: 1840 | Loss: 0.1400417983531952 | Test loss: 0.3363417387008667\n",
      "Epoch: 1841 | Loss: 0.14004145562648773 | Test loss: 0.3363790810108185\n",
      "Epoch: 1842 | Loss: 0.14004114270210266 | Test loss: 0.3364163935184479\n",
      "Epoch: 1843 | Loss: 0.1400408148765564 | Test loss: 0.33645370602607727\n",
      "Epoch: 1844 | Loss: 0.14004048705101013 | Test loss: 0.33649104833602905\n",
      "Epoch: 1845 | Loss: 0.14004014432430267 | Test loss: 0.33652839064598083\n",
      "Epoch: 1846 | Loss: 0.1400398164987564 | Test loss: 0.33656570315361023\n",
      "Epoch: 1847 | Loss: 0.14003950357437134 | Test loss: 0.3366030156612396\n",
      "Epoch: 1848 | Loss: 0.14003917574882507 | Test loss: 0.3366205394268036\n",
      "Epoch: 1849 | Loss: 0.1400388926267624 | Test loss: 0.33663809299468994\n",
      "Epoch: 1850 | Loss: 0.1400385946035385 | Test loss: 0.3366556167602539\n",
      "Epoch: 1851 | Loss: 0.14003831148147583 | Test loss: 0.33667317032814026\n",
      "Epoch: 1852 | Loss: 0.14003802835941315 | Test loss: 0.33669066429138184\n",
      "Epoch: 1853 | Loss: 0.14003773033618927 | Test loss: 0.3367081880569458\n",
      "Epoch: 1854 | Loss: 0.1400374323129654 | Test loss: 0.3367256820201874\n",
      "Epoch: 1855 | Loss: 0.1400371491909027 | Test loss: 0.33674323558807373\n",
      "Epoch: 1856 | Loss: 0.14003686606884003 | Test loss: 0.3367607295513153\n",
      "Epoch: 1857 | Loss: 0.14003656804561615 | Test loss: 0.3367782533168793\n",
      "Epoch: 1858 | Loss: 0.14003627002239227 | Test loss: 0.3367958068847656\n",
      "Epoch: 1859 | Loss: 0.1400359869003296 | Test loss: 0.3368133008480072\n",
      "Epoch: 1860 | Loss: 0.1400356888771057 | Test loss: 0.33683082461357117\n",
      "Epoch: 1861 | Loss: 0.14003540575504303 | Test loss: 0.3368483781814575\n",
      "Epoch: 1862 | Loss: 0.14003510773181915 | Test loss: 0.3368658721446991\n",
      "Epoch: 1863 | Loss: 0.14003482460975647 | Test loss: 0.33688342571258545\n",
      "Epoch: 1864 | Loss: 0.1400345265865326 | Test loss: 0.336900919675827\n",
      "Epoch: 1865 | Loss: 0.1400342583656311 | Test loss: 0.336918443441391\n",
      "Epoch: 1866 | Loss: 0.14003394544124603 | Test loss: 0.33693599700927734\n",
      "Epoch: 1867 | Loss: 0.14003366231918335 | Test loss: 0.3369534909725189\n",
      "Epoch: 1868 | Loss: 0.14003337919712067 | Test loss: 0.3369710445404053\n",
      "Epoch: 1869 | Loss: 0.14003309607505798 | Test loss: 0.33698856830596924\n",
      "Epoch: 1870 | Loss: 0.1400327980518341 | Test loss: 0.3370060622692108\n",
      "Epoch: 1871 | Loss: 0.14003251492977142 | Test loss: 0.33702361583709717\n",
      "Epoch: 1872 | Loss: 0.14003221690654755 | Test loss: 0.33704107999801636\n",
      "Epoch: 1873 | Loss: 0.14003191888332367 | Test loss: 0.3370586335659027\n",
      "Epoch: 1874 | Loss: 0.140031635761261 | Test loss: 0.33707618713378906\n",
      "Epoch: 1875 | Loss: 0.1400313377380371 | Test loss: 0.33709368109703064\n",
      "Epoch: 1876 | Loss: 0.14003105461597443 | Test loss: 0.3371112048625946\n",
      "Epoch: 1877 | Loss: 0.14003078639507294 | Test loss: 0.3371427357196808\n",
      "Epoch: 1878 | Loss: 0.14003051817417145 | Test loss: 0.33717429637908936\n",
      "Epoch: 1879 | Loss: 0.14003026485443115 | Test loss: 0.33720582723617554\n",
      "Epoch: 1880 | Loss: 0.14002999663352966 | Test loss: 0.3372373878955841\n",
      "Epoch: 1881 | Loss: 0.14002972841262817 | Test loss: 0.3372688889503479\n",
      "Epoch: 1882 | Loss: 0.14002947509288788 | Test loss: 0.33730047941207886\n",
      "Epoch: 1883 | Loss: 0.14002922177314758 | Test loss: 0.33733198046684265\n",
      "Epoch: 1884 | Loss: 0.1400289535522461 | Test loss: 0.3373635411262512\n",
      "Epoch: 1885 | Loss: 0.1400287002325058 | Test loss: 0.3373950123786926\n",
      "Epoch: 1886 | Loss: 0.1400284320116043 | Test loss: 0.3374266028404236\n",
      "Epoch: 1887 | Loss: 0.14002816379070282 | Test loss: 0.33745813369750977\n",
      "Epoch: 1888 | Loss: 0.14002791047096252 | Test loss: 0.33748969435691833\n",
      "Epoch: 1889 | Loss: 0.14002765715122223 | Test loss: 0.33752119541168213\n",
      "Epoch: 1890 | Loss: 0.14002738893032074 | Test loss: 0.3375527262687683\n",
      "Epoch: 1891 | Loss: 0.14002713561058044 | Test loss: 0.3375842869281769\n",
      "Epoch: 1892 | Loss: 0.14002686738967896 | Test loss: 0.33761581778526306\n",
      "Epoch: 1893 | Loss: 0.14002662897109985 | Test loss: 0.3376273214817047\n",
      "Epoch: 1894 | Loss: 0.14002639055252075 | Test loss: 0.33763885498046875\n",
      "Epoch: 1895 | Loss: 0.14002616703510284 | Test loss: 0.3376503586769104\n",
      "Epoch: 1896 | Loss: 0.14002594351768494 | Test loss: 0.33766189217567444\n",
      "Epoch: 1897 | Loss: 0.14002570509910583 | Test loss: 0.3376733958721161\n",
      "Epoch: 1898 | Loss: 0.14002548158168793 | Test loss: 0.3376849293708801\n",
      "Epoch: 1899 | Loss: 0.14002524316310883 | Test loss: 0.33769646286964417\n",
      "Epoch: 1900 | Loss: 0.14002501964569092 | Test loss: 0.3377079665660858\n",
      "Epoch: 1901 | Loss: 0.14002478122711182 | Test loss: 0.3377194404602051\n",
      "Epoch: 1902 | Loss: 0.14002454280853271 | Test loss: 0.3377309739589691\n",
      "Epoch: 1903 | Loss: 0.1400243192911148 | Test loss: 0.33774253726005554\n",
      "Epoch: 1904 | Loss: 0.1400240957736969 | Test loss: 0.3377540111541748\n",
      "Epoch: 1905 | Loss: 0.1400238573551178 | Test loss: 0.33776554465293884\n",
      "Epoch: 1906 | Loss: 0.1400236338376999 | Test loss: 0.3377770483493805\n",
      "Epoch: 1907 | Loss: 0.1400233954191208 | Test loss: 0.33778852224349976\n",
      "Epoch: 1908 | Loss: 0.14002317190170288 | Test loss: 0.3378000557422638\n",
      "Epoch: 1909 | Loss: 0.14002294838428497 | Test loss: 0.3378116190433502\n",
      "Epoch: 1910 | Loss: 0.14002270996570587 | Test loss: 0.33782312273979187\n",
      "Epoch: 1911 | Loss: 0.14002247154712677 | Test loss: 0.3378346562385559\n",
      "Epoch: 1912 | Loss: 0.14002224802970886 | Test loss: 0.33784613013267517\n",
      "Epoch: 1913 | Loss: 0.14002200961112976 | Test loss: 0.3378576636314392\n",
      "Epoch: 1914 | Loss: 0.14002180099487305 | Test loss: 0.33788299560546875\n",
      "Epoch: 1915 | Loss: 0.14002159237861633 | Test loss: 0.33790823817253113\n",
      "Epoch: 1916 | Loss: 0.140021413564682 | Test loss: 0.33793359994888306\n",
      "Epoch: 1917 | Loss: 0.1400212049484253 | Test loss: 0.3379589021205902\n",
      "Epoch: 1918 | Loss: 0.14002101123332977 | Test loss: 0.33798420429229736\n",
      "Epoch: 1919 | Loss: 0.14002081751823425 | Test loss: 0.33800947666168213\n",
      "Epoch: 1920 | Loss: 0.14002062380313873 | Test loss: 0.33803480863571167\n",
      "Epoch: 1921 | Loss: 0.1400204300880432 | Test loss: 0.3380601406097412\n",
      "Epoch: 1922 | Loss: 0.1400202363729477 | Test loss: 0.3380853831768036\n",
      "Epoch: 1923 | Loss: 0.14002004265785217 | Test loss: 0.3381107747554779\n",
      "Epoch: 1924 | Loss: 0.14001983404159546 | Test loss: 0.33813604712486267\n",
      "Epoch: 1925 | Loss: 0.14001964032649994 | Test loss: 0.3381613790988922\n",
      "Epoch: 1926 | Loss: 0.14001944661140442 | Test loss: 0.33818668127059937\n",
      "Epoch: 1927 | Loss: 0.1400192528963089 | Test loss: 0.33821195363998413\n",
      "Epoch: 1928 | Loss: 0.14001904428005219 | Test loss: 0.33823728561401367\n",
      "Epoch: 1929 | Loss: 0.14001886546611786 | Test loss: 0.33826255798339844\n",
      "Epoch: 1930 | Loss: 0.14001865684986115 | Test loss: 0.338287889957428\n",
      "Epoch: 1931 | Loss: 0.14001846313476562 | Test loss: 0.3383132219314575\n",
      "Epoch: 1932 | Loss: 0.1400182694196701 | Test loss: 0.33833855390548706\n",
      "Epoch: 1933 | Loss: 0.14001807570457458 | Test loss: 0.3383638262748718\n",
      "Epoch: 1934 | Loss: 0.14001786708831787 | Test loss: 0.338389128446579\n",
      "Epoch: 1935 | Loss: 0.14001767337322235 | Test loss: 0.3384144604206085\n",
      "Epoch: 1936 | Loss: 0.14001747965812683 | Test loss: 0.33843979239463806\n",
      "Epoch: 1937 | Loss: 0.14001727104187012 | Test loss: 0.3384650647640228\n",
      "Epoch: 1938 | Loss: 0.1400170922279358 | Test loss: 0.33849036693573\n",
      "Epoch: 1939 | Loss: 0.14001689851284027 | Test loss: 0.3384954333305359\n",
      "Epoch: 1940 | Loss: 0.14001671969890594 | Test loss: 0.3385004997253418\n",
      "Epoch: 1941 | Loss: 0.1400165557861328 | Test loss: 0.3385055363178253\n",
      "Epoch: 1942 | Loss: 0.1400163769721985 | Test loss: 0.33851057291030884\n",
      "Epoch: 1943 | Loss: 0.14001619815826416 | Test loss: 0.3385156989097595\n",
      "Epoch: 1944 | Loss: 0.14001601934432983 | Test loss: 0.33852073550224304\n",
      "Epoch: 1945 | Loss: 0.1400158405303955 | Test loss: 0.33852580189704895\n",
      "Epoch: 1946 | Loss: 0.14001566171646118 | Test loss: 0.3385511040687561\n",
      "Epoch: 1947 | Loss: 0.14001549780368805 | Test loss: 0.338556170463562\n",
      "Epoch: 1948 | Loss: 0.14001531898975372 | Test loss: 0.3385612368583679\n",
      "Epoch: 1949 | Loss: 0.1400151401758194 | Test loss: 0.33856630325317383\n",
      "Epoch: 1950 | Loss: 0.14001497626304626 | Test loss: 0.33857133984565735\n",
      "Epoch: 1951 | Loss: 0.14001478254795074 | Test loss: 0.33857643604278564\n",
      "Epoch: 1952 | Loss: 0.1400146186351776 | Test loss: 0.33858150243759155\n",
      "Epoch: 1953 | Loss: 0.1400144398212433 | Test loss: 0.3386000990867615\n",
      "Epoch: 1954 | Loss: 0.14001432061195374 | Test loss: 0.33861875534057617\n",
      "Epoch: 1955 | Loss: 0.1400141716003418 | Test loss: 0.3386373817920685\n",
      "Epoch: 1956 | Loss: 0.14001403748989105 | Test loss: 0.3386560082435608\n",
      "Epoch: 1957 | Loss: 0.1400139033794403 | Test loss: 0.3386746048927307\n",
      "Epoch: 1958 | Loss: 0.14001376926898956 | Test loss: 0.3386933207511902\n",
      "Epoch: 1959 | Loss: 0.14001362025737762 | Test loss: 0.3387119174003601\n",
      "Epoch: 1960 | Loss: 0.14001348614692688 | Test loss: 0.3387305438518524\n",
      "Epoch: 1961 | Loss: 0.14001335203647614 | Test loss: 0.33874914050102234\n",
      "Epoch: 1962 | Loss: 0.1400132030248642 | Test loss: 0.33876776695251465\n",
      "Epoch: 1963 | Loss: 0.14001306891441345 | Test loss: 0.33878645300865173\n",
      "Epoch: 1964 | Loss: 0.1400129348039627 | Test loss: 0.33880507946014404\n",
      "Epoch: 1965 | Loss: 0.14001280069351196 | Test loss: 0.33882370591163635\n",
      "Epoch: 1966 | Loss: 0.14001266658306122 | Test loss: 0.3388423025608063\n",
      "Epoch: 1967 | Loss: 0.14001251757144928 | Test loss: 0.33886095881462097\n",
      "Epoch: 1968 | Loss: 0.14001236855983734 | Test loss: 0.33887961506843567\n",
      "Epoch: 1969 | Loss: 0.1400122493505478 | Test loss: 0.338898241519928\n",
      "Epoch: 1970 | Loss: 0.14001210033893585 | Test loss: 0.3389168381690979\n",
      "Epoch: 1971 | Loss: 0.1400119513273239 | Test loss: 0.3389354944229126\n",
      "Epoch: 1972 | Loss: 0.14001181721687317 | Test loss: 0.3389541506767273\n",
      "Epoch: 1973 | Loss: 0.14001168310642242 | Test loss: 0.3389727771282196\n",
      "Epoch: 1974 | Loss: 0.14001154899597168 | Test loss: 0.3389913737773895\n",
      "Epoch: 1975 | Loss: 0.14001139998435974 | Test loss: 0.3390100300312042\n",
      "Epoch: 1976 | Loss: 0.140011265873909 | Test loss: 0.33902865648269653\n",
      "Epoch: 1977 | Loss: 0.14001113176345825 | Test loss: 0.33904731273651123\n",
      "Epoch: 1978 | Loss: 0.1400109976530075 | Test loss: 0.33906593918800354\n",
      "Epoch: 1979 | Loss: 0.14001086354255676 | Test loss: 0.33908453583717346\n",
      "Epoch: 1980 | Loss: 0.14001071453094482 | Test loss: 0.33910319209098816\n",
      "Epoch: 1981 | Loss: 0.14001058042049408 | Test loss: 0.33912181854248047\n",
      "Epoch: 1982 | Loss: 0.14001044631004333 | Test loss: 0.3391404151916504\n",
      "Epoch: 1983 | Loss: 0.1400103121995926 | Test loss: 0.3391590714454651\n",
      "Epoch: 1984 | Loss: 0.14001016318798065 | Test loss: 0.3391776978969574\n",
      "Epoch: 1985 | Loss: 0.1400100290775299 | Test loss: 0.3391963243484497\n",
      "Epoch: 1986 | Loss: 0.14000988006591797 | Test loss: 0.3392149806022644\n",
      "Epoch: 1987 | Loss: 0.14000974595546722 | Test loss: 0.3392336070537567\n",
      "Epoch: 1988 | Loss: 0.14000961184501648 | Test loss: 0.339252233505249\n",
      "Epoch: 1989 | Loss: 0.14000947773456573 | Test loss: 0.33927085995674133\n",
      "Epoch: 1990 | Loss: 0.14000935852527618 | Test loss: 0.3392690122127533\n",
      "Epoch: 1991 | Loss: 0.14000922441482544 | Test loss: 0.33926719427108765\n",
      "Epoch: 1992 | Loss: 0.1400090903043747 | Test loss: 0.33928585052490234\n",
      "Epoch: 1993 | Loss: 0.14000897109508514 | Test loss: 0.3392840027809143\n",
      "Epoch: 1994 | Loss: 0.1400088518857956 | Test loss: 0.33928215503692627\n",
      "Epoch: 1995 | Loss: 0.14000871777534485 | Test loss: 0.33931422233581543\n",
      "Epoch: 1996 | Loss: 0.1400086134672165 | Test loss: 0.3393256962299347\n",
      "Epoch: 1997 | Loss: 0.14000852406024933 | Test loss: 0.33933722972869873\n",
      "Epoch: 1998 | Loss: 0.14000844955444336 | Test loss: 0.3393487334251404\n",
      "Epoch: 1999 | Loss: 0.140008345246315 | Test loss: 0.3393602669239044\n",
      "Epoch: 2000 | Loss: 0.14000827074050903 | Test loss: 0.33937180042266846\n",
      "Epoch: 2001 | Loss: 0.14000818133354187 | Test loss: 0.3393833041191101\n",
      "Epoch: 2002 | Loss: 0.1400080770254135 | Test loss: 0.33939483761787415\n",
      "Epoch: 2003 | Loss: 0.14000798761844635 | Test loss: 0.3394063115119934\n",
      "Epoch: 2004 | Loss: 0.14000791311264038 | Test loss: 0.33941784501075745\n",
      "Epoch: 2005 | Loss: 0.14000782370567322 | Test loss: 0.3394293487071991\n",
      "Epoch: 2006 | Loss: 0.14000773429870605 | Test loss: 0.33944088220596313\n",
      "Epoch: 2007 | Loss: 0.1400076448917389 | Test loss: 0.3394523859024048\n",
      "Epoch: 2008 | Loss: 0.14000755548477173 | Test loss: 0.33946388959884644\n",
      "Epoch: 2009 | Loss: 0.14000748097896576 | Test loss: 0.3394753932952881\n",
      "Epoch: 2010 | Loss: 0.1400073766708374 | Test loss: 0.3394869267940521\n",
      "Epoch: 2011 | Loss: 0.14000728726387024 | Test loss: 0.3394984304904938\n",
      "Epoch: 2012 | Loss: 0.14000719785690308 | Test loss: 0.3395099639892578\n",
      "Epoch: 2013 | Loss: 0.1400071084499359 | Test loss: 0.33952146768569946\n",
      "Epoch: 2014 | Loss: 0.14000701904296875 | Test loss: 0.3395330011844635\n",
      "Epoch: 2015 | Loss: 0.14000694453716278 | Test loss: 0.33954450488090515\n",
      "Epoch: 2016 | Loss: 0.14000684022903442 | Test loss: 0.3395560085773468\n",
      "Epoch: 2017 | Loss: 0.14000676572322845 | Test loss: 0.33956748247146606\n",
      "Epoch: 2018 | Loss: 0.1400066763162613 | Test loss: 0.3395790457725525\n",
      "Epoch: 2019 | Loss: 0.14000658690929413 | Test loss: 0.3395906090736389\n",
      "Epoch: 2020 | Loss: 0.14000649750232697 | Test loss: 0.3396020531654358\n",
      "Epoch: 2021 | Loss: 0.1400064080953598 | Test loss: 0.33961355686187744\n",
      "Epoch: 2022 | Loss: 0.14000631868839264 | Test loss: 0.3396250903606415\n",
      "Epoch: 2023 | Loss: 0.14000622928142548 | Test loss: 0.33963659405708313\n",
      "Epoch: 2024 | Loss: 0.1400061398744583 | Test loss: 0.33964812755584717\n",
      "Epoch: 2025 | Loss: 0.14000605046749115 | Test loss: 0.3396596312522888\n",
      "Epoch: 2026 | Loss: 0.140005961060524 | Test loss: 0.33967116475105286\n",
      "Epoch: 2027 | Loss: 0.14000587165355682 | Test loss: 0.3396826386451721\n",
      "Epoch: 2028 | Loss: 0.14000579714775085 | Test loss: 0.33969417214393616\n",
      "Epoch: 2029 | Loss: 0.1400056928396225 | Test loss: 0.3397056460380554\n",
      "Epoch: 2030 | Loss: 0.14000560343265533 | Test loss: 0.33971720933914185\n",
      "Epoch: 2031 | Loss: 0.14000552892684937 | Test loss: 0.3397286832332611\n",
      "Epoch: 2032 | Loss: 0.1400054395198822 | Test loss: 0.33974021673202515\n",
      "Epoch: 2033 | Loss: 0.14000535011291504 | Test loss: 0.3397517204284668\n",
      "Epoch: 2034 | Loss: 0.14000524580478668 | Test loss: 0.33976325392723083\n",
      "Epoch: 2035 | Loss: 0.1400051712989807 | Test loss: 0.3397747874259949\n",
      "Epoch: 2036 | Loss: 0.14000509679317474 | Test loss: 0.3397994637489319\n",
      "Epoch: 2037 | Loss: 0.14000500738620758 | Test loss: 0.33981096744537354\n",
      "Epoch: 2038 | Loss: 0.1400049328804016 | Test loss: 0.33983564376831055\n",
      "Epoch: 2039 | Loss: 0.14000485837459564 | Test loss: 0.33986034989356995\n",
      "Epoch: 2040 | Loss: 0.14000476896762848 | Test loss: 0.33988508582115173\n",
      "Epoch: 2041 | Loss: 0.1400046944618225 | Test loss: 0.33990973234176636\n",
      "Epoch: 2042 | Loss: 0.14000461995601654 | Test loss: 0.33993446826934814\n",
      "Epoch: 2043 | Loss: 0.14000456035137177 | Test loss: 0.3399384319782257\n",
      "Epoch: 2044 | Loss: 0.14000451564788818 | Test loss: 0.3399423658847809\n",
      "Epoch: 2045 | Loss: 0.1400044560432434 | Test loss: 0.33994632959365845\n",
      "Epoch: 2046 | Loss: 0.14000441133975983 | Test loss: 0.3399502635002136\n",
      "Epoch: 2047 | Loss: 0.14000436663627625 | Test loss: 0.3399542272090912\n",
      "Epoch: 2048 | Loss: 0.14000432193279266 | Test loss: 0.33995819091796875\n",
      "Epoch: 2049 | Loss: 0.1400042623281479 | Test loss: 0.3399621248245239\n",
      "Epoch: 2050 | Loss: 0.1400042176246643 | Test loss: 0.3399660289287567\n",
      "Epoch: 2051 | Loss: 0.14000415802001953 | Test loss: 0.33997002243995667\n",
      "Epoch: 2052 | Loss: 0.14000411331653595 | Test loss: 0.33997398614883423\n",
      "Epoch: 2053 | Loss: 0.14000406861305237 | Test loss: 0.33997786045074463\n",
      "Epoch: 2054 | Loss: 0.1400040239095688 | Test loss: 0.3399818241596222\n",
      "Epoch: 2055 | Loss: 0.140003964304924 | Test loss: 0.33998578786849976\n",
      "Epoch: 2056 | Loss: 0.14000391960144043 | Test loss: 0.33998972177505493\n",
      "Epoch: 2057 | Loss: 0.14000387489795685 | Test loss: 0.3399936854839325\n",
      "Epoch: 2058 | Loss: 0.14000381529331207 | Test loss: 0.33999761939048767\n",
      "Epoch: 2059 | Loss: 0.1400037705898285 | Test loss: 0.34000158309936523\n",
      "Epoch: 2060 | Loss: 0.14000371098518372 | Test loss: 0.3400055170059204\n",
      "Epoch: 2061 | Loss: 0.14000366628170013 | Test loss: 0.340009480714798\n",
      "Epoch: 2062 | Loss: 0.14000362157821655 | Test loss: 0.34001341462135315\n",
      "Epoch: 2063 | Loss: 0.14000356197357178 | Test loss: 0.3400173485279083\n",
      "Epoch: 2064 | Loss: 0.1400035321712494 | Test loss: 0.3400213122367859\n",
      "Epoch: 2065 | Loss: 0.14000347256660461 | Test loss: 0.34002524614334106\n",
      "Epoch: 2066 | Loss: 0.14000342786312103 | Test loss: 0.34002920985221863\n",
      "Epoch: 2067 | Loss: 0.14000336825847626 | Test loss: 0.3400331735610962\n",
      "Epoch: 2068 | Loss: 0.14000332355499268 | Test loss: 0.34003710746765137\n",
      "Epoch: 2069 | Loss: 0.1400032788515091 | Test loss: 0.34004107117652893\n",
      "Epoch: 2070 | Loss: 0.14000321924686432 | Test loss: 0.3400450050830841\n",
      "Epoch: 2071 | Loss: 0.14000317454338074 | Test loss: 0.34004896879196167\n",
      "Epoch: 2072 | Loss: 0.14000312983989716 | Test loss: 0.34005293250083923\n",
      "Epoch: 2073 | Loss: 0.14000307023525238 | Test loss: 0.340056836605072\n",
      "Epoch: 2074 | Loss: 0.1400030255317688 | Test loss: 0.3400608003139496\n",
      "Epoch: 2075 | Loss: 0.14000296592712402 | Test loss: 0.34006473422050476\n",
      "Epoch: 2076 | Loss: 0.14000292122364044 | Test loss: 0.3400686979293823\n",
      "Epoch: 2077 | Loss: 0.14000286161899567 | Test loss: 0.3400726318359375\n",
      "Epoch: 2078 | Loss: 0.14000281691551208 | Test loss: 0.3400895595550537\n",
      "Epoch: 2079 | Loss: 0.1400027871131897 | Test loss: 0.34010645747184753\n",
      "Epoch: 2080 | Loss: 0.1400027573108673 | Test loss: 0.34012335538864136\n",
      "Epoch: 2081 | Loss: 0.14000271260738373 | Test loss: 0.34014028310775757\n",
      "Epoch: 2082 | Loss: 0.14000268280506134 | Test loss: 0.3401572108268738\n",
      "Epoch: 2083 | Loss: 0.14000265300273895 | Test loss: 0.34016111493110657\n",
      "Epoch: 2084 | Loss: 0.14000260829925537 | Test loss: 0.34017807245254517\n",
      "Epoch: 2085 | Loss: 0.14000257849693298 | Test loss: 0.3401949107646942\n",
      "Epoch: 2086 | Loss: 0.1400025337934494 | Test loss: 0.3402118682861328\n",
      "Epoch: 2087 | Loss: 0.14000248908996582 | Test loss: 0.340228796005249\n",
      "Epoch: 2088 | Loss: 0.14000247418880463 | Test loss: 0.34024566411972046\n",
      "Epoch: 2089 | Loss: 0.14000244438648224 | Test loss: 0.34026259183883667\n",
      "Epoch: 2090 | Loss: 0.14000238478183746 | Test loss: 0.3402794897556305\n",
      "Epoch: 2091 | Loss: 0.14000235497951508 | Test loss: 0.3402964472770691\n",
      "Epoch: 2092 | Loss: 0.1400023251771927 | Test loss: 0.3403133451938629\n",
      "Epoch: 2093 | Loss: 0.1400022804737091 | Test loss: 0.34033024311065674\n",
      "Epoch: 2094 | Loss: 0.14000225067138672 | Test loss: 0.34034714102745056\n",
      "Epoch: 2095 | Loss: 0.14000220596790314 | Test loss: 0.3403640687465668\n",
      "Epoch: 2096 | Loss: 0.14000217616558075 | Test loss: 0.340380996465683\n",
      "Epoch: 2097 | Loss: 0.14000214636325836 | Test loss: 0.3403978645801544\n",
      "Epoch: 2098 | Loss: 0.14000210165977478 | Test loss: 0.34041476249694824\n",
      "Epoch: 2099 | Loss: 0.1400020718574524 | Test loss: 0.34043169021606445\n",
      "Epoch: 2100 | Loss: 0.14000204205513 | Test loss: 0.3404356837272644\n",
      "Epoch: 2101 | Loss: 0.14000199735164642 | Test loss: 0.3404525816440582\n",
      "Epoch: 2102 | Loss: 0.14000196754932404 | Test loss: 0.34046950936317444\n",
      "Epoch: 2103 | Loss: 0.14000192284584045 | Test loss: 0.3404863774776459\n",
      "Epoch: 2104 | Loss: 0.14000190794467926 | Test loss: 0.34048232436180115\n",
      "Epoch: 2105 | Loss: 0.14000187814235687 | Test loss: 0.3404782712459564\n",
      "Epoch: 2106 | Loss: 0.14000186324119568 | Test loss: 0.3404741883277893\n",
      "Epoch: 2107 | Loss: 0.1400018334388733 | Test loss: 0.34049108624458313\n",
      "Epoch: 2108 | Loss: 0.1400018036365509 | Test loss: 0.3404870629310608\n",
      "Epoch: 2109 | Loss: 0.1400017887353897 | Test loss: 0.34048300981521606\n",
      "Epoch: 2110 | Loss: 0.14000175893306732 | Test loss: 0.3404999077320099\n",
      "Epoch: 2111 | Loss: 0.14000175893306732 | Test loss: 0.3404958248138428\n",
      "Epoch: 2112 | Loss: 0.14000171422958374 | Test loss: 0.34049177169799805\n",
      "Epoch: 2113 | Loss: 0.14000169932842255 | Test loss: 0.3404877483844757\n",
      "Epoch: 2114 | Loss: 0.14000166952610016 | Test loss: 0.34050464630126953\n",
      "Epoch: 2115 | Loss: 0.14000165462493896 | Test loss: 0.3405005633831024\n",
      "Epoch: 2116 | Loss: 0.14000162482261658 | Test loss: 0.3404964804649353\n",
      "Epoch: 2117 | Loss: 0.1400015950202942 | Test loss: 0.3404924273490906\n",
      "Epoch: 2118 | Loss: 0.140001580119133 | Test loss: 0.3405093252658844\n",
      "Epoch: 2119 | Loss: 0.1400015652179718 | Test loss: 0.34050530195236206\n",
      "Epoch: 2120 | Loss: 0.14000153541564941 | Test loss: 0.34050121903419495\n",
      "Epoch: 2121 | Loss: 0.14000152051448822 | Test loss: 0.34051814675331116\n",
      "Epoch: 2122 | Loss: 0.14000149071216583 | Test loss: 0.34051406383514404\n",
      "Epoch: 2123 | Loss: 0.14000146090984344 | Test loss: 0.3405100107192993\n",
      "Epoch: 2124 | Loss: 0.14000143110752106 | Test loss: 0.3405059278011322\n",
      "Epoch: 2125 | Loss: 0.14000143110752106 | Test loss: 0.3405228853225708\n",
      "Epoch: 2126 | Loss: 0.14000140130519867 | Test loss: 0.3405188024044037\n",
      "Epoch: 2127 | Loss: 0.14000137150287628 | Test loss: 0.34051474928855896\n",
      "Epoch: 2128 | Loss: 0.1400013566017151 | Test loss: 0.34051066637039185\n",
      "Epoch: 2129 | Loss: 0.1400013267993927 | Test loss: 0.34052756428718567\n",
      "Epoch: 2130 | Loss: 0.1400012969970703 | Test loss: 0.34052351117134094\n",
      "Epoch: 2131 | Loss: 0.14000128209590912 | Test loss: 0.3405194878578186\n",
      "Epoch: 2132 | Loss: 0.14000126719474792 | Test loss: 0.3405154049396515\n",
      "Epoch: 2133 | Loss: 0.14000123739242554 | Test loss: 0.3405323028564453\n",
      "Epoch: 2134 | Loss: 0.14000122249126434 | Test loss: 0.3405282497406006\n",
      "Epoch: 2135 | Loss: 0.14000119268894196 | Test loss: 0.34052419662475586\n",
      "Epoch: 2136 | Loss: 0.14000116288661957 | Test loss: 0.3405410647392273\n",
      "Epoch: 2137 | Loss: 0.14000114798545837 | Test loss: 0.34053701162338257\n",
      "Epoch: 2138 | Loss: 0.140001118183136 | Test loss: 0.34053298830986023\n",
      "Epoch: 2139 | Loss: 0.1400010883808136 | Test loss: 0.3405289351940155\n",
      "Epoch: 2140 | Loss: 0.1400010734796524 | Test loss: 0.34054580330848694\n",
      "Epoch: 2141 | Loss: 0.1400010585784912 | Test loss: 0.3405417501926422\n",
      "Epoch: 2142 | Loss: 0.14000102877616882 | Test loss: 0.3405377268791199\n",
      "Epoch: 2143 | Loss: 0.14000101387500763 | Test loss: 0.34053367376327515\n",
      "Epoch: 2144 | Loss: 0.14000098407268524 | Test loss: 0.3405505418777466\n",
      "Epoch: 2145 | Loss: 0.14000095427036285 | Test loss: 0.34054648876190186\n",
      "Epoch: 2146 | Loss: 0.14000093936920166 | Test loss: 0.34054240584373474\n",
      "Epoch: 2147 | Loss: 0.14000092446804047 | Test loss: 0.34055107831954956\n",
      "Epoch: 2148 | Loss: 0.14000090956687927 | Test loss: 0.3405597507953644\n",
      "Epoch: 2149 | Loss: 0.14000090956687927 | Test loss: 0.3405684530735016\n",
      "Epoch: 2150 | Loss: 0.14000090956687927 | Test loss: 0.3405771255493164\n",
      "Epoch: 2151 | Loss: 0.14000089466571808 | Test loss: 0.34058576822280884\n",
      "Epoch: 2152 | Loss: 0.14000087976455688 | Test loss: 0.34059447050094604\n",
      "Epoch: 2153 | Loss: 0.14000087976455688 | Test loss: 0.34060317277908325\n",
      "Epoch: 2154 | Loss: 0.1400008648633957 | Test loss: 0.34061184525489807\n",
      "Epoch: 2155 | Loss: 0.1400008499622345 | Test loss: 0.3406204879283905\n",
      "Epoch: 2156 | Loss: 0.1400008499622345 | Test loss: 0.3406291902065277\n",
      "Epoch: 2157 | Loss: 0.1400008350610733 | Test loss: 0.3406378924846649\n",
      "Epoch: 2158 | Loss: 0.1400008201599121 | Test loss: 0.34064650535583496\n",
      "Epoch: 2159 | Loss: 0.14000080525875092 | Test loss: 0.34065520763397217\n",
      "Epoch: 2160 | Loss: 0.14000080525875092 | Test loss: 0.3406639099121094\n",
      "Epoch: 2161 | Loss: 0.14000080525875092 | Test loss: 0.3406725525856018\n",
      "Epoch: 2162 | Loss: 0.14000079035758972 | Test loss: 0.3406812250614166\n",
      "Epoch: 2163 | Loss: 0.14000077545642853 | Test loss: 0.34068992733955383\n",
      "Epoch: 2164 | Loss: 0.14000077545642853 | Test loss: 0.34069857001304626\n",
      "Epoch: 2165 | Loss: 0.14000076055526733 | Test loss: 0.34070730209350586\n",
      "Epoch: 2166 | Loss: 0.14000074565410614 | Test loss: 0.3407159447669983\n",
      "Epoch: 2167 | Loss: 0.14000074565410614 | Test loss: 0.3407246470451355\n",
      "Epoch: 2168 | Loss: 0.14000073075294495 | Test loss: 0.3407333195209503\n",
      "Epoch: 2169 | Loss: 0.14000071585178375 | Test loss: 0.34074196219444275\n",
      "Epoch: 2170 | Loss: 0.14000070095062256 | Test loss: 0.34075063467025757\n",
      "Epoch: 2171 | Loss: 0.14000070095062256 | Test loss: 0.3407593369483948\n",
      "Epoch: 2172 | Loss: 0.14000068604946136 | Test loss: 0.3407680094242096\n",
      "Epoch: 2173 | Loss: 0.14000067114830017 | Test loss: 0.340776652097702\n",
      "Epoch: 2174 | Loss: 0.14000067114830017 | Test loss: 0.34078535437583923\n",
      "Epoch: 2175 | Loss: 0.14000067114830017 | Test loss: 0.34079402685165405\n",
      "Epoch: 2176 | Loss: 0.14000065624713898 | Test loss: 0.34080272912979126\n",
      "Epoch: 2177 | Loss: 0.14000064134597778 | Test loss: 0.34081143140792847\n",
      "Epoch: 2178 | Loss: 0.14000064134597778 | Test loss: 0.3408200740814209\n",
      "Epoch: 2179 | Loss: 0.1400006264448166 | Test loss: 0.3408287465572357\n",
      "Epoch: 2180 | Loss: 0.1400006115436554 | Test loss: 0.34083741903305054\n",
      "Epoch: 2181 | Loss: 0.1400006115436554 | Test loss: 0.34084609150886536\n",
      "Epoch: 2182 | Loss: 0.140000581741333 | Test loss: 0.3408547639846802\n",
      "Epoch: 2183 | Loss: 0.140000581741333 | Test loss: 0.340863436460495\n",
      "Epoch: 2184 | Loss: 0.14000056684017181 | Test loss: 0.3408721387386322\n",
      "Epoch: 2185 | Loss: 0.14000056684017181 | Test loss: 0.340880811214447\n",
      "Epoch: 2186 | Loss: 0.14000055193901062 | Test loss: 0.34088945388793945\n",
      "Epoch: 2187 | Loss: 0.14000055193901062 | Test loss: 0.34089815616607666\n",
      "Epoch: 2188 | Loss: 0.14000053703784943 | Test loss: 0.34090685844421387\n",
      "Epoch: 2189 | Loss: 0.14000053703784943 | Test loss: 0.3409155309200287\n",
      "Epoch: 2190 | Loss: 0.14000050723552704 | Test loss: 0.3409241735935211\n",
      "Epoch: 2191 | Loss: 0.14000050723552704 | Test loss: 0.3409328758716583\n",
      "Epoch: 2192 | Loss: 0.14000050723552704 | Test loss: 0.34094154834747314\n",
      "Epoch: 2193 | Loss: 0.14000049233436584 | Test loss: 0.3409501910209656\n",
      "Epoch: 2194 | Loss: 0.14000047743320465 | Test loss: 0.3409588932991028\n",
      "Epoch: 2195 | Loss: 0.14000047743320465 | Test loss: 0.34096759557724\n",
      "Epoch: 2196 | Loss: 0.14000046253204346 | Test loss: 0.3409762382507324\n",
      "Epoch: 2197 | Loss: 0.14000044763088226 | Test loss: 0.340963751077652\n",
      "Epoch: 2198 | Loss: 0.14000044763088226 | Test loss: 0.3409723937511444\n",
      "Epoch: 2199 | Loss: 0.14000044763088226 | Test loss: 0.34098106622695923\n",
      "Epoch: 2200 | Loss: 0.14000043272972107 | Test loss: 0.3409685790538788\n",
      "Epoch: 2201 | Loss: 0.14000043272972107 | Test loss: 0.3409772515296936\n",
      "Epoch: 2202 | Loss: 0.14000041782855988 | Test loss: 0.34098589420318604\n",
      "Epoch: 2203 | Loss: 0.14000041782855988 | Test loss: 0.3409733772277832\n",
      "Epoch: 2204 | Loss: 0.14000041782855988 | Test loss: 0.340982049703598\n",
      "Epoch: 2205 | Loss: 0.14000041782855988 | Test loss: 0.3409695625305176\n",
      "Epoch: 2206 | Loss: 0.14000040292739868 | Test loss: 0.3409782350063324\n",
      "Epoch: 2207 | Loss: 0.14000040292739868 | Test loss: 0.3409869372844696\n",
      "Epoch: 2208 | Loss: 0.14000040292739868 | Test loss: 0.3409744203090668\n",
      "Epoch: 2209 | Loss: 0.1400003880262375 | Test loss: 0.3409830927848816\n",
      "Epoch: 2210 | Loss: 0.1400003731250763 | Test loss: 0.34097057580947876\n",
      "Epoch: 2211 | Loss: 0.1400003731250763 | Test loss: 0.34097927808761597\n",
      "Epoch: 2212 | Loss: 0.1400003731250763 | Test loss: 0.3409879207611084\n",
      "Epoch: 2213 | Loss: 0.1400003582239151 | Test loss: 0.34097540378570557\n",
      "Epoch: 2214 | Loss: 0.1400003582239151 | Test loss: 0.340984046459198\n",
      "Epoch: 2215 | Loss: 0.1400003433227539 | Test loss: 0.3409927189350128\n",
      "Epoch: 2216 | Loss: 0.1400003433227539 | Test loss: 0.34098026156425476\n",
      "Epoch: 2217 | Loss: 0.1400003433227539 | Test loss: 0.3409889042377472\n",
      "Epoch: 2218 | Loss: 0.1400003284215927 | Test loss: 0.34097641706466675\n",
      "Epoch: 2219 | Loss: 0.1400003284215927 | Test loss: 0.3409850299358368\n",
      "Epoch: 2220 | Loss: 0.1400003284215927 | Test loss: 0.340993732213974\n",
      "Epoch: 2221 | Loss: 0.1400003284215927 | Test loss: 0.34098121523857117\n",
      "Epoch: 2222 | Loss: 0.14000031352043152 | Test loss: 0.3409899175167084\n",
      "Epoch: 2223 | Loss: 0.14000029861927032 | Test loss: 0.3409985899925232\n",
      "Epoch: 2224 | Loss: 0.14000029861927032 | Test loss: 0.34098610281944275\n",
      "Epoch: 2225 | Loss: 0.14000029861927032 | Test loss: 0.3409947156906128\n",
      "Epoch: 2226 | Loss: 0.14000028371810913 | Test loss: 0.34098219871520996\n",
      "Epoch: 2227 | Loss: 0.14000028371810913 | Test loss: 0.34099090099334717\n",
      "Epoch: 2228 | Loss: 0.14000026881694794 | Test loss: 0.3409996032714844\n",
      "Epoch: 2229 | Loss: 0.14000026881694794 | Test loss: 0.34098705649375916\n",
      "Epoch: 2230 | Loss: 0.14000026881694794 | Test loss: 0.3409956991672516\n",
      "Epoch: 2231 | Loss: 0.14000025391578674 | Test loss: 0.3410044014453888\n",
      "Epoch: 2232 | Loss: 0.14000023901462555 | Test loss: 0.34099188446998596\n",
      "Epoch: 2233 | Loss: 0.14000023901462555 | Test loss: 0.34100058674812317\n",
      "Epoch: 2234 | Loss: 0.14000023901462555 | Test loss: 0.34098806977272034\n",
      "Epoch: 2235 | Loss: 0.14000022411346436 | Test loss: 0.34099674224853516\n",
      "Epoch: 2236 | Loss: 0.14000022411346436 | Test loss: 0.3410053849220276\n",
      "Epoch: 2237 | Loss: 0.14000020921230316 | Test loss: 0.34099286794662476\n",
      "Epoch: 2238 | Loss: 0.14000020921230316 | Test loss: 0.34100157022476196\n",
      "Epoch: 2239 | Loss: 0.14000020921230316 | Test loss: 0.3410102128982544\n",
      "Epoch: 2240 | Loss: 0.14000020921230316 | Test loss: 0.34099772572517395\n",
      "Epoch: 2241 | Loss: 0.14000019431114197 | Test loss: 0.34100639820098877\n",
      "Epoch: 2242 | Loss: 0.14000019431114197 | Test loss: 0.34099388122558594\n",
      "Epoch: 2243 | Loss: 0.14000019431114197 | Test loss: 0.34100258350372314\n",
      "Epoch: 2244 | Loss: 0.14000017940998077 | Test loss: 0.3410112261772156\n",
      "Epoch: 2245 | Loss: 0.14000016450881958 | Test loss: 0.34099870920181274\n",
      "Epoch: 2246 | Loss: 0.14000016450881958 | Test loss: 0.34100741147994995\n",
      "Epoch: 2247 | Loss: 0.14000016450881958 | Test loss: 0.34099486470222473\n",
      "Epoch: 2248 | Loss: 0.1400001496076584 | Test loss: 0.34100356698036194\n",
      "Epoch: 2249 | Loss: 0.1400001496076584 | Test loss: 0.34101226925849915\n",
      "Epoch: 2250 | Loss: 0.1400001347064972 | Test loss: 0.3409997224807739\n",
      "Epoch: 2251 | Loss: 0.1400001347064972 | Test loss: 0.34100839495658875\n",
      "Epoch: 2252 | Loss: 0.1400001347064972 | Test loss: 0.34101706743240356\n",
      "Epoch: 2253 | Loss: 0.1400001347064972 | Test loss: 0.34100455045700073\n",
      "Epoch: 2254 | Loss: 0.1400001049041748 | Test loss: 0.3410131633281708\n",
      "Epoch: 2255 | Loss: 0.1400001049041748 | Test loss: 0.3410007059574127\n",
      "Epoch: 2256 | Loss: 0.1400001049041748 | Test loss: 0.3410094082355499\n",
      "Epoch: 2257 | Loss: 0.1400001049041748 | Test loss: 0.34101805090904236\n",
      "Epoch: 2258 | Loss: 0.1400000900030136 | Test loss: 0.3410055339336395\n",
      "Epoch: 2259 | Loss: 0.1400000900030136 | Test loss: 0.34101423621177673\n",
      "Epoch: 2260 | Loss: 0.14000007510185242 | Test loss: 0.34102290868759155\n",
      "Epoch: 2261 | Loss: 0.14000007510185242 | Test loss: 0.34101036190986633\n",
      "Epoch: 2262 | Loss: 0.14000007510185242 | Test loss: 0.34101906418800354\n",
      "Epoch: 2263 | Loss: 0.14000006020069122 | Test loss: 0.3410065472126007\n",
      "Epoch: 2264 | Loss: 0.14000006020069122 | Test loss: 0.3410152494907379\n",
      "Epoch: 2265 | Loss: 0.14000006020069122 | Test loss: 0.34102392196655273\n",
      "Epoch: 2266 | Loss: 0.14000006020069122 | Test loss: 0.3410114347934723\n",
      "Epoch: 2267 | Loss: 0.14000004529953003 | Test loss: 0.34102004766464233\n",
      "Epoch: 2268 | Loss: 0.14000003039836884 | Test loss: 0.34102874994277954\n",
      "Epoch: 2269 | Loss: 0.14000003039836884 | Test loss: 0.3410162329673767\n",
      "Epoch: 2270 | Loss: 0.14000003039836884 | Test loss: 0.3410249352455139\n",
      "Epoch: 2271 | Loss: 0.14000001549720764 | Test loss: 0.3410124182701111\n",
      "Epoch: 2272 | Loss: 0.14000001549720764 | Test loss: 0.3410210609436035\n",
      "Epoch: 2273 | Loss: 0.14000000059604645 | Test loss: 0.34102973341941833\n",
      "Epoch: 2274 | Loss: 0.14000000059604645 | Test loss: 0.3410172164440155\n",
      "Epoch: 2275 | Loss: 0.14000000059604645 | Test loss: 0.34103843569755554\n",
      "Epoch: 2276 | Loss: 0.14000000059604645 | Test loss: 0.3410258889198303\n",
      "Epoch: 2277 | Loss: 0.14000000059604645 | Test loss: 0.3410470485687256\n",
      "Epoch: 2278 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2279 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2280 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2281 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2282 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2283 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2284 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2285 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2286 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2287 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2288 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2289 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2290 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2291 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2292 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2293 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2294 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2295 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2296 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2297 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2298 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2299 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2300 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2301 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2302 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2303 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2304 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2305 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2306 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2307 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2308 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2309 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2310 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2311 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2312 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2313 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2314 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2315 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2316 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2317 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2318 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2319 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2320 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2321 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2322 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2323 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2324 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2325 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2326 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2327 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2328 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2329 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2330 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2331 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2332 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2333 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2334 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2335 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2336 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2337 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2338 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2339 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2340 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2341 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2342 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2343 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2344 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2345 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2346 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2347 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2348 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2349 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2350 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2351 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2352 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2353 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2354 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2355 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2356 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2357 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2358 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2359 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2360 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2361 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2362 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2363 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2364 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2365 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2366 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2367 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2368 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2369 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2370 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2371 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2372 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2373 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2374 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2375 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2376 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2377 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2378 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2379 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2380 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2381 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2382 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2383 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2384 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2385 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2386 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2387 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2388 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2389 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2390 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2391 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2392 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2393 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2394 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2395 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2396 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2397 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2398 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2399 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2400 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2401 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2402 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2403 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2404 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2405 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2406 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2407 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2408 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2409 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2410 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2411 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2412 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2413 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2414 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2415 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2416 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2417 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2418 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2419 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2420 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2421 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2422 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2423 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2424 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2425 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2426 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2427 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2428 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2429 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2430 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2431 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2432 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2433 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2434 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2435 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2436 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2437 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2438 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2439 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2440 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2441 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2442 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2443 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2444 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2445 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2446 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2447 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2448 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2449 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2450 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2451 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2452 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2453 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2454 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2455 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2456 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2457 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2458 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2459 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2460 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2461 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2462 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2463 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2464 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2465 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2466 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2467 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2468 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2469 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2470 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2471 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2472 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2473 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2474 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2475 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2476 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2477 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2478 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2479 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2480 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2481 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2482 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2483 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2484 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2485 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2486 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2487 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2488 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2489 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2490 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2491 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2492 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2493 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2494 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2495 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2496 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2497 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2498 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2499 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2500 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2501 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2502 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2503 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2504 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2505 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2506 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2507 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2508 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2509 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2510 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2511 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2512 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2513 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2514 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2515 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2516 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2517 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2518 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2519 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2520 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2521 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2522 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2523 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2524 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2525 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2526 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2527 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2528 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2529 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2530 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2531 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2532 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2533 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2534 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2535 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2536 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2537 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2538 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2539 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2540 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2541 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2542 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2543 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2544 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2545 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2546 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2547 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2548 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2549 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2550 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2551 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2552 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2553 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2554 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2555 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2556 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2557 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2558 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2559 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2560 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2561 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2562 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2563 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2564 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2565 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2566 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2567 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2568 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2569 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2570 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2571 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2572 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2573 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2574 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2575 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2576 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2577 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2578 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2579 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2580 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2581 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2582 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2583 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2584 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2585 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2586 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2587 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2588 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2589 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2590 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2591 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2592 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2593 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2594 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2595 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2596 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2597 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2598 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2599 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2600 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2601 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2602 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2603 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2604 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2605 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2606 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2607 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2608 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2609 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2610 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2611 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2612 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2613 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2614 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2615 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2616 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2617 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2618 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2619 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2620 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2621 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2622 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2623 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2624 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2625 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2626 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2627 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2628 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2629 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2630 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2631 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2632 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2633 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2634 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2635 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2636 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2637 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2638 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2639 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2640 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2641 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2642 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2643 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2644 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2645 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2646 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2647 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2648 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2649 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2650 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2651 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2652 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2653 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2654 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2655 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2656 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2657 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2658 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2659 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2660 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2661 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2662 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2663 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2664 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2665 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2666 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2667 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2668 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2669 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2670 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2671 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2672 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2673 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2674 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2675 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2676 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2677 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2678 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2679 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2680 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2681 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2682 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2683 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2684 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2685 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2686 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2687 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2688 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2689 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2690 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2691 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2692 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2693 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2694 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2695 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2696 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2697 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2698 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2699 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2700 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2701 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2702 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2703 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2704 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2705 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2706 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2707 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2708 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2709 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2710 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2711 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2712 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2713 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2714 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2715 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2716 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2717 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2718 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2719 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2720 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2721 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2722 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2723 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2724 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2725 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2726 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2727 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2728 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2729 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2730 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2731 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2732 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2733 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2734 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2735 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2736 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2737 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2738 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2739 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2740 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2741 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2742 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2743 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2744 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2745 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2746 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2747 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2748 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2749 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2750 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2751 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2752 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2753 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2754 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2755 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2756 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2757 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2758 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2759 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2760 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2761 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2762 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2763 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2764 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2765 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2766 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2767 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2768 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2769 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2770 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2771 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2772 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2773 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2774 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2775 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2776 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2777 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2778 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2779 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2780 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2781 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2782 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2783 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2784 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2785 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2786 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2787 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2788 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2789 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2790 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2791 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2792 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2793 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2794 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2795 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2796 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2797 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2798 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2799 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2800 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2801 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2802 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2803 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2804 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2805 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2806 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2807 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2808 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2809 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2810 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2811 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2812 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2813 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2814 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2815 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2816 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2817 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2818 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2819 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2820 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2821 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2822 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2823 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2824 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2825 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2826 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2827 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2828 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2829 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2830 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2831 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2832 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2833 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2834 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2835 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2836 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2837 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2838 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2839 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2840 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2841 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2842 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2843 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2844 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2845 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2846 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2847 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2848 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2849 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2850 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2851 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2852 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2853 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2854 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2855 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2856 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2857 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2858 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2859 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2860 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2861 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2862 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2863 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2864 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2865 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2866 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2867 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2868 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2869 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2870 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2871 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2872 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2873 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2874 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2875 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2876 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2877 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2878 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2879 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2880 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2881 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2882 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2883 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2884 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2885 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2886 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2887 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2888 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2889 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2890 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2891 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2892 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2893 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2894 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2895 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2896 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2897 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2898 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2899 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2900 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2901 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2902 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2903 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2904 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2905 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2906 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2907 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2908 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2909 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2910 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2911 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2912 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2913 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2914 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2915 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2916 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2917 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2918 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2919 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2920 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2921 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2922 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2923 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2924 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2925 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2926 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2927 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2928 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2929 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2930 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2931 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2932 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2933 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2934 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2935 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2936 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2937 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2938 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2939 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2940 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2941 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2942 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2943 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2944 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2945 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2946 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2947 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2948 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2949 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2950 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2951 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2952 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2953 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2954 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2955 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2956 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2957 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2958 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2959 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2960 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2961 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2962 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2963 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2964 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2965 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2966 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2967 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2968 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2969 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2970 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2971 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2972 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2973 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2974 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2975 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2976 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2977 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2978 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2979 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2980 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2981 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2982 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2983 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2984 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2985 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2986 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2987 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2988 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2989 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2990 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2991 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2992 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2993 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2994 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2995 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2996 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2997 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2998 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 2999 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3000 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3001 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3002 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3003 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3004 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3005 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3006 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3007 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3008 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3009 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3010 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3011 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3012 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3013 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3014 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3015 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3016 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3017 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3018 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3019 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3020 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3021 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3022 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3023 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3024 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3025 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3026 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3027 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3028 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3029 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3030 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3031 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3032 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3033 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3034 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3035 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3036 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3037 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3038 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3039 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3040 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3041 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3042 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3043 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3044 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3045 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3046 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3047 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3048 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3049 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3050 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3051 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3052 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3053 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3054 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3055 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3056 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3057 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3058 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3059 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3060 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3061 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3062 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3063 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3064 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3065 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3066 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3067 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3068 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3069 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3070 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3071 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3072 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3073 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3074 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3075 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3076 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3077 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3078 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3079 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3080 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3081 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3082 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3083 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3084 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3085 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3086 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3087 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3088 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3089 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3090 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3091 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3092 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3093 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3094 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3095 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3096 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3097 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3098 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3099 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3100 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3101 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3102 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3103 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3104 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3105 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3106 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3107 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3108 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3109 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3110 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3111 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3112 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3113 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3114 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3115 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3116 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3117 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3118 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3119 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3120 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3121 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3122 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3123 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3124 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3125 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3126 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3127 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3128 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3129 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3130 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3131 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3132 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3133 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3134 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3135 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3136 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3137 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3138 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3139 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3140 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3141 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3142 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3143 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3144 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3145 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3146 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3147 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3148 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3149 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3150 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3151 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3152 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3153 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3154 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3155 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3156 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3157 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3158 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3159 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3160 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3161 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3162 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3163 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3164 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3165 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3166 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3167 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3168 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3169 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3170 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3171 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3172 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3173 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3174 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3175 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3176 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3177 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3178 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3179 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3180 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3181 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3182 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3183 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3184 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3185 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3186 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3187 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3188 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3189 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3190 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3191 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3192 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3193 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3194 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3195 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3196 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3197 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3198 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3199 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3200 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3201 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3202 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3203 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3204 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3205 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3206 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3207 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3208 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3209 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3210 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3211 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3212 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3213 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3214 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3215 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3216 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3217 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3218 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3219 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3220 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3221 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3222 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3223 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3224 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3225 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3226 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3227 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3228 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3229 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3230 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3231 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3232 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3233 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3234 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3235 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3236 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3237 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3238 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3239 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3240 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3241 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3242 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3243 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3244 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3245 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3246 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3247 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3248 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3249 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3250 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3251 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3252 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3253 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3254 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3255 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3256 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3257 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3258 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3259 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3260 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3261 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3262 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3263 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3264 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3265 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3266 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3267 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3268 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3269 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3270 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3271 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3272 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3273 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3274 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3275 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3276 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3277 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3278 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3279 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3280 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3281 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3282 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3283 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3284 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3285 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3286 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3287 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3288 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3289 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3290 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3291 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3292 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3293 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3294 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3295 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3296 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3297 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3298 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3299 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3300 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3301 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3302 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3303 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3304 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3305 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3306 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3307 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3308 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3309 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3310 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3311 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3312 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3313 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3314 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3315 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3316 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3317 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3318 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3319 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3320 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3321 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3322 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3323 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3324 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3325 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3326 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3327 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3328 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3329 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3330 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3331 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3332 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3333 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3334 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3335 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3336 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3337 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3338 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3339 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3340 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3341 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3342 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3343 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3344 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3345 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3346 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3347 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3348 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3349 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3350 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3351 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3352 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3353 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3354 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3355 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3356 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3357 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3358 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3359 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3360 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3361 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3362 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3363 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3364 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3365 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3366 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3367 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3368 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3369 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3370 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3371 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3372 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3373 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3374 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3375 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3376 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3377 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3378 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3379 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3380 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3381 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3382 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3383 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3384 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3385 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3386 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3387 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3388 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3389 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3390 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3391 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3392 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3393 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3394 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3395 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3396 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3397 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3398 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3399 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3400 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3401 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3402 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3403 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3404 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3405 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3406 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3407 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3408 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3409 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3410 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3411 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3412 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3413 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3414 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3415 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3416 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3417 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3418 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3419 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3420 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3421 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3422 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3423 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3424 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3425 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3426 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3427 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3428 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3429 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3430 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3431 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3432 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3433 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3434 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3435 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3436 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3437 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3438 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3439 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3440 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3441 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3442 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3443 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3444 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3445 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3446 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3447 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3448 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3449 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3450 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3451 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3452 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3453 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3454 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3455 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3456 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3457 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3458 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3459 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3460 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3461 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3462 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3463 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3464 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3465 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3466 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3467 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3468 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3469 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3470 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3471 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3472 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3473 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3474 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3475 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3476 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3477 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3478 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3479 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3480 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3481 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3482 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3483 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3484 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3485 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3486 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3487 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3488 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3489 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3490 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3491 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3492 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3493 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3494 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3495 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3496 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3497 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3498 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3499 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3500 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3501 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3502 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3503 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3504 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3505 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3506 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3507 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3508 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3509 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3510 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3511 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3512 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3513 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3514 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3515 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3516 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3517 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3518 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3519 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3520 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3521 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3522 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3523 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3524 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3525 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3526 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3527 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3528 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3529 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3530 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3531 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3532 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3533 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3534 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3535 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3536 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3537 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3538 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3539 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3540 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3541 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3542 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3543 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3544 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3545 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3546 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3547 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3548 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3549 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3550 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3551 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3552 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3553 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3554 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3555 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3556 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3557 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3558 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3559 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3560 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3561 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3562 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3563 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3564 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3565 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3566 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3567 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3568 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3569 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3570 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3571 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3572 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3573 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3574 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3575 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3576 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3577 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3578 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3579 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3580 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3581 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3582 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3583 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3584 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3585 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3586 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3587 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3588 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3589 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3590 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3591 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3592 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3593 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3594 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3595 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3596 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3597 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3598 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3599 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3600 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3601 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3602 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3603 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3604 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3605 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3606 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3607 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3608 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3609 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3610 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3611 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3612 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3613 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3614 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3615 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3616 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3617 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3618 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3619 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3620 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3621 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3622 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3623 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3624 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3625 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3626 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3627 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3628 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3629 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3630 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3631 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3632 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3633 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3634 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3635 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3636 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3637 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3638 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3639 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3640 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3641 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3642 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3643 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3644 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3645 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3646 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3647 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3648 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3649 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3650 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3651 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3652 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3653 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3654 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3655 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3656 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3657 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3658 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3659 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3660 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3661 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3662 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3663 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3664 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3665 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3666 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3667 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3668 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3669 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3670 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3671 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3672 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3673 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3674 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3675 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3676 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3677 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3678 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3679 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3680 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3681 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3682 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3683 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3684 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3685 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3686 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3687 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3688 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3689 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3690 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3691 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3692 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3693 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3694 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3695 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3696 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3697 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3698 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3699 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3700 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3701 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3702 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3703 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3704 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3705 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3706 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3707 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3708 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3709 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3710 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3711 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3712 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3713 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3714 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3715 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3716 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3717 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3718 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3719 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3720 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3721 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3722 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3723 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3724 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3725 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3726 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3727 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3728 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3729 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3730 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3731 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3732 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3733 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3734 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3735 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3736 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3737 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3738 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3739 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3740 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3741 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3742 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3743 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3744 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3745 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3746 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3747 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3748 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3749 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3750 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3751 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3752 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3753 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3754 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3755 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3756 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3757 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3758 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3759 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3760 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3761 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3762 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3763 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3764 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3765 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3766 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3767 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3768 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3769 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3770 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3771 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3772 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3773 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3774 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3775 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3776 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3777 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3778 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3779 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3780 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3781 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3782 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3783 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3784 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3785 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3786 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3787 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3788 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3789 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3790 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3791 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3792 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3793 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3794 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3795 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3796 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3797 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3798 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3799 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3800 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3801 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3802 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3803 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3804 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3805 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3806 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3807 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3808 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3809 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3810 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3811 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3812 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3813 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3814 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3815 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3816 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3817 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3818 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3819 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3820 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3821 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3822 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3823 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3824 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3825 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3826 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3827 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3828 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3829 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3830 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3831 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3832 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3833 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3834 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3835 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3836 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3837 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3838 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3839 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3840 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3841 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3842 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3843 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3844 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3845 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3846 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3847 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3848 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3849 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3850 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3851 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3852 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3853 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3854 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3855 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3856 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3857 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3858 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3859 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3860 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3861 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3862 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3863 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3864 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3865 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3866 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3867 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3868 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3869 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3870 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3871 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3872 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3873 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3874 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3875 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3876 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3877 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3878 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3879 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3880 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3881 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3882 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3883 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3884 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3885 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3886 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3887 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3888 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3889 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3890 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3891 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3892 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3893 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3894 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3895 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3896 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3897 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3898 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3899 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3900 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3901 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3902 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3903 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3904 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3905 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3906 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3907 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3908 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3909 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3910 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3911 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3912 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3913 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3914 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3915 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3916 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3917 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3918 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3919 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3920 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3921 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3922 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3923 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3924 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3925 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3926 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3927 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3928 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3929 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3930 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3931 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3932 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3933 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3934 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3935 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3936 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3937 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3938 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3939 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3940 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3941 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3942 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3943 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3944 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3945 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3946 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3947 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3948 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3949 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3950 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3951 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3952 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3953 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3954 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3955 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3956 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3957 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3958 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3959 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3960 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3961 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3962 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3963 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3964 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3965 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3966 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3967 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3968 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3969 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3970 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3971 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3972 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3973 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3974 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3975 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3976 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3977 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3978 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3979 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3980 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3981 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3982 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3983 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3984 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3985 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3986 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3987 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3988 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3989 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3990 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3991 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3992 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3993 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3994 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3995 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3996 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3997 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3998 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 3999 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4000 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4001 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4002 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4003 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4004 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4005 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4006 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4007 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4008 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4009 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4010 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4011 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4012 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4013 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4014 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4015 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4016 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4017 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4018 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4019 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4020 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4021 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4022 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4023 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4024 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4025 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4026 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4027 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4028 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4029 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4030 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4031 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4032 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4033 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4034 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4035 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4036 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4037 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4038 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4039 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4040 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4041 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4042 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4043 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4044 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4045 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4046 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4047 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4048 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4049 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4050 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4051 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4052 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4053 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4054 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4055 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4056 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4057 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4058 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4059 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4060 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4061 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4062 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4063 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4064 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4065 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4066 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4067 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4068 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4069 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4070 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4071 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4072 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4073 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4074 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4075 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4076 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4077 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4078 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4079 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4080 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4081 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4082 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4083 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4084 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4085 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4086 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4087 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4088 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4089 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4090 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4091 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4092 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4093 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4094 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4095 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4096 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4097 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4098 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4099 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4100 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4101 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4102 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4103 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4104 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4105 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4106 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4107 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4108 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4109 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4110 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4111 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4112 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4113 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4114 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4115 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4116 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4117 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4118 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4119 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4120 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4121 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4122 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4123 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4124 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4125 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4126 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4127 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4128 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4129 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4130 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4131 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4132 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4133 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4134 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4135 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4136 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4137 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4138 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4139 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4140 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4141 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4142 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4143 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4144 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4145 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4146 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4147 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4148 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4149 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4150 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4151 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4152 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4153 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4154 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4155 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4156 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4157 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4158 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4159 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4160 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4161 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4162 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4163 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4164 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4165 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4166 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4167 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4168 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4169 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4170 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4171 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4172 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4173 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4174 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4175 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4176 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4177 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4178 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4179 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4180 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4181 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4182 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4183 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4184 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4185 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4186 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4187 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4188 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4189 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4190 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4191 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4192 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4193 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4194 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4195 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4196 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4197 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4198 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4199 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4200 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4201 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4202 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4203 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4204 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4205 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4206 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4207 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4208 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4209 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4210 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4211 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4212 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4213 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4214 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4215 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4216 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4217 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4218 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4219 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4220 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4221 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4222 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4223 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4224 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4225 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4226 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4227 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4228 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4229 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4230 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4231 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4232 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4233 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4234 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4235 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4236 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4237 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4238 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4239 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4240 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4241 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4242 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4243 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4244 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4245 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4246 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4247 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4248 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4249 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4250 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4251 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4252 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4253 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4254 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4255 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4256 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4257 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4258 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4259 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4260 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4261 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4262 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4263 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4264 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4265 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4266 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4267 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4268 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4269 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4270 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4271 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4272 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4273 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4274 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4275 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4276 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4277 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4278 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4279 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4280 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4281 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4282 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4283 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4284 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4285 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4286 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4287 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4288 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4289 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4290 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4291 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4292 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4293 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4294 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4295 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4296 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4297 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4298 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4299 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4300 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4301 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4302 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4303 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4304 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4305 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4306 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4307 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4308 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4309 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4310 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4311 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4312 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4313 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4314 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4315 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4316 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4317 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4318 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4319 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4320 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4321 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4322 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4323 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4324 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4325 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4326 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4327 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4328 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4329 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4330 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4331 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4332 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4333 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4334 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4335 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4336 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4337 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4338 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4339 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4340 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4341 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4342 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4343 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4344 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4345 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4346 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4347 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4348 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4349 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4350 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4351 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4352 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4353 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4354 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4355 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4356 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4357 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4358 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4359 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4360 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4361 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4362 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4363 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4364 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4365 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4366 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4367 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4368 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4369 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4370 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4371 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4372 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4373 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4374 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4375 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4376 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4377 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4378 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4379 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4380 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4381 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4382 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4383 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4384 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4385 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4386 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4387 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4388 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4389 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4390 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4391 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4392 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4393 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4394 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4395 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4396 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4397 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4398 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4399 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4400 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4401 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4402 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4403 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4404 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4405 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4406 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4407 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4408 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4409 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4410 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4411 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4412 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4413 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4414 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4415 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4416 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4417 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4418 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4419 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4420 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4421 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4422 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4423 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4424 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4425 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4426 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4427 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4428 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4429 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4430 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4431 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4432 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4433 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4434 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4435 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4436 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4437 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4438 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4439 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4440 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4441 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4442 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4443 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4444 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4445 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4446 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4447 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4448 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4449 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4450 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4451 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4452 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4453 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4454 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4455 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4456 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4457 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4458 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4459 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4460 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4461 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4462 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4463 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4464 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4465 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4466 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4467 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4468 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4469 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4470 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4471 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4472 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4473 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4474 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4475 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4476 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4477 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4478 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4479 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4480 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4481 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4482 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4483 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4484 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4485 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4486 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4487 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4488 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4489 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4490 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4491 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4492 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4493 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4494 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4495 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4496 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4497 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4498 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4499 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4500 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4501 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4502 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4503 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4504 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4505 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4506 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4507 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4508 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4509 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4510 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4511 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4512 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4513 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4514 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4515 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4516 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4517 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4518 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4519 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4520 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4521 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4522 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4523 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4524 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4525 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4526 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4527 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4528 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4529 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4530 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4531 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4532 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4533 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4534 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4535 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4536 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4537 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4538 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4539 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4540 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4541 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4542 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4543 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4544 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4545 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4546 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4547 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4548 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4549 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4550 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4551 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4552 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4553 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4554 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4555 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4556 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4557 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4558 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4559 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4560 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4561 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4562 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4563 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4564 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4565 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4566 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4567 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4568 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4569 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4570 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4571 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4572 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4573 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4574 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4575 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4576 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4577 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4578 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4579 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4580 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4581 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4582 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4583 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4584 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4585 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4586 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4587 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4588 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4589 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4590 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4591 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4592 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4593 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4594 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4595 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4596 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4597 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4598 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4599 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4600 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4601 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4602 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4603 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4604 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4605 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4606 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4607 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4608 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4609 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4610 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4611 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4612 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4613 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4614 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4615 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4616 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4617 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4618 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4619 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4620 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4621 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4622 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4623 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4624 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4625 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4626 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4627 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4628 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4629 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4630 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4631 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4632 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4633 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4634 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4635 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4636 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4637 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4638 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4639 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4640 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4641 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4642 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4643 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4644 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4645 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4646 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4647 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4648 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4649 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4650 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4651 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4652 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4653 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4654 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4655 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4656 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4657 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4658 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4659 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4660 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4661 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4662 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4663 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4664 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4665 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4666 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4667 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4668 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4669 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4670 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4671 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4672 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4673 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4674 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4675 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4676 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4677 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4678 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4679 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4680 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4681 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4682 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4683 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4684 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4685 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4686 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4687 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4688 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4689 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4690 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4691 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4692 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4693 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4694 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4695 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4696 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4697 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4698 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4699 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4700 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4701 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4702 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4703 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4704 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4705 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4706 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4707 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4708 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4709 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4710 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4711 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4712 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4713 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4714 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4715 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4716 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4717 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4718 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4719 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4720 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4721 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4722 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4723 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4724 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4725 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4726 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4727 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4728 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4729 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4730 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4731 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4732 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4733 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4734 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4735 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4736 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4737 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4738 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4739 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4740 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4741 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4742 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4743 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4744 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4745 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4746 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4747 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4748 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4749 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4750 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4751 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4752 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4753 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4754 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4755 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4756 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4757 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4758 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4759 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4760 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4761 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4762 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4763 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4764 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4765 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4766 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4767 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4768 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4769 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4770 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4771 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4772 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4773 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4774 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4775 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4776 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4777 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4778 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4779 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4780 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4781 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4782 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4783 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4784 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4785 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4786 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4787 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4788 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4789 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4790 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4791 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4792 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4793 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4794 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4795 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4796 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4797 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4798 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4799 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4800 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4801 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4802 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4803 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4804 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4805 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4806 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4807 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4808 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4809 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4810 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4811 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4812 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4813 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4814 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4815 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4816 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4817 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4818 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4819 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4820 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4821 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4822 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4823 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4824 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4825 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4826 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4827 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4828 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4829 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4830 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4831 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4832 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4833 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4834 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4835 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4836 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4837 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4838 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4839 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4840 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4841 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4842 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4843 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4844 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4845 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4846 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4847 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4848 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4849 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4850 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4851 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4852 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4853 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4854 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4855 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4856 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4857 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4858 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4859 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4860 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4861 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4862 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4863 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4864 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4865 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4866 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4867 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4868 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4869 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4870 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4871 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4872 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4873 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4874 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4875 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4876 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4877 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4878 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4879 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4880 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4881 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4882 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4883 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4884 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4885 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4886 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4887 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4888 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4889 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4890 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4891 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4892 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4893 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4894 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4895 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4896 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4897 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4898 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4899 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4900 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4901 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4902 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4903 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4904 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4905 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4906 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4907 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4908 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4909 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4910 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4911 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4912 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4913 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4914 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4915 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4916 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4917 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4918 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4919 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4920 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4921 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4922 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4923 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4924 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4925 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4926 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4927 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4928 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4929 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4930 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4931 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4932 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4933 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4934 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4935 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4936 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4937 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4938 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4939 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4940 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4941 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4942 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4943 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4944 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4945 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4946 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4947 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4948 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4949 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4950 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4951 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4952 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4953 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4954 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4955 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4956 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4957 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4958 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4959 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4960 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4961 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4962 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4963 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4964 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4965 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4966 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4967 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4968 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4969 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4970 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4971 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4972 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4973 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4974 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4975 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4976 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4977 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4978 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4979 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4980 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4981 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4982 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4983 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4984 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4985 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4986 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4987 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4988 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4989 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4990 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4991 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4992 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4993 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4994 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4995 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4996 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4997 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4998 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 4999 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5000 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5001 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5002 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5003 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5004 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5005 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5006 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5007 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5008 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5009 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5010 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5011 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5012 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5013 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5014 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5015 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5016 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5017 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5018 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5019 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5020 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5021 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5022 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5023 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5024 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5025 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5026 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5027 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5028 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5029 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5030 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5031 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5032 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5033 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5034 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5035 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5036 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5037 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5038 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5039 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5040 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5041 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5042 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5043 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5044 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5045 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5046 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5047 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5048 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5049 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5050 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5051 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5052 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5053 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5054 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5055 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5056 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5057 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5058 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5059 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5060 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5061 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5062 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5063 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5064 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5065 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5066 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5067 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5068 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5069 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5070 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5071 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5072 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5073 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5074 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5075 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5076 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5077 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5078 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5079 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5080 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5081 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5082 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5083 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5084 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5085 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5086 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5087 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5088 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5089 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5090 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5091 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5092 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5093 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5094 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5095 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5096 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5097 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5098 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5099 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5100 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5101 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5102 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5103 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5104 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5105 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5106 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5107 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5108 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5109 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5110 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5111 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5112 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5113 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5114 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5115 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5116 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5117 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5118 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5119 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5120 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5121 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5122 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5123 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5124 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5125 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5126 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5127 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5128 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5129 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5130 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5131 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5132 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5133 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5134 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5135 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5136 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5137 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5138 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5139 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5140 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5141 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5142 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5143 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5144 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5145 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5146 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5147 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5148 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5149 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5150 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5151 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5152 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5153 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5154 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5155 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5156 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5157 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5158 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5159 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5160 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5161 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5162 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5163 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5164 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5165 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5166 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5167 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5168 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5169 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5170 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5171 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5172 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5173 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5174 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5175 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5176 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5177 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5178 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5179 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5180 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5181 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5182 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5183 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5184 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5185 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5186 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5187 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5188 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5189 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5190 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5191 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5192 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5193 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5194 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5195 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5196 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5197 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5198 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5199 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5200 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5201 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5202 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5203 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5204 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5205 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5206 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5207 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5208 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5209 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5210 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5211 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5212 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5213 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5214 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5215 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5216 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5217 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5218 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5219 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5220 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5221 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5222 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5223 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5224 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5225 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5226 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5227 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5228 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5229 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5230 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5231 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5232 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5233 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5234 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5235 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5236 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5237 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5238 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5239 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5240 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5241 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5242 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5243 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5244 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5245 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5246 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5247 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5248 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5249 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5250 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5251 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5252 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5253 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5254 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5255 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5256 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5257 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5258 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5259 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5260 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5261 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5262 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5263 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5264 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5265 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5266 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5267 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5268 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5269 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5270 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5271 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5272 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5273 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5274 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5275 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5276 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5277 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5278 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5279 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5280 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5281 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5282 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5283 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5284 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5285 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5286 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5287 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5288 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5289 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5290 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5291 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5292 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5293 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5294 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5295 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5296 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5297 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5298 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5299 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5300 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5301 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5302 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5303 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5304 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5305 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5306 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5307 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5308 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5309 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5310 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5311 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5312 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5313 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5314 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5315 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5316 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5317 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5318 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5319 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5320 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5321 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5322 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5323 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5324 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5325 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5326 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5327 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5328 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5329 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5330 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5331 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5332 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5333 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5334 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5335 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5336 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5337 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5338 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5339 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5340 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5341 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5342 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5343 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5344 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5345 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5346 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5347 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5348 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5349 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5350 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5351 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5352 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5353 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5354 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5355 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5356 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5357 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5358 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5359 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5360 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5361 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5362 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5363 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5364 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5365 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5366 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5367 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5368 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5369 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5370 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5371 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5372 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5373 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5374 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5375 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5376 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5377 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5378 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5379 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5380 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5381 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5382 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5383 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5384 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5385 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5386 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5387 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5388 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5389 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5390 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5391 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5392 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5393 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5394 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5395 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5396 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5397 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5398 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5399 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5400 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5401 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5402 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5403 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5404 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5405 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5406 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5407 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5408 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5409 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5410 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5411 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5412 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5413 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5414 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5415 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5416 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5417 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5418 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5419 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5420 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5421 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5422 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5423 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5424 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5425 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5426 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5427 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5428 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5429 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5430 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5431 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5432 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5433 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5434 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5435 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5436 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5437 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5438 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5439 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5440 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5441 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5442 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5443 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5444 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5445 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5446 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5447 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5448 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5449 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5450 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5451 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5452 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5453 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5454 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5455 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5456 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5457 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5458 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5459 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5460 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5461 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5462 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5463 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5464 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5465 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5466 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5467 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5468 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5469 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5470 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5471 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5472 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5473 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5474 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5475 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5476 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5477 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5478 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5479 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5480 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5481 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5482 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5483 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5484 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5485 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5486 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5487 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5488 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5489 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5490 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5491 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5492 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5493 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5494 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5495 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5496 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5497 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5498 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5499 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5500 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5501 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5502 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5503 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5504 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5505 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5506 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5507 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5508 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5509 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5510 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5511 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5512 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5513 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5514 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5515 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5516 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5517 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5518 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5519 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5520 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5521 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5522 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5523 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5524 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5525 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5526 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5527 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5528 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5529 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5530 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5531 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5532 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5533 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5534 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5535 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5536 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5537 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5538 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5539 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5540 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5541 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5542 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5543 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5544 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5545 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5546 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5547 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5548 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5549 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5550 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5551 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5552 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5553 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5554 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5555 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5556 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5557 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5558 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5559 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5560 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5561 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5562 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5563 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5564 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5565 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5566 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5567 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5568 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5569 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5570 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5571 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5572 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5573 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5574 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5575 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5576 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5577 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5578 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5579 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5580 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5581 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5582 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5583 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5584 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5585 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5586 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5587 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5588 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5589 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5590 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5591 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5592 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5593 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5594 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5595 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5596 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5597 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5598 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5599 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5600 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5601 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5602 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5603 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5604 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5605 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5606 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5607 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5608 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5609 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5610 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5611 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5612 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5613 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5614 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5615 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5616 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5617 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5618 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5619 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5620 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5621 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5622 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5623 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5624 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5625 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5626 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5627 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5628 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5629 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5630 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5631 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5632 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5633 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5634 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5635 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5636 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5637 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5638 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5639 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5640 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5641 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5642 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5643 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5644 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5645 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5646 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5647 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5648 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5649 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5650 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5651 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5652 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5653 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5654 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5655 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5656 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5657 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5658 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5659 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5660 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5661 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5662 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5663 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5664 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5665 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5666 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5667 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5668 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5669 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5670 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5671 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5672 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5673 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5674 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5675 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5676 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5677 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5678 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5679 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5680 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5681 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5682 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5683 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5684 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5685 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5686 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5687 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5688 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5689 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5690 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5691 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5692 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5693 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5694 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5695 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5696 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5697 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5698 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5699 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5700 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5701 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5702 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5703 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5704 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5705 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5706 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5707 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5708 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5709 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5710 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5711 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5712 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5713 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5714 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5715 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5716 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5717 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5718 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5719 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5720 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5721 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5722 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5723 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5724 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5725 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5726 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5727 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5728 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5729 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5730 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5731 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5732 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5733 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5734 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5735 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5736 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5737 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5738 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5739 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5740 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5741 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5742 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5743 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5744 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5745 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5746 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5747 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5748 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5749 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5750 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5751 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5752 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5753 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5754 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5755 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5756 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5757 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5758 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5759 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5760 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5761 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5762 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5763 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5764 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5765 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5766 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5767 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5768 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5769 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5770 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5771 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5772 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5773 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5774 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5775 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5776 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5777 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5778 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5779 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5780 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5781 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5782 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5783 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5784 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5785 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5786 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5787 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5788 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5789 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5790 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5791 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5792 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5793 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5794 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5795 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5796 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5797 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5798 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5799 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5800 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5801 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5802 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5803 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5804 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5805 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5806 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5807 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5808 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5809 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5810 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5811 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5812 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5813 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5814 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5815 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5816 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5817 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5818 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5819 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5820 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5821 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5822 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5823 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5824 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5825 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5826 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5827 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5828 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5829 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5830 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5831 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5832 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5833 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5834 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5835 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5836 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5837 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5838 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5839 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5840 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5841 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5842 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5843 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5844 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5845 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5846 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5847 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5848 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5849 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5850 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5851 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5852 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5853 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5854 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5855 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5856 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5857 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5858 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5859 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5860 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5861 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5862 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5863 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5864 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5865 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5866 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5867 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5868 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5869 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5870 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5871 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5872 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5873 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5874 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5875 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5876 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5877 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5878 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5879 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5880 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5881 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5882 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5883 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5884 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5885 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5886 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5887 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5888 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5889 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5890 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5891 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5892 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5893 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5894 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5895 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5896 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5897 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5898 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5899 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5900 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5901 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5902 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5903 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5904 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5905 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5906 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5907 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5908 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5909 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5910 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5911 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5912 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5913 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5914 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5915 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5916 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5917 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5918 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5919 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5920 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5921 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5922 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5923 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5924 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5925 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5926 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5927 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5928 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5929 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5930 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5931 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5932 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5933 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5934 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5935 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5936 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5937 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5938 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5939 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5940 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5941 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5942 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5943 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5944 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5945 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5946 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5947 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5948 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5949 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5950 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5951 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5952 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5953 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5954 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5955 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5956 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5957 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5958 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5959 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5960 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5961 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5962 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5963 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5964 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5965 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5966 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5967 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5968 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5969 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5970 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5971 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5972 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5973 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5974 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5975 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5976 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5977 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5978 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5979 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5980 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5981 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5982 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5983 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5984 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5985 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5986 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5987 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5988 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5989 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5990 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5991 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5992 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5993 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5994 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5995 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5996 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5997 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5998 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 5999 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6000 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6001 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6002 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6003 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6004 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6005 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6006 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6007 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6008 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6009 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6010 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6011 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6012 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6013 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6014 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6015 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6016 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6017 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6018 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6019 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6020 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6021 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6022 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6023 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6024 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6025 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6026 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6027 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6028 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6029 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6030 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6031 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6032 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6033 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6034 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6035 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6036 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6037 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6038 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6039 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6040 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6041 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6042 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6043 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6044 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6045 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6046 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6047 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6048 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6049 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6050 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6051 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6052 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6053 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6054 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6055 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6056 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6057 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6058 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6059 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6060 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6061 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6062 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6063 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6064 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6065 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6066 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6067 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6068 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6069 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6070 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6071 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6072 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6073 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6074 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6075 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6076 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6077 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6078 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6079 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6080 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6081 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6082 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6083 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6084 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6085 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6086 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6087 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6088 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6089 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6090 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6091 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6092 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6093 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6094 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6095 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6096 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6097 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6098 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6099 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6100 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6101 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6102 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6103 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6104 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6105 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6106 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6107 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6108 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6109 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6110 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6111 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6112 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6113 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6114 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6115 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6116 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6117 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6118 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6119 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6120 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6121 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6122 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6123 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6124 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6125 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6126 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6127 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6128 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6129 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6130 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6131 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6132 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6133 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6134 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6135 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6136 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6137 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6138 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6139 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6140 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6141 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6142 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6143 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6144 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6145 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6146 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6147 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6148 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6149 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6150 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6151 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6152 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6153 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6154 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6155 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6156 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6157 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6158 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6159 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6160 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6161 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6162 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6163 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6164 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6165 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6166 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6167 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6168 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6169 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6170 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6171 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6172 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6173 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6174 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6175 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6176 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6177 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6178 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6179 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6180 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6181 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6182 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6183 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6184 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6185 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6186 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6187 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6188 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6189 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6190 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6191 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6192 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6193 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6194 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6195 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6196 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6197 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6198 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6199 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6200 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6201 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6202 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6203 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6204 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6205 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6206 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6207 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6208 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6209 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6210 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6211 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6212 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6213 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6214 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6215 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6216 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6217 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6218 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6219 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6220 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6221 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6222 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6223 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6224 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6225 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6226 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6227 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6228 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6229 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6230 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6231 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6232 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6233 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6234 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6235 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6236 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6237 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6238 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6239 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6240 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6241 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6242 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6243 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6244 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6245 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6246 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6247 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6248 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6249 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6250 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6251 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6252 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6253 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6254 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6255 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6256 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6257 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6258 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6259 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6260 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6261 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6262 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6263 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6264 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6265 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6266 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6267 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6268 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6269 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6270 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6271 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6272 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6273 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6274 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6275 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6276 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6277 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6278 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6279 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6280 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6281 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6282 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6283 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6284 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6285 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6286 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6287 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6288 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6289 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6290 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6291 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6292 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6293 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6294 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6295 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6296 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6297 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6298 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6299 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6300 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6301 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6302 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6303 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6304 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6305 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6306 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6307 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6308 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6309 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6310 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6311 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6312 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6313 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6314 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6315 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6316 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6317 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6318 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6319 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6320 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6321 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6322 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6323 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6324 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6325 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6326 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6327 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6328 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6329 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6330 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6331 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6332 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6333 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6334 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6335 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6336 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6337 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6338 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6339 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6340 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6341 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6342 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6343 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6344 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6345 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6346 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6347 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6348 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6349 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6350 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6351 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6352 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6353 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6354 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6355 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6356 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6357 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6358 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6359 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6360 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6361 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6362 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6363 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6364 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6365 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6366 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6367 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6368 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6369 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6370 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6371 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6372 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6373 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6374 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6375 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6376 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6377 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6378 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6379 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6380 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6381 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6382 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6383 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6384 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6385 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6386 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6387 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6388 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6389 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6390 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6391 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6392 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6393 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6394 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6395 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6396 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6397 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6398 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6399 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6400 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6401 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6402 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6403 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6404 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6405 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6406 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6407 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6408 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6409 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6410 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6411 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6412 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6413 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6414 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6415 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6416 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6417 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6418 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6419 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6420 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6421 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6422 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6423 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6424 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6425 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6426 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6427 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6428 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6429 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6430 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6431 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6432 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6433 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6434 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6435 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6436 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6437 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6438 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6439 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6440 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6441 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6442 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6443 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6444 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6445 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6446 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6447 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6448 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6449 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6450 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6451 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6452 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6453 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6454 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6455 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6456 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6457 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6458 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6459 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6460 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6461 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6462 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6463 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6464 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6465 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6466 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6467 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6468 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6469 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6470 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6471 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6472 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6473 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6474 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6475 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6476 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6477 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6478 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6479 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6480 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6481 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6482 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6483 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6484 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6485 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6486 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6487 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6488 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6489 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6490 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6491 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6492 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6493 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6494 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6495 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6496 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6497 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6498 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6499 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6500 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6501 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6502 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6503 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6504 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6505 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6506 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6507 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6508 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6509 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6510 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6511 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6512 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6513 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6514 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6515 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6516 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6517 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6518 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6519 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6520 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6521 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6522 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6523 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6524 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6525 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6526 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6527 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6528 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6529 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6530 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6531 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6532 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6533 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6534 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6535 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6536 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6537 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6538 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6539 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6540 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6541 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6542 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6543 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6544 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6545 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6546 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6547 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6548 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6549 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6550 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6551 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6552 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6553 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6554 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6555 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6556 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6557 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6558 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6559 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6560 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6561 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6562 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6563 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6564 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6565 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6566 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6567 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6568 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6569 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6570 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6571 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6572 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6573 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6574 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6575 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6576 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6577 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6578 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6579 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6580 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6581 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6582 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6583 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6584 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6585 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6586 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6587 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6588 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6589 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6590 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6591 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6592 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6593 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6594 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6595 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6596 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6597 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6598 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6599 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6600 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6601 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6602 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6603 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6604 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6605 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6606 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6607 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6608 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6609 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6610 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6611 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6612 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6613 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6614 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6615 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6616 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6617 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6618 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6619 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6620 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6621 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6622 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6623 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6624 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6625 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6626 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6627 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6628 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6629 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6630 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6631 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6632 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6633 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6634 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6635 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6636 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6637 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6638 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6639 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6640 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6641 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6642 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6643 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6644 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6645 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6646 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6647 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6648 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6649 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6650 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6651 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6652 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6653 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6654 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6655 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6656 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6657 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6658 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6659 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6660 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6661 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6662 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6663 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6664 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6665 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6666 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6667 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6668 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6669 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6670 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6671 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6672 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6673 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6674 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6675 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6676 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6677 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6678 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6679 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6680 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6681 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6682 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6683 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6684 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6685 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6686 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6687 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6688 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6689 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6690 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6691 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6692 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6693 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6694 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6695 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6696 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6697 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6698 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6699 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6700 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6701 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6702 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6703 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6704 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6705 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6706 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6707 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6708 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6709 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6710 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6711 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6712 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6713 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6714 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6715 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6716 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6717 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6718 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6719 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6720 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6721 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6722 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6723 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6724 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6725 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6726 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6727 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6728 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6729 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6730 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6731 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6732 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6733 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6734 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6735 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6736 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6737 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6738 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6739 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6740 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6741 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6742 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6743 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6744 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6745 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6746 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6747 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6748 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6749 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6750 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6751 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6752 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6753 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6754 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6755 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6756 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6757 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6758 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6759 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6760 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6761 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6762 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6763 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6764 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6765 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6766 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6767 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6768 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6769 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6770 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6771 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6772 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6773 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6774 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6775 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6776 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6777 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6778 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6779 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6780 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6781 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6782 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6783 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6784 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6785 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6786 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6787 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6788 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6789 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6790 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6791 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6792 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6793 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6794 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6795 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6796 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6797 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6798 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6799 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6800 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6801 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6802 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6803 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6804 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6805 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6806 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6807 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6808 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6809 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6810 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6811 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6812 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6813 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6814 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6815 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6816 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6817 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6818 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6819 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6820 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6821 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6822 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6823 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6824 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6825 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6826 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6827 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6828 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6829 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6830 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6831 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6832 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6833 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6834 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6835 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6836 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6837 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6838 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6839 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6840 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6841 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6842 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6843 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6844 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6845 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6846 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6847 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6848 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6849 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6850 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6851 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6852 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6853 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6854 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6855 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6856 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6857 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6858 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6859 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6860 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6861 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6862 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6863 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6864 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6865 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6866 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6867 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6868 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6869 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6870 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6871 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6872 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6873 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6874 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6875 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6876 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6877 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6878 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6879 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6880 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6881 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6882 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6883 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6884 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6885 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6886 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6887 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6888 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6889 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6890 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6891 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6892 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6893 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6894 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6895 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6896 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6897 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6898 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6899 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6900 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6901 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6902 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6903 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6904 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6905 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6906 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6907 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6908 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6909 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6910 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6911 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6912 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6913 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6914 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6915 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6916 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6917 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6918 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6919 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6920 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6921 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6922 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6923 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6924 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6925 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6926 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6927 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6928 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6929 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6930 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6931 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6932 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6933 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6934 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6935 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6936 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6937 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6938 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6939 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6940 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6941 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6942 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6943 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6944 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6945 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6946 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6947 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6948 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6949 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6950 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6951 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6952 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6953 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6954 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6955 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6956 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6957 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6958 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6959 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6960 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6961 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6962 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6963 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6964 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6965 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6966 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6967 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6968 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6969 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6970 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6971 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6972 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6973 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6974 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6975 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6976 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6977 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6978 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6979 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6980 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6981 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6982 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6983 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6984 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6985 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6986 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6987 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6988 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6989 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6990 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6991 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6992 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6993 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6994 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6995 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6996 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6997 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6998 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 6999 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7000 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7001 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7002 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7003 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7004 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7005 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7006 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7007 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7008 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7009 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7010 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7011 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7012 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7013 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7014 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7015 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7016 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7017 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7018 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7019 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7020 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7021 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7022 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7023 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7024 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7025 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7026 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7027 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7028 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7029 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7030 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7031 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7032 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7033 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7034 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7035 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7036 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7037 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7038 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7039 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7040 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7041 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7042 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7043 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7044 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7045 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7046 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7047 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7048 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7049 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7050 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7051 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7052 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7053 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7054 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7055 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7056 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7057 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7058 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7059 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7060 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7061 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7062 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7063 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7064 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7065 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7066 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7067 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7068 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7069 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7070 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7071 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7072 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7073 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7074 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7075 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7076 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7077 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7078 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7079 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7080 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7081 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7082 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7083 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7084 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7085 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7086 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7087 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7088 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7089 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7090 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7091 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7092 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7093 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7094 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7095 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7096 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7097 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7098 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7099 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7100 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7101 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7102 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7103 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7104 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7105 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7106 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7107 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7108 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7109 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7110 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7111 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7112 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7113 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7114 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7115 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7116 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7117 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7118 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7119 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7120 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7121 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7122 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7123 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7124 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7125 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7126 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7127 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7128 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7129 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7130 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7131 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7132 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7133 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7134 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7135 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7136 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7137 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7138 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7139 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7140 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7141 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7142 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7143 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7144 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7145 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7146 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7147 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7148 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7149 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7150 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7151 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7152 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7153 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7154 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7155 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7156 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7157 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7158 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7159 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7160 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7161 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7162 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7163 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7164 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7165 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7166 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7167 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7168 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7169 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7170 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7171 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7172 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7173 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7174 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7175 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7176 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7177 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7178 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7179 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7180 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7181 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7182 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7183 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7184 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7185 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7186 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7187 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7188 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7189 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7190 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7191 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7192 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7193 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7194 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7195 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7196 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7197 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7198 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7199 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7200 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7201 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7202 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7203 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7204 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7205 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7206 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7207 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7208 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7209 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7210 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7211 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7212 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7213 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7214 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7215 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7216 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7217 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7218 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7219 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7220 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7221 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7222 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7223 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7224 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7225 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7226 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7227 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7228 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7229 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7230 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7231 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7232 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7233 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7234 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7235 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7236 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7237 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7238 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7239 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7240 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7241 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7242 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7243 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7244 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7245 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7246 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7247 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7248 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7249 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7250 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7251 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7252 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7253 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7254 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7255 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7256 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7257 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7258 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7259 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7260 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7261 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7262 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7263 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7264 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7265 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7266 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7267 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7268 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7269 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7270 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7271 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7272 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7273 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7274 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7275 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7276 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7277 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7278 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7279 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7280 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7281 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7282 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7283 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7284 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7285 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7286 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7287 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7288 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7289 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7290 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7291 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7292 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7293 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7294 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7295 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7296 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7297 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7298 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7299 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7300 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7301 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7302 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7303 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7304 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7305 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7306 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7307 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7308 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7309 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7310 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7311 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7312 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7313 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7314 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7315 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7316 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7317 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7318 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7319 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7320 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7321 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7322 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7323 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7324 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7325 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7326 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7327 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7328 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7329 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7330 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7331 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7332 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7333 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7334 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7335 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7336 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7337 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7338 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7339 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7340 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7341 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7342 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7343 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7344 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7345 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7346 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7347 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7348 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7349 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7350 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7351 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7352 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7353 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7354 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7355 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7356 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7357 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7358 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7359 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7360 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7361 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7362 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7363 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7364 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7365 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7366 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7367 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7368 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7369 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7370 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7371 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7372 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7373 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7374 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7375 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7376 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7377 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7378 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7379 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7380 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7381 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7382 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7383 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7384 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7385 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7386 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7387 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7388 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7389 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7390 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7391 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7392 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7393 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7394 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7395 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7396 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7397 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7398 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7399 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7400 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7401 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7402 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7403 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7404 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7405 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7406 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7407 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7408 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7409 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7410 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7411 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7412 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7413 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7414 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7415 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7416 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7417 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7418 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7419 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7420 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7421 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7422 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7423 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7424 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7425 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7426 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7427 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7428 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7429 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7430 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7431 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7432 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7433 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7434 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7435 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7436 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7437 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7438 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7439 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7440 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7441 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7442 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7443 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7444 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7445 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7446 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7447 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7448 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7449 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7450 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7451 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7452 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7453 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7454 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7455 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7456 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7457 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7458 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7459 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7460 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7461 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7462 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7463 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7464 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7465 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7466 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7467 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7468 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7469 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7470 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7471 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7472 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7473 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7474 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7475 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7476 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7477 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7478 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7479 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7480 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7481 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7482 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7483 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7484 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7485 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7486 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7487 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7488 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7489 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7490 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7491 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7492 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7493 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7494 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7495 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7496 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7497 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7498 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7499 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7500 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7501 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7502 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7503 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7504 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7505 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7506 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7507 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7508 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7509 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7510 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7511 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7512 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7513 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7514 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7515 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7516 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7517 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7518 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7519 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7520 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7521 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7522 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7523 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7524 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7525 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7526 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7527 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7528 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7529 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7530 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7531 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7532 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7533 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7534 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7535 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7536 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7537 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7538 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7539 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7540 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7541 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7542 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7543 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7544 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7545 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7546 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7547 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7548 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7549 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7550 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7551 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7552 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7553 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7554 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7555 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7556 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7557 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7558 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7559 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7560 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7561 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7562 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7563 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7564 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7565 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7566 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7567 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7568 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7569 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7570 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7571 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7572 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7573 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7574 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7575 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7576 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7577 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7578 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7579 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7580 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7581 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7582 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7583 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7584 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7585 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7586 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7587 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7588 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7589 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7590 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7591 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7592 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7593 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7594 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7595 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7596 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7597 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7598 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7599 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7600 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7601 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7602 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7603 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7604 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7605 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7606 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7607 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7608 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7609 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7610 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7611 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7612 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7613 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7614 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7615 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7616 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7617 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7618 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7619 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7620 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7621 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7622 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7623 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7624 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7625 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7626 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7627 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7628 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7629 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7630 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7631 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7632 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7633 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7634 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7635 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7636 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7637 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7638 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7639 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7640 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7641 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7642 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7643 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7644 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7645 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7646 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7647 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7648 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7649 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7650 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7651 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7652 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7653 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7654 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7655 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7656 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7657 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7658 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7659 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7660 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7661 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7662 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7663 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7664 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7665 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7666 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7667 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7668 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7669 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7670 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7671 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7672 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7673 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7674 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7675 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7676 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7677 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7678 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7679 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7680 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7681 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7682 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7683 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7684 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7685 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7686 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7687 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7688 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7689 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7690 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7691 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7692 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7693 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7694 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7695 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7696 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7697 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7698 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7699 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7700 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7701 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7702 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7703 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7704 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7705 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7706 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7707 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7708 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7709 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7710 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7711 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7712 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7713 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7714 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7715 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7716 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7717 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7718 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7719 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7720 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7721 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7722 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7723 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7724 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7725 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7726 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7727 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7728 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7729 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7730 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7731 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7732 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7733 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7734 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7735 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7736 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7737 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7738 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7739 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7740 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7741 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7742 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7743 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7744 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7745 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7746 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7747 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7748 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7749 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7750 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7751 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7752 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7753 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7754 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7755 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7756 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7757 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7758 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7759 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7760 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7761 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7762 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7763 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7764 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7765 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7766 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7767 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7768 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7769 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7770 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7771 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7772 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7773 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7774 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7775 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7776 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7777 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7778 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7779 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7780 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7781 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7782 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7783 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7784 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7785 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7786 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7787 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7788 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7789 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7790 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7791 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7792 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7793 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7794 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7795 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7796 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7797 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7798 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7799 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7800 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7801 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7802 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7803 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7804 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7805 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7806 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7807 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7808 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7809 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7810 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7811 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7812 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7813 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7814 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7815 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7816 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7817 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7818 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7819 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7820 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7821 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7822 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7823 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7824 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7825 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7826 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7827 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7828 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7829 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7830 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7831 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7832 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7833 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7834 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7835 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7836 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7837 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7838 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7839 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7840 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7841 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7842 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7843 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7844 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7845 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7846 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7847 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7848 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7849 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7850 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7851 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7852 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7853 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7854 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7855 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7856 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7857 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7858 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7859 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7860 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7861 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7862 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7863 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7864 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7865 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7866 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7867 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7868 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7869 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7870 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7871 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7872 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7873 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7874 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7875 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7876 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7877 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7878 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7879 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7880 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7881 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7882 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7883 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7884 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7885 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7886 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7887 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7888 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7889 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7890 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7891 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7892 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7893 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7894 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7895 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7896 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7897 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7898 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7899 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7900 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7901 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7902 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7903 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7904 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7905 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7906 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7907 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7908 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7909 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7910 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7911 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7912 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7913 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7914 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7915 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7916 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7917 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7918 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7919 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7920 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7921 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7922 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7923 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7924 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7925 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7926 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7927 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7928 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7929 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7930 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7931 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7932 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7933 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7934 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7935 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7936 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7937 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7938 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7939 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7940 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7941 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7942 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7943 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7944 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7945 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7946 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7947 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7948 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7949 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7950 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7951 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7952 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7953 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7954 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7955 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7956 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7957 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7958 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7959 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7960 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7961 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7962 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7963 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7964 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7965 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7966 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7967 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7968 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7969 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7970 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7971 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7972 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7973 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7974 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7975 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7976 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7977 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7978 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7979 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7980 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7981 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7982 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7983 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7984 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7985 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7986 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7987 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7988 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7989 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7990 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7991 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7992 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7993 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7994 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7995 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7996 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7997 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7998 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 7999 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8000 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8001 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8002 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8003 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8004 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8005 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8006 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8007 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8008 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8009 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8010 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8011 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8012 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8013 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8014 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8015 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8016 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8017 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8018 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8019 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8020 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8021 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8022 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8023 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8024 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8025 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8026 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8027 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8028 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8029 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8030 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8031 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8032 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8033 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8034 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8035 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8036 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8037 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8038 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8039 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8040 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8041 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8042 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8043 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8044 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8045 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8046 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8047 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8048 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8049 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8050 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8051 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8052 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8053 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8054 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8055 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8056 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8057 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8058 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8059 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8060 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8061 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8062 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8063 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8064 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8065 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8066 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8067 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8068 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8069 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8070 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8071 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8072 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8073 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8074 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8075 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8076 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8077 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8078 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8079 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8080 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8081 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8082 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8083 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8084 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8085 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8086 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8087 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8088 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8089 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8090 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8091 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8092 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8093 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8094 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8095 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8096 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8097 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8098 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8099 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8100 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8101 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8102 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8103 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8104 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8105 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8106 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8107 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8108 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8109 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8110 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8111 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8112 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8113 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8114 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8115 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8116 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8117 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8118 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8119 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8120 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8121 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8122 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8123 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8124 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8125 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8126 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8127 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8128 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8129 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8130 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8131 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8132 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8133 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8134 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8135 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8136 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8137 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8138 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8139 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8140 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8141 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8142 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8143 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8144 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8145 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8146 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8147 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8148 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8149 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8150 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8151 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8152 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8153 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8154 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8155 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8156 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8157 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8158 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8159 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8160 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8161 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8162 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8163 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8164 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8165 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8166 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8167 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8168 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8169 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8170 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8171 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8172 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8173 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8174 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8175 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8176 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8177 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8178 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8179 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8180 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8181 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8182 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8183 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8184 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8185 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8186 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8187 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8188 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8189 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8190 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8191 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8192 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8193 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8194 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8195 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8196 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8197 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8198 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8199 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8200 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8201 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8202 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8203 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8204 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8205 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8206 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8207 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8208 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8209 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8210 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8211 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8212 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8213 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8214 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8215 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8216 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8217 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8218 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8219 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8220 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8221 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8222 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8223 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8224 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8225 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8226 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8227 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8228 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8229 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8230 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8231 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8232 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8233 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8234 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8235 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8236 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8237 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8238 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8239 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8240 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8241 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8242 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8243 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8244 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8245 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8246 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8247 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8248 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8249 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8250 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8251 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8252 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8253 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8254 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8255 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8256 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8257 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8258 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8259 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8260 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8261 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8262 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8263 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8264 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8265 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8266 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8267 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8268 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8269 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8270 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8271 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8272 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8273 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8274 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8275 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8276 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8277 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8278 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8279 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8280 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8281 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8282 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8283 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8284 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8285 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8286 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8287 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8288 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8289 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8290 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8291 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8292 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8293 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8294 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8295 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8296 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8297 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8298 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8299 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8300 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8301 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8302 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8303 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8304 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8305 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8306 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8307 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8308 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8309 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8310 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8311 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8312 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8313 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8314 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8315 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8316 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8317 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8318 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8319 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8320 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8321 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8322 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8323 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8324 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8325 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8326 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8327 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8328 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8329 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8330 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8331 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8332 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8333 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8334 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8335 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8336 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8337 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8338 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8339 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8340 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8341 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8342 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8343 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8344 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8345 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8346 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8347 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8348 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8349 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8350 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8351 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8352 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8353 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8354 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8355 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8356 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8357 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8358 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8359 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8360 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8361 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8362 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8363 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8364 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8365 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8366 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8367 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8368 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8369 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8370 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8371 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8372 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8373 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8374 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8375 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8376 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8377 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8378 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8379 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8380 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8381 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8382 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8383 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8384 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8385 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8386 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8387 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8388 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8389 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8390 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8391 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8392 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8393 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8394 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8395 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8396 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8397 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8398 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8399 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8400 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8401 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8402 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8403 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8404 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8405 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8406 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8407 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8408 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8409 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8410 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8411 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8412 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8413 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8414 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8415 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8416 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8417 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8418 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8419 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8420 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8421 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8422 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8423 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8424 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8425 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8426 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8427 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8428 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8429 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8430 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8431 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8432 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8433 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8434 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8435 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8436 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8437 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8438 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8439 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8440 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8441 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8442 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8443 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8444 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8445 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8446 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8447 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8448 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8449 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8450 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8451 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8452 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8453 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8454 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8455 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8456 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8457 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8458 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8459 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8460 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8461 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8462 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8463 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8464 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8465 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8466 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8467 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8468 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8469 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8470 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8471 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8472 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8473 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8474 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8475 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8476 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8477 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8478 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8479 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8480 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8481 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8482 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8483 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8484 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8485 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8486 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8487 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8488 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8489 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8490 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8491 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8492 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8493 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8494 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8495 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8496 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8497 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8498 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8499 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8500 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8501 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8502 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8503 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8504 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8505 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8506 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8507 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8508 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8509 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8510 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8511 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8512 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8513 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8514 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8515 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8516 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8517 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8518 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8519 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8520 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8521 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8522 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8523 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8524 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8525 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8526 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8527 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8528 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8529 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8530 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8531 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8532 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8533 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8534 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8535 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8536 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8537 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8538 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8539 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8540 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8541 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8542 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8543 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8544 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8545 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8546 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8547 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8548 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8549 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8550 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8551 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8552 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8553 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8554 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8555 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8556 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8557 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8558 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8559 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8560 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8561 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8562 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8563 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8564 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8565 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8566 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8567 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8568 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8569 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8570 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8571 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8572 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8573 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8574 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8575 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8576 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8577 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8578 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8579 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8580 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8581 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8582 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8583 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8584 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8585 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8586 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8587 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8588 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8589 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8590 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8591 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8592 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8593 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8594 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8595 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8596 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8597 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8598 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8599 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8600 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8601 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8602 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8603 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8604 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8605 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8606 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8607 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8608 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8609 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8610 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8611 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8612 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8613 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8614 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8615 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8616 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8617 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8618 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8619 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8620 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8621 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8622 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8623 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8624 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8625 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8626 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8627 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8628 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8629 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8630 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8631 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8632 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8633 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8634 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8635 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8636 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8637 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8638 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8639 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8640 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8641 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8642 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8643 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8644 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8645 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8646 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8647 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8648 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8649 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8650 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8651 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8652 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8653 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8654 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8655 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8656 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8657 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8658 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8659 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8660 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8661 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8662 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8663 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8664 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8665 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8666 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8667 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8668 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8669 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8670 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8671 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8672 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8673 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8674 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8675 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8676 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8677 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8678 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8679 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8680 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8681 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8682 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8683 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8684 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8685 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8686 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8687 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8688 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8689 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8690 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8691 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8692 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8693 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8694 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8695 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8696 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8697 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8698 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8699 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8700 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8701 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8702 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8703 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8704 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8705 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8706 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8707 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8708 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8709 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8710 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8711 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8712 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8713 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8714 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8715 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8716 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8717 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8718 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8719 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8720 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8721 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8722 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8723 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8724 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8725 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8726 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8727 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8728 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8729 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8730 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8731 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8732 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8733 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8734 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8735 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8736 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8737 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8738 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8739 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8740 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8741 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8742 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8743 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8744 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8745 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8746 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8747 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8748 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8749 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8750 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8751 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8752 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8753 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8754 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8755 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8756 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8757 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8758 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8759 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8760 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8761 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8762 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8763 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8764 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8765 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8766 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8767 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8768 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8769 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8770 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8771 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8772 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8773 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8774 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8775 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8776 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8777 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8778 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8779 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8780 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8781 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8782 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8783 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8784 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8785 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8786 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8787 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8788 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8789 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8790 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8791 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8792 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8793 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8794 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8795 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8796 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8797 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8798 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8799 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8800 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8801 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8802 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8803 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8804 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8805 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8806 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8807 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8808 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8809 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8810 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8811 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8812 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8813 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8814 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8815 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8816 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8817 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8818 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8819 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8820 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8821 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8822 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8823 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8824 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8825 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8826 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8827 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8828 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8829 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8830 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8831 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8832 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8833 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8834 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8835 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8836 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8837 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8838 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8839 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8840 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8841 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8842 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8843 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8844 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8845 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8846 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8847 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8848 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8849 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8850 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8851 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8852 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8853 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8854 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8855 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8856 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8857 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8858 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8859 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8860 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8861 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8862 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8863 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8864 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8865 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8866 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8867 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8868 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8869 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8870 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8871 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8872 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8873 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8874 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8875 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8876 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8877 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8878 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8879 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8880 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8881 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8882 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8883 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8884 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8885 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8886 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8887 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8888 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8889 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8890 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8891 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8892 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8893 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8894 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8895 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8896 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8897 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8898 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8899 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8900 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8901 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8902 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8903 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8904 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8905 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8906 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8907 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8908 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8909 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8910 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8911 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8912 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8913 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8914 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8915 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8916 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8917 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8918 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8919 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8920 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8921 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8922 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8923 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8924 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8925 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8926 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8927 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8928 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8929 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8930 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8931 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8932 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8933 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8934 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8935 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8936 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8937 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8938 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8939 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8940 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8941 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8942 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8943 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8944 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8945 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8946 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8947 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8948 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8949 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8950 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8951 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8952 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8953 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8954 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8955 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8956 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8957 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8958 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8959 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8960 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8961 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8962 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8963 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8964 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8965 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8966 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8967 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8968 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8969 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8970 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8971 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8972 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8973 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8974 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8975 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8976 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8977 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8978 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8979 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8980 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8981 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8982 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8983 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8984 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8985 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8986 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8987 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8988 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8989 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8990 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8991 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8992 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8993 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8994 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8995 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8996 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8997 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8998 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 8999 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9000 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9001 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9002 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9003 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9004 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9005 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9006 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9007 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9008 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9009 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9010 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9011 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9012 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9013 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9014 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9015 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9016 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9017 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9018 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9019 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9020 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9021 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9022 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9023 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9024 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9025 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9026 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9027 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9028 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9029 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9030 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9031 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9032 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9033 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9034 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9035 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9036 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9037 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9038 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9039 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9040 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9041 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9042 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9043 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9044 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9045 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9046 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9047 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9048 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9049 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9050 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9051 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9052 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9053 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9054 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9055 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9056 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9057 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9058 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9059 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9060 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9061 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9062 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9063 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9064 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9065 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9066 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9067 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9068 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9069 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9070 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9071 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9072 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9073 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9074 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9075 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9076 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9077 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9078 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9079 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9080 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9081 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9082 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9083 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9084 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9085 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9086 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9087 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9088 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9089 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9090 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9091 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9092 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9093 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9094 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9095 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9096 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9097 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9098 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9099 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9100 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9101 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9102 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9103 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9104 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9105 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9106 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9107 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9108 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9109 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9110 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9111 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9112 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9113 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9114 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9115 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9116 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9117 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9118 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9119 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9120 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9121 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9122 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9123 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9124 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9125 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9126 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9127 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9128 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9129 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9130 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9131 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9132 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9133 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9134 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9135 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9136 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9137 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9138 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9139 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9140 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9141 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9142 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9143 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9144 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9145 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9146 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9147 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9148 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9149 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9150 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9151 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9152 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9153 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9154 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9155 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9156 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9157 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9158 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9159 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9160 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9161 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9162 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9163 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9164 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9165 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9166 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9167 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9168 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9169 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9170 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9171 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9172 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9173 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9174 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9175 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9176 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9177 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9178 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9179 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9180 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9181 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9182 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9183 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9184 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9185 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9186 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9187 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9188 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9189 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9190 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9191 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9192 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9193 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9194 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9195 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9196 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9197 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9198 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9199 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9200 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9201 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9202 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9203 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9204 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9205 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9206 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9207 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9208 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9209 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9210 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9211 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9212 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9213 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9214 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9215 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9216 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9217 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9218 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9219 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9220 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9221 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9222 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9223 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9224 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9225 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9226 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9227 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9228 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9229 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9230 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9231 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9232 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9233 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9234 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9235 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9236 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9237 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9238 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9239 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9240 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9241 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9242 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9243 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9244 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9245 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9246 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9247 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9248 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9249 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9250 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9251 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9252 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9253 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9254 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9255 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9256 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9257 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9258 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9259 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9260 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9261 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9262 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9263 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9264 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9265 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9266 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9267 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9268 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9269 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9270 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9271 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9272 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9273 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9274 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9275 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9276 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9277 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9278 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9279 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9280 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9281 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9282 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9283 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9284 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9285 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9286 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9287 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9288 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9289 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9290 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9291 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9292 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9293 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9294 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9295 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9296 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9297 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9298 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9299 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9300 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9301 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9302 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9303 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9304 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9305 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9306 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9307 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9308 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9309 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9310 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9311 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9312 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9313 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9314 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9315 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9316 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9317 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9318 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9319 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9320 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9321 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9322 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9323 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9324 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9325 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9326 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9327 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9328 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9329 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9330 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9331 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9332 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9333 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9334 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9335 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9336 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9337 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9338 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9339 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9340 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9341 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9342 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9343 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9344 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9345 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9346 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9347 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9348 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9349 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9350 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9351 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9352 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9353 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9354 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9355 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9356 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9357 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9358 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9359 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9360 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9361 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9362 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9363 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9364 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9365 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9366 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9367 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9368 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9369 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9370 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9371 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9372 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9373 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9374 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9375 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9376 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9377 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9378 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9379 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9380 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9381 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9382 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9383 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9384 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9385 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9386 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9387 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9388 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9389 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9390 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9391 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9392 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9393 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9394 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9395 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9396 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9397 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9398 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9399 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9400 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9401 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9402 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9403 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9404 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9405 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9406 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9407 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9408 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9409 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9410 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9411 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9412 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9413 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9414 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9415 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9416 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9417 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9418 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9419 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9420 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9421 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9422 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9423 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9424 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9425 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9426 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9427 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9428 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9429 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9430 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9431 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9432 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9433 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9434 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9435 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9436 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9437 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9438 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9439 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9440 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9441 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9442 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9443 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9444 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9445 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9446 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9447 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9448 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9449 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9450 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9451 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9452 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9453 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9454 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9455 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9456 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9457 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9458 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9459 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9460 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9461 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9462 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9463 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9464 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9465 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9466 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9467 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9468 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9469 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9470 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9471 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9472 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9473 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9474 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9475 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9476 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9477 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9478 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9479 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9480 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9481 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9482 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9483 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9484 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9485 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9486 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9487 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9488 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9489 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9490 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9491 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9492 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9493 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9494 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9495 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9496 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9497 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9498 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9499 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9500 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9501 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9502 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9503 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9504 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9505 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9506 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9507 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9508 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9509 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9510 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9511 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9512 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9513 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9514 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9515 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9516 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9517 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9518 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9519 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9520 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9521 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9522 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9523 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9524 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9525 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9526 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9527 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9528 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9529 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9530 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9531 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9532 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9533 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9534 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9535 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9536 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9537 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9538 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9539 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9540 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9541 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9542 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9543 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9544 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9545 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9546 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9547 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9548 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9549 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9550 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9551 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9552 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9553 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9554 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9555 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9556 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9557 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9558 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9559 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9560 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9561 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9562 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9563 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9564 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9565 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9566 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9567 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9568 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9569 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9570 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9571 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9572 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9573 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9574 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9575 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9576 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9577 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9578 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9579 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9580 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9581 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9582 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9583 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9584 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9585 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9586 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9587 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9588 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9589 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9590 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9591 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9592 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9593 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9594 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9595 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9596 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9597 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9598 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9599 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9600 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9601 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9602 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9603 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9604 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9605 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9606 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9607 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9608 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9609 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9610 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9611 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9612 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9613 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9614 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9615 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9616 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9617 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9618 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9619 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9620 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9621 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9622 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9623 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9624 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9625 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9626 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9627 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9628 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9629 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9630 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9631 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9632 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9633 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9634 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9635 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9636 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9637 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9638 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9639 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9640 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9641 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9642 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9643 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9644 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9645 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9646 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9647 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9648 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9649 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9650 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9651 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9652 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9653 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9654 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9655 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9656 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9657 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9658 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9659 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9660 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9661 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9662 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9663 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9664 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9665 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9666 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9667 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9668 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9669 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9670 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9671 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9672 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9673 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9674 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9675 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9676 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9677 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9678 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9679 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9680 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9681 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9682 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9683 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9684 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9685 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9686 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9687 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9688 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9689 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9690 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9691 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9692 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9693 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9694 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9695 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9696 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9697 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9698 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9699 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9700 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9701 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9702 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9703 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9704 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9705 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9706 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9707 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9708 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9709 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9710 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9711 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9712 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9713 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9714 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9715 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9716 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9717 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9718 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9719 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9720 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9721 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9722 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9723 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9724 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9725 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9726 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9727 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9728 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9729 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9730 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9731 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9732 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9733 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9734 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9735 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9736 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9737 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9738 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9739 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9740 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9741 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9742 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9743 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9744 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9745 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9746 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9747 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9748 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9749 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9750 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9751 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9752 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9753 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9754 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9755 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9756 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9757 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9758 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9759 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9760 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9761 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9762 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9763 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9764 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9765 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9766 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9767 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9768 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9769 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9770 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9771 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9772 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9773 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9774 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9775 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9776 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9777 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9778 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9779 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9780 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9781 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9782 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9783 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9784 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9785 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9786 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9787 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9788 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9789 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9790 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9791 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9792 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9793 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9794 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9795 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9796 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9797 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9798 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9799 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9800 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9801 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9802 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9803 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9804 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9805 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9806 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9807 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9808 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9809 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9810 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9811 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9812 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9813 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9814 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9815 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9816 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9817 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9818 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9819 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9820 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9821 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9822 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9823 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9824 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9825 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9826 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9827 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9828 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9829 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9830 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9831 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9832 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9833 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9834 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9835 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9836 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9837 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9838 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9839 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9840 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9841 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9842 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9843 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9844 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9845 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9846 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9847 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9848 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9849 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9850 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9851 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9852 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9853 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9854 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9855 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9856 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9857 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9858 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9859 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9860 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9861 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9862 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9863 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9864 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9865 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9866 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9867 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9868 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9869 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9870 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9871 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9872 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9873 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9874 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9875 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9876 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9877 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9878 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9879 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9880 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9881 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9882 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9883 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9884 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9885 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9886 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9887 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9888 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9889 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9890 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9891 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9892 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9893 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9894 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9895 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9896 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9897 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9898 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9899 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9900 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9901 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9902 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9903 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9904 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9905 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9906 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9907 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9908 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9909 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9910 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9911 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9912 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9913 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9914 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9915 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9916 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9917 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9918 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9919 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9920 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9921 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9922 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9923 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9924 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9925 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9926 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9927 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9928 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9929 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9930 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9931 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9932 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9933 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9934 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9935 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9936 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9937 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9938 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9939 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9940 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9941 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9942 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9943 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9944 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9945 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9946 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9947 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9948 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9949 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9950 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9951 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9952 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9953 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9954 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9955 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9956 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9957 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9958 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9959 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9960 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9961 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9962 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9963 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9964 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9965 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9966 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9967 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9968 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9969 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9970 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9971 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9972 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9973 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9974 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9975 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9976 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9977 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9978 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9979 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9980 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9981 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9982 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9983 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9984 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9985 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9986 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9987 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9988 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9989 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9990 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9991 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9992 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9993 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9994 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9995 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9996 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9997 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9998 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 9999 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n",
      "Epoch: 10000 | Loss: 0.14000000059604645 | Test loss: 0.34103459119796753\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    model.train()\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Eval loop\n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():    \n",
    "        test_pred = model(X_test)\n",
    "\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "    # print\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():    \n",
    "    test_pred = model(X_test)\n",
    "plot_predictions(predictions=test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mltorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
